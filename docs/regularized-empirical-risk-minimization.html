<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="support-vector-machine.html"/>
<link rel="next" href="uncertainty-quantification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hat-eta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat \eta_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#erm-with-kernels"><i class="fa fa-check"></i><b>8.2</b> ERM with Kernels</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#comparison"><i class="fa fa-check"></i><b>8.2.1</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#regularity"><i class="fa fa-check"></i><b>8.3</b> Regularity</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html"><i class="fa fa-check"></i><b>9</b> Uncertainty Quantification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#bayes-perspective"><i class="fa fa-check"></i><b>9.1</b> Bayes Perspective</a></li>
<li class="chapter" data-level="9.2" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#gaussians-in-mathcal-rd"><i class="fa fa-check"></i><b>9.2</b> Gaussians in <span class="math inline">\(\mathcal R^d\)</span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-empirical-risk-minimization" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regularized Empirical Risk Minimization<a href="regularized-empirical-risk-minimization.html#regularized-empirical-risk-minimization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="ill-posed-problems" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> ill Posed Problems<a href="regularized-empirical-risk-minimization.html#ill-posed-problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a simple example: in linear regression, we try to find <span class="math inline">\(\beta \in \mathcal R^d\)</span> s.t <span class="math inline">\(\sum_{i=1}^n (y_i - &lt;\beta, x_i&gt;_{\mathcal R^d})^2\)</span> is minimized. The solution <span class="math inline">\(\beta^* = (X^T X)^{-1} X^T Y\)</span> only make sense when <span class="math inline">\((X^TX)^{-1}\)</span> is invertible, or the optimization problem has a unique solution. The ill posed problems are the situation where there is no unique solution or <span class="math inline">\((X^TX)^{-1}\)</span> is not invertible (<span class="math inline">\(d &gt;&gt; n\)</span>, not full rank, …).</p>
<p>Ridge regression aims at tackling this problem by adding a “regularizer” to the optimization problem: <span class="math inline">\(\underset{\beta}{\operatorname{min}} \lambda || \beta ||^2 + \sum_{i=1}^n (y_i - &lt;x_i, \beta&gt;)^2\)</span> with <span class="math inline">\(\beta^* = (X^T X + \lambda I)^{-1} X^T Y\)</span>, where higher <span class="math inline">\(\lambda\)</span> implies more “regular” and lower <span class="math inline">\(\lambda\)</span> implies lesser “regular”. Naturally, one needs to choose <span class="math inline">\(\lambda \in (0, \infty)\)</span> to balance between variance and bias.</p>
<p>In more generality, suppose we wanted to fit a function <span class="math inline">\(f\)</span> that explains some observed data (<span class="math inline">\(f: [0,1] \to \mathcal R\)</span>):</p>
<p><img src="figures/ridge-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We are trying to solve <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} \sum_{i=1}^n (f(x_i) - y_i)^2\)</span>. How many functions are close to the observations is the ill posed problem.</p>
<p>The idea in empirical risk minimization is to enforce “regularity” on the functions by adding a regularizer to the empirical risk object function: <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} \sum_{i=1}^n (f(x_i) - y_i)^2 + \lambda R(f)\)</span>, where <span class="math inline">\(Z\)</span> is some family of functions for which the objective function makes sense and <span class="math inline">\(R\)</span> is some regularizer.</p>
<div class="remark">
<p><span id="unlabeled-div-39" class="remark"><em>Remark</em>. </span>It is important to make sure that the objective function makes sense for elements <span class="math inline">\(f \in Z\)</span>. For example, <span class="math inline">\(Z\)</span> can not be <span class="math inline">\(\mathcal L^2(\mathcal R)\)</span>, since for an arbitrary <span class="math inline">\(f \in \mathcal L^2(\mathcal R)\)</span>, we can not make sense of <span class="math inline">\(f(x_i)\)</span>.</p>
</div>
<p>In any case, after choosing <span class="math inline">\(Z\)</span> and <span class="math inline">\(R\)</span>, we consider the solution to the regularized empirical risk minimization problem as a regression function for the data. Notice that this approach is a non-parametric approach (<span class="math inline">\(f \in Z\)</span>).</p>
<p>Q: What types of regularizers we could work with?</p>
<p>A: Later we will consider <span class="math inline">\(Z = \mathcal H\)</span> (RKHS) and <span class="math inline">\(R(f) = ||f||^2_\mathcal H\)</span>, but for now let us consider an unrelated setting:</p>
<div class="example">
<p><span id="exm:J" class="example"><strong>Example 8.1  </strong></span>Let <span class="math inline">\(\mathcal X = [0,1]\)</span> and the data <span class="math inline">\(0&lt;x_1&lt;\cdots&lt;x_n&lt;1\)</span> with corresponding values <span class="math inline">\(y_1,\cdots,y_n \in \mathcal R\)</span>. Let <span class="math inline">\(Z\)</span> be the set of functions <span class="math inline">\(f:[0,1] \to \mathcal R\)</span> s.t <span class="math inline">\(f(x) = \int_0^x f&#39;(t)dt\)</span>, <span class="math inline">\(\forall x \in (0,1)\)</span> and s.t <span class="math inline">\(\int_0^1 (f&#39;(x))^2 dx &lt; \infty\)</span>. Define <span class="math inline">\(C_1([0,1])\)</span> as <em>Absolutely Continuous Functions defined on [0,1]</em>. Then, <span class="math inline">\(C_1([0,1]) \subset Z\)</span> i.e. <span class="math inline">\(f \in C_1([0,1]) \text{ and } f(0) = 0\Rightarrow f\in Z\)</span>.</p>
<p>Now let <span class="math inline">\(J(f):= \lambda \int_0^1 (f&#39;(x))^2 dx (\text{ Regularization Term}) + \sum_{i=1}^n (f(x_i) - y_i)^2 (\text{Fidality Term})\)</span>. The goal is to:</p>
<ul>
<li>Minimize <span class="math inline">\(J\)</span>: <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} J(f)\)</span>.</li>
<li>Obtain a “first order” condition for optimality.</li>
</ul>
</div>
<hr />
<p>In search of inspiration, let’s go back to the Euclidean case: <span class="math inline">\(\underset{x \in \mathcal R^m}{\operatorname{min}} F(x)\)</span>.</p>
<p>First order condition: <span class="math inline">\(\nabla F(x^*) = \vec 0\)</span>, that is <span class="math inline">\(\frac{\partial F}{\partial x_i}(x^*) = 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,m\)</span>.</p>
<p>Moreover, if <span class="math inline">\(x^*\)</span> minimizes <span class="math inline">\(F\)</span> over the whole <span class="math inline">\(\mathcal R^m\)</span>, it certainly minimizes <span class="math inline">\(F\)</span> over the line <span class="math inline">\(\{x^* + tv: t\in \mathcal R\}\)</span>. Thus, fix <span class="math inline">\(v\)</span> and define the function <span class="math inline">\(\phi_v(t):= F(x^* + tv)\)</span>. Then this function has a minimum at <span class="math inline">\(t=0\)</span>, which means that</p>
<p><span class="math display">\[
0 = \phi&#39;_v(0) = \frac{d}{dt} F(x^* + tv) |_{t=0} = &lt;\nabla F(x^*), v&gt; = \partial_v F(x^*)
\]</span></p>
<p>The optimality condition in <span class="math inline">\(\mathcal R^m\)</span> is equivalent to the optimality condition fro a collection of functions in <span class="math inline">\(\mathcal R\)</span>.</p>
<hr />
<p>So, going back to <strong>Example <a href="regularized-empirical-risk-minimization.html#exm:J">8.1</a></strong>, suppose <span class="math inline">\(f^*\)</span> solves the problem (it is a minimizer). Pick an arbitrary <span class="math inline">\(g \in Z\)</span> and consider the function: <span class="math inline">\(\phi_g(t):= J(f^* + tg)\)</span>, <span class="math inline">\(t \in \mathcal R\)</span>. Then <span class="math inline">\(\phi_g\)</span> has a minimum at <span class="math inline">\(t=0\)</span> (<span class="math inline">\(\phi&#39;_g(0) = 0\)</span>). Thus, we have:</p>
<p><span class="math display">\[
\begin{aligned}
J(f^* + tg) &amp;= \lambda \int_0^1 (f^{*\prime} + tg&#39;)^2 dx + \sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2\\
&amp;= \lambda \int_0^1 (f^{*\prime})^2 dx + 2 \lambda t \int_0^1 f^{*\prime} g&#39; dx + \lambda t^2 \int_0^1 (g&#39;)^2 dx + \sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2
\end{aligned}
\]</span></p>
<p>Then, <span class="math inline">\(0 = \phi&#39;_g(0) = 2 \lambda \int_0^1 f^{*\prime} g&#39; dx + 2\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i)\)</span>, which means that</p>
<p><span class="math display">\[
\lambda \int_0^1 f^{*\prime} g&#39; dx = -\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i)
\]</span></p>
<p>This has to be true <span class="math inline">\(\forall g \in Z\)</span>. These equations are called <em>Euler Lagrange Equations</em> or <em>First Order Optimality Conditions</em>.</p>
<p>Above goes from <span class="math inline">\(f^*\)</span> to First Order Optimality Conditions. Now consider the opposite situation:</p>
<p>Take <span class="math inline">\(g=g_1\)</span> where <span class="math inline">\(g_1(x):= \operatorname{min} \{x,x_1\}\)</span>. It follows that <span class="math inline">\(\lambda \int_0^{x_1} f^{*\prime} dx = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_1)\)</span>. As <span class="math inline">\(\int_0^{x_1} f^{*\prime} dx = f^*(x_1) - f^*(0) = f^*(x_1)\)</span>, we have:</p>
<p><span class="math display">\[
\lambda f^*(x_1) = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_1)
\]</span></p>
<p>Doing the same for <span class="math inline">\(j = 2, \cdots, n\)</span>, we obtain the system of equations:</p>
<p><span class="math display">\[
\lambda f^*(x_j) = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_j), ~j = 1, \cdots, n
\]</span></p>
<p>We can solve this system to get the values of <span class="math inline">\(f^*\)</span> at the points <span class="math inline">\(x_1, \cdots, x_n\)</span>. For the rest of <span class="math inline">\(f^*\)</span>, use linear interpolation for each interval and use a constant after <span class="math inline">\(x_n\)</span>:</p>
<ul>
<li><p>To see <span class="math inline">\(f^*\)</span> is linear in <span class="math inline">\([0,x_1]\)</span>, it is enough to show <span class="math inline">\((f^*)&#39;&#39; = 0\)</span> in the interval:<br />
Take <span class="math inline">\(g: [0,1] \to \mathcal R\)</span> s.t <span class="math inline">\(g(x)=0\)</span>, <span class="math inline">\(\forall x \geq x_1\)</span> and <span class="math inline">\(g(0) = 0\)</span>, <span class="math inline">\(g \in Z\)</span>. Then, we must have <span class="math inline">\(\int_0^{x_1} (f^*)&#39; g&#39; dx = 0\)</span>.<br />
As <span class="math inline">\(\int_0^{x_1}(f^*)&#39; g&#39; dx = -\int_0^{x_1}(f^*)&#39;&#39; g dx + (f^*)&#39; g |_0^{x_1} = -\int_0^{x_1}(f^*)&#39;&#39; g dx\)</span>, <span class="math inline">\(\forall g\)</span>. Thus, we have <span class="math inline">\((f^*)&#39;&#39; \equiv 0\)</span>.</p></li>
<li><p>To see <span class="math inline">\(f^*\)</span> is constant in <span class="math inline">\([x_n, 1]\)</span>:<br />
<span class="math display">\[
\begin{aligned}
J(f)&amp;= \lambda \int_0^1 (f&#39;(x))^2 dx + \sum_{i=1}^n (f(x_i) - y_i)^2\\
&amp;= \lambda \int_0^{x_n} (f&#39;(x))^2 dx + \lambda \int_{x_n}^1 (f&#39;(x))^2 dx + \sum_{i=1}^n (f(x_i) - y_i)^2
\end{aligned}
\]</span></p></li>
</ul>
<p>As <span class="math inline">\(J(f)\)</span> has a minimum when <span class="math inline">\(f&#39;(x) = 0\)</span> at <span class="math inline">\([x_n, 1]\)</span> and <span class="math inline">\(f^*\)</span> minimizes <span class="math inline">\(J(f)\)</span>, we have <span class="math inline">\(f^*\)</span> is constant in <span class="math inline">\([x_n, 1]\)</span>.</p>
</div>
<div id="erm-with-kernels" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> ERM with Kernels<a href="regularized-empirical-risk-minimization.html#erm-with-kernels" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(K\)</span> be a kernel over <span class="math inline">\(\mathcal X\)</span> and consider <span class="math inline">\(\mathcal H\)</span> the RKHS associated to <span class="math inline">\(K\)</span>. Consider the problem:</p>
<p><span class="math display">\[
\underset{f \in \mathcal H}{\operatorname{min}}\underset{\text{Regularizer}}{\lambda || f ||_\mathcal H^2} + \underset{\text{Fidality}}{\sum_{i=1}^n (y_i - f(x_i))^2}
\]</span></p>
<p>This is actually Ridge Regression in disguise: let <span class="math inline">\(\psi: \begin{matrix}\mathcal X \to \mathcal H\\x \to K(\cdot, x) \end{matrix}\)</span>, <span class="math inline">\(f \in \mathcal H\)</span>, then <span class="math inline">\(f(x_i) = &lt;f, K(\cdot, x_i)&gt;_\mathcal H = &lt;f, \psi(x_i)&gt;_\mathcal H\)</span>, which means we want to solve:</p>
<p><span class="math display">\[
\underset{f \in \mathcal H}{\operatorname{min}}\underset{\text{Regularizer}}{\lambda || f ||_\mathcal H^2} + \underset{\text{Fidality}}{\sum_{i=1}^n (y_i - &lt;f, \psi(x_i)&gt;_\mathcal H)^2}
\]</span>
Here, <span class="math inline">\(\phi_g(t) = \lambda || f^* + tg||^2_\mathcal H + \sum_{i=1}^n (f^*(x_i) - y_i)^2\)</span>, where <span class="math inline">\(|| f^* + tg||^2_\mathcal H = &lt;f^* + tg, f^* + tg&gt;_\mathcal H = &lt;f^*, f^*&gt;_\mathcal H + 2t&lt;f^*, g&gt;_\mathcal H + t^2 &lt;g,g&gt;_\mathcal H\)</span>.</p>
<div class="theorem">
<p><span id="thm:representer" class="theorem"><strong>Theorem 8.1  (Representer Theorem) </strong></span>After using the Euler-Lagrange equations with the reproducing property of the kernel, we will deduce:</p>
<p><span class="math display">\[
f^* = \sum_{i=1}^n a_i^* K(\cdot, x_i)
\]</span></p>
<p>Moreover, we can characterize the coefficient completely from <span class="math inline">\(y_1, \cdots, y_n\)</span> and the matrix <span class="math inline">\(K = \{K(x_i, x_j)\}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{a_1, \cdots, a_n}{\operatorname{min}} \lambda || \sum_{j=1}^n a_j K(\cdot, x_j)||^2_\mathcal H &amp;+  \sum_{i=1}^n(\sum_{j=1}^n a_j K(x_i, x_j) - y_i)^2\\
&amp;\Updownarrow\\
\underset{\vec a \in \mathcal R^n}{\operatorname{min}} \lambda &lt;K \vec a, \vec a&gt;_{\mathcal R^n} &amp;+  ||K \vec a - \vec y||^2\\
\end{aligned}
\]</span></p>
</div>
<div id="comparison" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Comparison<a href="regularized-empirical-risk-minimization.html#comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- ```{r RKHS,echo=F, fig.align='center', fig.pos="ht", out.width = "70%"} -->
<!-- knitr::include_graphics("./figures/RKHS.png") -->
<!-- ``` -->
<div class="example">
<p><span id="exm:unlabeled-div-40" class="example"><strong>Example 8.2  </strong></span>Go back to <strong>Example <a href="regularized-empirical-risk-minimization.html#exm:J">8.1</a></strong>, <span class="math inline">\(f^*\)</span> coincides with the solution to a problem of the form</p>
<p><span class="math display">\[
\underset{f \in \mathcal H}{\operatorname{min}} \lambda ||f||^2_\mathcal H + \sum_{i=1}^n (f(x_i) - y_i)^2
\]</span></p>
<p>where <span class="math inline">\(\mathcal H\)</span> is the RKHS for a well chosen kernel <span class="math inline">\(K\)</span>.</p>
<p>Let <span class="math inline">\(K: [0,1] \times [0,1] \to \mathcal R\)</span>, <span class="math inline">\(K(x, \tilde x) = x \wedge \tilde x\)</span>, by the representer theorem, the solution to the above problem takes the form:</p>
<p><span class="math display">\[
\tilde f := \sum_{i=1}^n a_i^* ~min\{\cdot, x_i\}
\]</span></p>
<p>which coincides with the solution <span class="math inline">\(f^*\)</span> to <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} J(f)\)</span> (linear within intervals).</p>
</div>
<hr />
<p>In general, we can consider the optimization problem:</p>
<p><span class="math display">\[
\underset{f \in \mathcal H}{\operatorname{min}} \lambda ||f||^2_\mathcal H + \sum_{i=1}^n G_i(f(x_i))
\]</span></p>
<p>where <span class="math inline">\(G: \mathcal R \to \mathcal R\)</span> is general depends on <span class="math inline">\(y_i\)</span>.</p>
<ul>
<li><p>For Regression: <span class="math inline">\(G_i(t) = (t - y_i)^2\)</span> (Usual Square Loss)<br />
But there are reasons to use things like <span class="math inline">\(G_i(t) = G(t - y_i)\)</span>, where <span class="math inline">\(G\)</span> is the Hubber Loss: <span class="math inline">\(G(t) = \begin{cases} t^2 &amp; t \in [-1,1]\\ |t| &amp; o.w.\end{cases}\)</span>.<br />
For example, to take care of outliers in the labels <span class="math inline">\(y_i\)</span>.</p></li>
<li><p>For Classification: <span class="math inline">\(y_i \in \{-1,1\}\)</span></p>
<ul>
<li><span class="math inline">\(G_i(t) = \log(\frac{exp(y_i-t)}{1 + exp(y_i-t)})\)</span>, problem above is then a regularized (non-parametric) logistic regression.<br />
</li>
<li><span class="math inline">\(G_i(t) = max\{0, 1 - ty_i\}\)</span>, problem above is then a kernelized soft margin SVM.</li>
</ul>
<p>Notice that when we solve the above problem for a classification problem, we get <span class="math inline">\(f^* :\mathcal X \to \mathcal R\)</span>, we still need to do<br />
<span class="math display">\[
l(x):=
\begin{cases}
1 &amp; f^*(x) &gt; 0\\
-1 &amp; f^*(x) &lt; 0
\end{cases}
\]</span></p></li>
</ul>
<div class="theorem">
<p><span id="thm:grt" class="theorem"><strong>Theorem 8.2  (Generalized Representer Theorem) </strong></span>Consider the problem</p>
<p><span class="math display">\[
\underset{f \in \mathcal H}{\operatorname{min}} \lambda ||f||^2_\mathcal H + \sum_{i=1}^n G_i(f(x_i)),
\]</span></p>
<p>the solution is <span class="math inline">\(f^* = \sum_{i=1}^n a_i^* K(\cdot, x_i)\)</span>, where <span class="math inline">\(a_i^*\)</span> are</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\vec a \in \mathcal R^n}{\operatorname{min}} \lambda &lt;K \vec a, \vec a&gt;_{\mathcal R^n} +  \sum_{i=1}^n G_i\Big (\sum_{j=1}^n a_j K(x_i, x_j)\Big)
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div id="regularity" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Regularity<a href="regularized-empirical-risk-minimization.html#regularity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a given kernel <span class="math inline">\(K\)</span> over <span class="math inline">\(\mathcal R^d\)</span>, its associated RKHS <span class="math inline">\(\mathcal H\)</span> is built using the function <span class="math inline">\(K(\cdot, x)\)</span>, <span class="math inline">\(x \in \mathcal R^d\)</span>. Thus, it is to be expected that the regularity of the kernel influences the regularity of the element in <span class="math inline">\(\mathcal H\)</span>. For example, if <span class="math inline">\(K(\cdot, \cdot)\)</span> has derivatives of all orders, it should not come as a surprise that the element in <span class="math inline">\(\mathcal H\)</span> have derivatives of all orders (This is the case for the Gaussian Kernel <span class="math inline">\(K(x, \tilde x) = exp(-\frac{|x - \tilde x|^2}{\sigma^2})\)</span>). Conversely, if the kernel <span class="math inline">\(K(x, \tilde x)\)</span> is not differentiable (<span class="math inline">\(K(x, \tilde x) = x \wedge \tilde x\)</span>), the function in <span class="math inline">\(\mathcal H\)</span> will not be very smooth either.</p>
<p>Also, notice that for the Gaussian Kernel , all functions <span class="math inline">\(K(\cdot, x)\)</span> have the same amount of regularity, this is because <span class="math inline">\(K\)</span> is <strong><em>homogeneous</em></strong>, i.e. <span class="math inline">\(K(x, \tilde x) = h(x - \tilde x)\)</span>. In some applications however, it may be useful to have a space of functions <span class="math inline">\(\mathcal H\)</span> with inhomogeneous regularity.</p>
<p>To change the set of examples of homogeneous kernels, we introduce the following theorem.</p>
<div class="theorem">
<p><span id="thm:Boch" class="theorem"><strong>Theorem 8.3  (Bochner's Theorem) </strong></span>(A simplified version that doesn’t involve complex random vectors)</p>
<p>Suppose that <span class="math inline">\(Z\)</span> is a d-dimensional random vector and that its characteristic function: <span class="math inline">\(h(\vec u):= E[e^{\textbf{i} &lt;Z, \vec u&gt;_{\mathcal R^d}}]\)</span>, <span class="math inline">\(\vec u \in \mathcal R^d\)</span> is real valued. Then, the function <span class="math inline">\(K: \mathcal R^d \times \mathcal R^d \to \mathcal R\)</span> defined by <span class="math inline">\(K(x, \tilde x) = h(x - \tilde x)\)</span> is a kernel.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-41" class="remark"><em>Remark</em>. </span></p>
<ul>
<li><p>Bochner’s Theorem includes a converse statement (If <span class="math inline">\(K\)</span> is a homogeneous kernel, then <span class="math inline">\(\exists r.v. Z)\)</span>, s.t. the above condition holds) and a more general version for complex valued random vectors.</p></li>
<li><p>One way to produce kernel: <span class="math inline">\(K(x, \tilde x) = E[e^{\textbf{i} &lt;Z, x - \tilde x&gt;}] \underset{M.C.S}{\approx} \frac{1}{N} \sum_{j=1}^n e^{\textbf{i} &lt;z_j, x - \tilde x&gt;}\)</span>.</p></li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-42" class="proof"><em>Proof</em>. </span>(Sketch)</p>
<ul>
<li><p>Symmetry: <span class="math inline">\(K(x, \tilde x) = K(\tilde x, x)\)</span>.<br />
We would need to show that <span class="math inline">\(h(\vec u) = h(-\vec u)\)</span>. However, if a characteristic function is real values, necessarily it has to be symmetric:<br />
<span class="math display">\[
\begin{aligned}
E[e^{\textbf{i} &lt;\vec u, Z&gt;}] &amp;= E[cos(&lt;\vec u, Z&gt;) + \textbf{i} ~sin(&lt;\vec u, Z&gt;)]\\
&amp;= E[cos(&lt;\vec u, Z&gt;)]\\
&amp;= E[cos(&lt;-\vec u, Z&gt;)]\\
&amp;= E[e^{\textbf{i} &lt;-\vec u, Z&gt;}]
\end{aligned}
\]</span></p></li>
<li><p>Positive Definiteness:<br />
<span class="math display">\[
\begin{aligned}
&lt;Kv ,v&gt;_{\mathcal R^d} &amp;= \sum_{i=1}^n \sum_{j=1}^n K(x_i, x_j) v_i v_j\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n E[e^{\textbf{i} &lt;x_i - x_j, Z&gt;}] v_i v_j\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n E[e^{\textbf{i} &lt;x_i, Z&gt;} e^{-\textbf{i} &lt;x_i, Z&gt;}] v_i v_j\\
&amp;= E\Big[(\sum_{i=1}^n v_ie^{\textbf{i} &lt;x_i, Z&gt;}) \cdot (\sum_{j=1}^n v_j e^{-\textbf{i} &lt;x_i, Z&gt;}) \Big]\\
&amp;:= E[T \cdot \bar T]\\
&amp;= E[|T|^2]\\
&amp;\underset{\text{conjugate of complex number}}{=}E\Big[|\sum_{j=1}^n v_j e^{\textbf{i} &lt;x_i, Z&gt;}|^2\Big]\\
&amp;\geq 0
\end{aligned}
\]</span></p></li>
</ul>
</div>
<p>Thanks to this theorem, we can show that the following <span class="math inline">\(K(x, \tilde x) = h(x - \tilde x)\)</span>, <span class="math inline">\(x, \tilde x \in \mathcal R^d\)</span> are kernels, where</p>
<div class="example">
<p><span id="exm:rqf" class="example"><strong>Example 8.3  (Rational Quadratic Family) </strong></span><span class="math display">\[
h(\vec u) = (1 + \frac{|\vec u|^2}{\alpha l^2})^{-\alpha}, ~\alpha, l &gt; 0
\]</span></p>
<p>When <span class="math inline">\(\alpha \to \infty\)</span>, we recover Gaussian Kernel.</p>
</div>
<div class="example">
<p><span id="exm:gammaef" class="example"><strong>Example 8.4  (Exponential Family) </strong></span><span class="math display">\[
h(\vec u) = exp(-\frac{|\vec u|^\gamma}{l^\gamma}), ~0 &lt; \gamma \leq 2, ~l &gt; 0
\]</span></p>
<p>When <span class="math inline">\(\gamma = 2\)</span>, we get Gaussian Kernel. Change <span class="math inline">\(\gamma\)</span>, we change the regularity.</p>
</div>
<div class="example">
<p><span id="exm:mf" class="example"><strong>Example 8.5  (Matern Family of Kernels) </strong></span><span class="math inline">\(\cdots\)</span></p>
</div>
<p>These are all examples of homogeneous kernels. Now let’s make inhomogeneous kernels from homogeneous ones.</p>
<p>Let <span class="math inline">\(\beta: \mathcal R^d \to \mathcal R^m\)</span> be a non-linear function and let <span class="math inline">\(\tilde K: \mathcal R^m \times \mathcal R^m \to \mathcal R\)</span> be as kernel. Then, <span class="math inline">\(K(x, \tilde x):= \tilde K(\beta(x), \beta(\tilde x))\)</span> is a kernel over <span class="math inline">\(\mathcal R^d\)</span>. In particular, if <span class="math inline">\(\tilde K(z, \tilde z) = h(z - \tilde z)\)</span> is one of the homogeneous kernels we considered before, <span class="math inline">\(K(x, \tilde x) = h(\beta(x) - \beta(\tilde x))\)</span> is a kernel which in general will not be homogeneous (<span class="math inline">\(h(\beta(x) - \beta(\tilde x)) \neq h(\beta(\tilde x) - \beta(x))\)</span>).</p>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 8.6  </strong></span>Let <span class="math inline">\(\beta(x): x \in \mathcal R \to (cos(x), sin(x))\)</span>. Take <span class="math inline">\(h(\vec u) = exp(-\frac{|\vec u|^2}{\sigma^2})\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
K(x, \tilde x) &amp;= exp(- \frac{[cos(x) - cos(\tilde x)]^2 + [sin(x) - sin(\tilde x)]^2}{\sigma^2})\\
&amp;= exp(-\frac{4sin^2(\frac{x - \tilde x}{2})}{\sigma^2})
\end{aligned}
\]</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="support-vector-machine.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="uncertainty-quantification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/07-RERM.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
