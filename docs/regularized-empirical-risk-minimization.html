<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regularized Empirical Risk Minimization | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="support-vector-machine.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hateta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat{\eta}_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-empirical-risk-minimization" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regularized Empirical Risk Minimization<a href="regularized-empirical-risk-minimization.html#regularized-empirical-risk-minimization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="ill-posed-problems" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> ill Posed Problems<a href="regularized-empirical-risk-minimization.html#ill-posed-problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a simple example: in linear regression, we try to find <span class="math inline">\(\beta \in \mathcal R^d\)</span> s.t <span class="math inline">\(\sum_{i=1}^n (y_i - &lt;\beta, x_i&gt;_{\mathcal R^d})^2\)</span> is minimized. The solution <span class="math inline">\(\beta^* = (X^T X)^{-1} X^T Y\)</span> only make sense when <span class="math inline">\((X^TX)^{-1}\)</span> is invertible, or the optimization problem has a unique solution. The ill posed problems are the situation where there is no unique solution or <span class="math inline">\((X^TX)^{-1}\)</span> is not invertible (<span class="math inline">\(d &gt;&gt; n\)</span>, not full rank, …).</p>
<p>Ridge regression aims at tackling this problem by adding a “regularizer” to the optimization problem: <span class="math inline">\(\underset{\beta}{\operatorname{min}} \lambda || \beta ||^2 + \sum_{i=1}^n (y_i - &lt;x_i, \beta&gt;)^2\)</span> with <span class="math inline">\(\beta^* = (X^T X + \lambda I)^{-1} X^T Y\)</span>, where higher <span class="math inline">\(\lambda\)</span> implies more “regular” and lower <span class="math inline">\(\lambda\)</span> implies lesser “regular”. Naturally, one needs to choose <span class="math inline">\(\lambda \in (0, \infty)\)</span> to balance between variance and bias.</p>
<p>In more generality, suppose we wanted to fit a function <span class="math inline">\(f\)</span> that explains some observed data (<span class="math inline">\(f: [0,1] \to \mathcal R\)</span>):</p>
<p><img src="figures/ridge-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We are trying to solve <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} \sum_{i=1}^n (f(x_i) - y_i)^2\)</span>. How many functions are close to the observations is the ill posed problem.</p>
<p>The idea in empirical risk minimization is to enforce “regularity” on the functions by adding a regularizer to the empirical risk object function: <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} \sum_{i=1}^n (f(x_i) - y_i)^2 + \lambda R(f)\)</span>, where <span class="math inline">\(Z\)</span> is some family of functions for which the objective function makes sense and <span class="math inline">\(R\)</span> is some regularizer.</p>
<div class="remark">
<p><span id="unlabeled-div-39" class="remark"><em>Remark</em>. </span>It is important to make sure that the objective function makes sense for elements <span class="math inline">\(f \in Z\)</span>. For example, <span class="math inline">\(Z\)</span> can not be <span class="math inline">\(\mathcal L^2(\mathcal R)\)</span>, since for an arbitrary <span class="math inline">\(f \in \mathcal L^2(\mathcal R)\)</span>, we can not make sense of <span class="math inline">\(f(x_i)\)</span>.</p>
</div>
<p>In any case, after choosing <span class="math inline">\(Z\)</span> and <span class="math inline">\(R\)</span>, we consider the solution to the regularized empirical risk minimization problem as a regression function for the data. Notice that this approach is a non-parametric approach (<span class="math inline">\(f \in Z\)</span>).</p>
<p>Q: What types of regularizers we could work with?</p>
<p>A: Later we will consider <span class="math inline">\(Z = \mathcal H\)</span> (RKHS) and <span class="math inline">\(R(f) = ||f||^2_\mathcal H\)</span>, but for now let us consider an unrelated setting:</p>
<div class="example">
<p><span id="exm:J" class="example"><strong>Example 8.1  </strong></span>Let <span class="math inline">\(\mathcal X = [0,1]\)</span> and the data <span class="math inline">\(0&lt;x_1&lt;\cdots&lt;x_n&lt;1\)</span> with corresponding values <span class="math inline">\(y_1,\cdots,y_n \in \mathcal R\)</span>. Let <span class="math inline">\(Z\)</span> be the set of functions <span class="math inline">\(f:[0,1] \to \mathcal R\)</span> s.t <span class="math inline">\(f(x) = \int_0^x f&#39;(t)dt\)</span>, <span class="math inline">\(\forall x \in (0,1)\)</span> and s.t <span class="math inline">\(\int_0^1 (f&#39;(x))^2 dx &lt; \infty\)</span>. Define <span class="math inline">\(C_1([0,1])\)</span> as <em>Absolutely Continuous Functions defined on [0,1]</em>. Then, <span class="math inline">\(C_1([0,1]) \subset Z\)</span> i.e. <span class="math inline">\(f \in C_1([0,1]) \text{ and } f(0) = 0\Rightarrow f\in Z\)</span>.</p>
<p>Now let <span class="math inline">\(J(f):= \lambda \int_0^1 (f&#39;(x))^2 dx (\text{ Regularization Term}) + \sum_{i=1}^n (f(x_i) - y_i)^2 (\text{Fidality Term})\)</span>. The goal is to:</p>
<ul>
<li>Minimize <span class="math inline">\(J\)</span>: <span class="math inline">\(\underset{f \in Z}{\operatorname{min}} J(f)\)</span>.</li>
<li>Obtain a “first order” condition for optimality.</li>
</ul>
</div>
<hr />
<p>In search of inspiration, let’s go back to the Euclidean case: <span class="math inline">\(\underset{x \in \mathcal R^m}{\operatorname{min}} F(x)\)</span>.</p>
<p>First order condition: <span class="math inline">\(\nabla F(x^*) = \vec 0\)</span>, that is <span class="math inline">\(\frac{\partial F}{\partial x_i}(x^*) = 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,m\)</span>.</p>
<p>Moreover, if <span class="math inline">\(x^*\)</span> minimizes <span class="math inline">\(F\)</span> over the whole <span class="math inline">\(\mathcal R^m\)</span>, it certainly minimizes <span class="math inline">\(F\)</span> over the line <span class="math inline">\(\{x^* + tv: t\in \mathcal R\}\)</span>. Thus, fix <span class="math inline">\(v\)</span> and define the function <span class="math inline">\(\phi_v(t):= F(x^* + tv)\)</span>. Then this function has a minimum at <span class="math inline">\(t=0\)</span>, which means that</p>
<p><span class="math display">\[
0 = \phi&#39;_v(0) = \frac{d}{dt} F(x^* + tv) |_{t=0} = &lt;\nabla F(x^*), v&gt; = \partial_v F(x^*)
\]</span></p>
<p>The optimality condition in <span class="math inline">\(\mathcal R^m\)</span> is equivalent to the optimality condition fro a collection of functions in <span class="math inline">\(\mathcal R\)</span>.</p>
<hr />
<p>So, going back to <strong>Example <a href="regularized-empirical-risk-minimization.html#exm:J">8.1</a></strong>, suppose <span class="math inline">\(f^*\)</span> solves the problem (it is a minimizer). Pick an arbitrary <span class="math inline">\(g \in Z\)</span> and consider the function: <span class="math inline">\(\phi_g(t):= J(f^* + tg)\)</span>, <span class="math inline">\(t \in \mathcal R\)</span>. Then <span class="math inline">\(\phi_g\)</span> has a minimum at <span class="math inline">\(t=0\)</span> (<span class="math inline">\(\phi&#39;_g(0) = 0\)</span>). Thus, we have:</p>
<p><span class="math display">\[
\begin{aligned}
J(f^* + tg) &amp;= \lambda \int_0^1 (f^{*\prime} + tg&#39;)^2 dx + \sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2\\
&amp;= \lambda \int_0^1 (f^{*\prime})^2 dx + 2 \lambda t \int_0^1 f^{*\prime} g&#39; dx + \lambda t^2 \int_0^1 (g&#39;)^2 dx + \sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2
\end{aligned}
\]</span></p>
<p>Then, <span class="math inline">\(0 = \phi&#39;_g(0) = 2 \lambda \int_0^1 f^{*\prime} g&#39; dx + 2\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i)\)</span>, which means that</p>
<p><span class="math display">\[
\lambda \int_0^1 f^{*\prime} g&#39; dx = -\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i)
\]</span></p>
<p>This has to be true <span class="math inline">\(\forall g \in Z\)</span>. These equations are called <em>Euler Lagrange Equations</em> or <em>First Order Optimality Conditions</em>.</p>
<p>Above goes from <span class="math inline">\(f^*\)</span> to First Order Optimality Conditions. Now consider the opposite situation:</p>
<p>Take <span class="math inline">\(g=g_1\)</span> where <span class="math inline">\(g_1(x):= \operatorname{min} \{x,x_1\}\)</span>. It follows that <span class="math inline">\(\lambda \int_0^{x_1} f^{*\prime} dx = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_1)\)</span>. As <span class="math inline">\(\int_0^{x_1} f^{*\prime} dx = f^*(x_1) - f^*(0) = f^*(x_1)\)</span>, we have:</p>
<p><span class="math display">\[
\lambda f^*(x_1) = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_1)
\]</span></p>
<p>Doing the same for <span class="math inline">\(j = 2, \cdots, n\)</span>, we obtain the system of equations:</p>
<p><span class="math display">\[
\lambda f^*(x_j) = -\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \wedge x_j), ~j = 1, \cdots, n
\]</span></p>
<p>We can solve this system to get the values of <span class="math inline">\(f^*\)</span> at the points <span class="math inline">\(x_1, \cdots, x_n\)</span>. For the rest of <span class="math inline">\(f^*\)</span>, use linear interpolation for each interval and use a constant after <span class="math inline">\(x_n\)</span>:</p>
<ul>
<li><p>To see <span class="math inline">\(f^*\)</span> is linear in <span class="math inline">\([0,x_1]\)</span>, it is enough to show <span class="math inline">\((f^*)&#39;&#39; = 0\)</span> in the interval:<br />
Take <span class="math inline">\(g: [0,1] \to \mathcal R\)</span> s.t <span class="math inline">\(g(x)=0\)</span>, <span class="math inline">\(\forall x \geq x_1\)</span> and <span class="math inline">\(g(0) = 0\)</span>, <span class="math inline">\(g \in Z\)</span>. Then, we must have <span class="math inline">\(\int_0^{x_1} (f^*)&#39; g&#39; dx = 0\)</span>.<br />
As <span class="math inline">\(\int_0^{x_1}(f^*)&#39; g&#39; dx = -\int_0^{x_1}(f^*)&#39;&#39; g dx + (f^*)&#39; g |_0^{x_1} = -\int_0^{x_1}(f^*)&#39;&#39; g dx\)</span>, <span class="math inline">\(\forall g\)</span>. Thus, we have <span class="math inline">\((f^*)&#39;&#39; \equiv 0\)</span>.</p></li>
<li><p>To see <span class="math inline">\(f^*\)</span> is constant in <span class="math inline">\([x_n, 1]\)</span>:<br />
<span class="math display">\[
\begin{aligned}
J(f)&amp;= \lambda \int_0^1 (f&#39;(x))^2 dx + \sum_{i=1}^n (f(x_i) - y_i)^2\\
&amp;= \lambda \int_0^{x_n} (f&#39;(x))^2 dx + \lambda \int_{x_n}^1 (f&#39;(x))^2 dx + \sum_{i=1}^n (f(x_i) - y_i)^2
\end{aligned}
\]</span></p></li>
</ul>
<p>As <span class="math inline">\(J(f)\)</span> has a minimum when <span class="math inline">\(f&#39;(x) = 0\)</span> at <span class="math inline">\([x_n, 1]\)</span> and <span class="math inline">\(f^*\)</span> minimizes <span class="math inline">\(J(f)\)</span>, we have <span class="math inline">\(f^*\)</span> is constant in <span class="math inline">\([x_n, 1]\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="support-vector-machine.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/07-RERM.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
