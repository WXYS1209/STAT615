<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Classification Problem &amp; Bayes Classifier | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Classification Problem &amp; Bayes Classifier | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Classification Problem &amp; Bayes Classifier | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="plug-in-classifiers-similarity-classifier.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hateta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat{\eta}_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-problem-bayes-classifier" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Classification Problem &amp; Bayes Classifier<a href="classification-problem-bayes-classifier.html#classification-problem-bayes-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="notation" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Notation<a href="classification-problem-bayes-classifier.html#notation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><span class="math inline">\(\mathcal X\)</span> = input space; <span class="math inline">\(\mathcal Y\)</span> = output space</p></li>
<li><p><span class="math inline">\(x\)</span> = feature vector, input, data point; <span class="math inline">\(y\)</span> = label</p></li>
<li><p><span class="math inline">\((X, Y)\)</span> = random variable.</p></li>
<li><p><strong>Supervised Learning</strong>: From given data set <span class="math inline">\((x_1,y_1), \cdots, (x_n,y_n)\)</span>, find <span class="math inline">\(\mathfrak f\)</span>, such that <span class="math inline">\(\mathfrak f: ~x (\in \mathcal X) \to y (\in \mathcal Y)\)</span>.<br />
<span class="math inline">\((x_1,y_1), \cdots, (x_n,y_n)\)</span> come from a distribution of input-output pairs.</p></li>
<li><p><strong>Distribution</strong>: <span class="math inline">\(\rho\)</span>, is a probability distribution over <span class="math inline">\(\mathcal X \times \mathcal Y\)</span>, the “Ground Truth”.</p></li>
</ul>
</div>
<div id="classification-problem" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Classification Problem<a href="classification-problem-bayes-classifier.html#classification-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given <span class="math inline">\(\mathfrak f : \mathcal X \to \mathcal Y\)</span>, compute <span class="math inline">\(P_{(X,Y) \sim \rho}(\mathfrak f(X) \neq Y)\)</span>, which is <strong>average misclassification error (AME)</strong>.</p>
<div id="goal" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Goal<a href="classification-problem-bayes-classifier.html#goal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To build the “best” possible classifier, that is find <span class="math inline">\(\mathfrak f\)</span> that makes the AME as small as possible.</p>
</div>
<div id="questions" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Questions<a href="classification-problem-bayes-classifier.html#questions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Does there exist a “best” classifier (relative to AME)?</p>
<div class="theorem">
<p><span id="thm:pyth" class="theorem"><strong>Theorem 2.1  (Bayes Classifier) </strong></span></p>
Given <span class="math inline">\(\rho\)</span> distribution over <span class="math inline">\(\mathcal X \times \mathcal Y\)</span>,
<ul>
<li><span class="math inline">\(l\)</span> = 0-1 loss;<br />
</li>
<li><span class="math inline">\(\mathcal Y\)</span> = {0,1} (binary problem);<br />
</li>
<li><span class="math inline">\(\mathfrak f_B(x)\)</span> := <span class="math inline">\(\begin{cases}1 &amp; if ~P_{(X,Y) \in \rho}(Y=1 \mid X=x) \geq P_{(X,Y) \in \rho}(Y=0 \mid X=x)\\0 &amp; o.w.\end{cases}\)</span>.</li>
</ul>
<p>Then <span class="math inline">\(\mathfrak f_B\)</span> minimizes the AME.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span><span class="math inline">\(\forall ~\mathfrak f: \mathcal X \to \{0,1\}\)</span>, we have<br />
<span class="math display">\[
\begin{aligned}
P(\mathfrak f(X) \neq Y) &amp;= E[1_{\mathfrak f(X) \neq Y}]\\
&amp;= E\Big[E[1_{\mathfrak f(X) \neq Y} \mid X]\Big]\\
&amp;= E\Big[1_{\mathfrak f(X) \neq 1} \cdot P(Y=1 \mid X) + 1_{\mathfrak f(X) \neq 0} \cdot P(Y=0 \mid X)\Big]\\
\end{aligned}
\]</span></p>
<ul>
<li>When <span class="math inline">\(P(Y=1 \mid X) \geq P(Y=0 \mid X)\)</span>, we have<br />
<span class="math display">\[
\begin{aligned}
1_{\mathfrak f(X) \neq 1} \cdot P(Y=1 \mid X) + 1_{\mathfrak f(X) \neq 0} \cdot P(Y=0 \mid X) &amp;\geq 1_{\mathfrak f(X) \neq 1} \cdot P(Y=0 \mid X) + 1_{\mathfrak f(X) \neq 0} \cdot P(Y=0 \mid X)\\
&amp;= P(Y=0 \mid X) \cdot (1_{\mathfrak f(X) \neq 1} + 1_{\mathfrak f(X) \neq 0})\\
&amp;= P(Y=0 \mid X)\\
&amp;=_{\mathfrak f_B(X) = 1} ~P(\mathfrak f_B(X) \neq Y \mid X)
\end{aligned}
\]</span></li>
<li>When <span class="math inline">\(P(Y=0 \mid X) &gt; P(Y=1 \mid X)\)</span>, we similarly have</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
1_{\mathfrak f(X) \neq 1} \cdot P(Y=1 \mid X) + 1_{\mathfrak f(X) \neq 0} \cdot P(Y=0 \mid X) &gt; P(\mathfrak f_B(X) \neq Y \mid X)
\end{aligned}
\]</span>
As a result, we have
<span class="math display">\[
\begin{aligned}
P(\mathfrak f(X) \neq Y) &amp;\geq E[P(\mathfrak f_B(X) \neq Y \mid X)]\\
&amp;= E\Big[E[1_{\mathfrak f_B(X) \neq Y} \mid X]\Big]\\
&amp;= P(\mathfrak f_B(X) \neq Y)
\end{aligned}
\]</span><br />
which means that <span class="math inline">\(\mathfrak f_B\)</span> minimizes the AME.</p>
</div></li>
<li><p>Can we construct this “best” classifier (if we only observe the data <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n)\)</span>)?</p></li>
<li><p>If we can not build this classifier from the observed data, then what can we do in that case?</p></li>
</ul>
</div>
<div id="the-0-1-loss" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> The 0-1 Loss<a href="classification-problem-bayes-classifier.html#the-0-1-loss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{aligned}l:\{1, \cdots, k\} \times \{1, \cdots, k\} &amp;\to \mathcal R\\(\mathcal Y, \mathcal Y&#39;) &amp;\to \mathcal R\end{aligned}; \quad l(y,y&#39;) = \begin{cases}1 &amp; if ~y\neq y&#39;\\0 &amp; if ~y = y&#39;\end{cases}.\]</span></p>
<p>Relative to this loss function, and relative to the distribution <span class="math inline">\(\rho\)</span>, we can define the <strong>risk</strong> of a given classifier <span class="math inline">\(\mathfrak f: \mathcal X \to \mathcal Y = \{1, \cdots, k\}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
R(\mathfrak f) :&amp;= E_{(X, Y) \in \rho}[l(\mathfrak f(X), Y)] \\
&amp;= P(\mathfrak f(X) \neq Y).
\end{aligned}
\]</span></p>
<p>To find the “best” classifier is to solve <span class="math inline">\(min_{\mathfrak f} R(\mathfrak f)\)</span>.</p>
<hr />
<div class="example">
<p><span id="exm:traffic" class="example"><strong>Example 2.1  </strong></span><span class="math inline">\(y_1\)</span> = stop sign, <span class="math inline">\(y_2\)</span> = 50 mph, <span class="math inline">\(y_3\)</span> = 40 mph. The classifier classifies <span class="math inline">\(\mathfrak f: x \to y_2\)</span>, when <span class="math inline">\((x,y)\)</span> was such that <span class="math inline">\(y\)</span> = stop sign. Then the 0-1 loss is <span class="math inline">\(l(\mathfrak f(x), y) = l(y_2, y_1) = 1\)</span>.</p>
<p>Predicting 50 mph, when <span class="math inline">\(y\)</span> = stop sign seems to be worse than predicting 40 mph, when <span class="math inline">\(y\)</span> = 50 mph. Thus other loss function may be better.</p>
</div>
<hr />
</div>
<div id="observations" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Observations<a href="classification-problem-bayes-classifier.html#observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><span class="math inline">\(\mathfrak f_B\)</span> depends on <span class="math inline">\(\rho\)</span>: if <span class="math inline">\((X,Y) \sim \rho&#39;\)</span>, you can not expect <span class="math inline">\(\mathfrak f_B\)</span> constructed from <span class="math inline">\(\rho\)</span> to do well at classifying <span class="math inline">\((X,Y)\)</span>.</p></li>
<li><p>Bayes Rule:<br />
<span class="math display">\[
P(Y=1 \mid X=x) = \frac{\rho_{X\mid Y=1}(x) \cdot P(Y=1)}{\rho_X(x)}
\]</span><br />
Thus,<br />
<span class="math display">\[\begin{aligned}P(Y=1 \mid X=x) \geq P(Y=0 \mid X=x) &amp;\Leftrightarrow \rho_{X\mid Y=1}(x) \cdot P(Y=1) \geq \rho_{X\mid Y=0}(x) \cdot P(Y=0)\\
&amp;\Leftrightarrow P(X=x, Y=1) \geq P(X=x, Y=0)\\
&amp;(\text{useful when the input space } \mathcal X \text{ is discrete})
\end{aligned}
\]</span></p></li>
<li><p>In general there are multiple solutions to the problem. However, all of them have the form of <span class="math inline">\(\mathfrak f_B\)</span>.</p></li>
<li><p><span class="math inline">\(R^*_B = min_{\mathfrak f} ~P(\mathfrak f(X) \neq Y) = P(\mathfrak f_B(X) \neq Y)\)</span> is <strong>Bayes Risk</strong>, which indicates how accurate the classifier is. Notice that regardless of what <span class="math inline">\(\rho\)</span> is, <span class="math inline">\(R^*_B \leq \frac{1}{2}\)</span>.</p></li>
<li><p>Both <span class="math inline">\(\mathfrak f_B\)</span> and <span class="math inline">\(R^*_B\)</span> depend on <span class="math inline">\(\rho\)</span>. But also if we were to change the loss function, the formula for <span class="math inline">\(\mathfrak f_B\)</span> and the value of <span class="math inline">\(R^*_B\)</span> would change.</p></li>
</ul>
</div>
<div id="examples" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Examples<a href="classification-problem-bayes-classifier.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="example">
<p><span id="exm:org" class="example"><strong>Example 2.2  </strong></span><span class="math inline">\(\mathcal X = \{a,b,c,d,e,f\};~ \mathcal Y = \{0,1\};~ \rho\)</span> is a distribution over <span class="math inline">\(\mathcal X \times \mathcal Y\)</span>.</p>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="right">a</th>
<th align="right">b</th>
<th align="right">c</th>
<th align="right">d</th>
<th align="right">e</th>
<th align="right">f</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.1</td>
<td align="right">0.2</td>
<td align="right">0.5</td>
<td align="right">0.3</td>
<td align="right">0.8</td>
<td align="right">0.9</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.9</td>
<td align="right">0.3</td>
<td align="right">0.1</td>
<td align="right">0.3</td>
<td align="right">0.1</td>
<td align="right">0.3</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(P(X=x, Y=y) = \frac{q(x,y)}{M}\)</span>, where <span class="math inline">\(q(\cdot, \cdot)\)</span> is element of the above matrix, and <span class="math inline">\(M\)</span> is the normalization constant.</p>
<p>According to the Bayes Rule, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_B(x) &amp;= \begin{cases}
1 &amp; if ~ P(X=x, Y=1) \geq P(X=x, Y=0)\\
0 &amp; o.w.
\end{cases}\\
&amp;= \begin{cases}
1 &amp;if ~ q(x,1) \geq q(x,0)\\
0 &amp; o.w.
\end{cases}
\end{aligned}
\]</span></p>
<p>Thus,</p>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="right">a</th>
<th align="right">b</th>
<th align="right">c</th>
<th align="right">d</th>
<th align="right">e</th>
<th align="right">f</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(\mathfrak f_B\)</span></td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1 (0 is also OK)</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<div class="example">
<p><span id="exm:gauss" class="example"><strong>Example 2.3  (Mixture of Gaussians Model) </strong></span><span class="math inline">\(\mathcal X = \mathcal R\)</span>; <span class="math inline">\(\mathcal Y = \{0,1\}\)</span>; <span class="math inline">\((X, Y) \sim \rho\)</span>, <span class="math inline">\(P(Y=1) = \omega_1\)</span>, <span class="math inline">\(P(Y=0) = \omega_0\)</span>; <span class="math inline">\(X \mid Y=1 \sim N(\mu_1, \sigma_1^2)\)</span>, <span class="math inline">\(X \mid Y=0 \sim N(\mu_0, \sigma_0^2)\)</span>.</p>
<p>According to the Bayes Rule, we have:</p>
<p><span class="math display">\[
\begin{aligned}
P(Y=1 \mid X=x) \geq P(Y=0 \mid X=x) &amp;\Leftrightarrow \frac{\rho_1(x) \cdot P(Y=1)}{\rho(x)} \geq \frac{\rho_0(x) \cdot P(Y=0)}{\rho(x)}\\
&amp;\Leftrightarrow \rho_1(x) \cdot \omega_1 \geq \rho_0(x) \cdot \omega_0\\
&amp;\Leftrightarrow \omega_1 \cdot \frac{1}{\sqrt{2\pi}\sigma_1} ~exp[-\frac{(x-\mu_1)^2}{2\sigma_1^2}] \geq \omega_0 \cdot \frac{1}{\sqrt{2\pi}\sigma_0} ~exp[-\frac{(x-\mu_0)^2}{2\sigma_0^2}]\\
&amp;\Leftrightarrow log(\frac{\omega_1}{\sigma_1}) - \frac{(x-\mu_1)^2}{2\sigma_1^2} \geq log(\frac{\omega_0}{\sigma_0}) - \frac{(x-\mu_0)^2}{2\sigma_0^2}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\rho_1(x) = P(X=x \mid Y=1)\)</span>, <span class="math inline">\(\rho_0(x) = P(X=x \mid Y=0)\)</span>, <span class="math inline">\(\rho(\cdot)\)</span> is the marginal density of <span class="math inline">\(X\)</span>.</p>
</div>
</div>
</div>
<div id="other-topics" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Other Topics<a href="classification-problem-bayes-classifier.html#other-topics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>“Weak Classifier” or “Probabilistic Classifier”</p>
<p><span class="math inline">\(\mathfrak g: \mathcal X \to [0,1]\)</span>.</p>
<p><span class="math inline">\(\mathfrak g(x)\)</span> = likelihood that we are going to label 1.</p>
<p><span class="math inline">\(min_{\mathfrak g} ~E[\mid \mathfrak g(x) - Y \mid]\)</span>.</p>
<div class="theorem">
<p><span id="thm:idk" class="theorem"><strong>Theorem 2.2  </strong></span>……</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="plug-in-classifiers-similarity-classifier.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
