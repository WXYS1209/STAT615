<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Uncertainty Quantification | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Uncertainty Quantification | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Uncertainty Quantification | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularized-empirical-risk-minimization.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hateta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat{\eta}_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#erm-with-kernels"><i class="fa fa-check"></i><b>8.2</b> ERM with Kernels</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#comparison"><i class="fa fa-check"></i><b>8.2.1</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#regularity"><i class="fa fa-check"></i><b>8.3</b> Regularity</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html"><i class="fa fa-check"></i><b>9</b> Uncertainty Quantification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#bayes-perspective"><i class="fa fa-check"></i><b>9.1</b> Bayes Perspective</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="uncertainty-quantification" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Uncertainty Quantification<a href="uncertainty-quantification.html#uncertainty-quantification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Imagine there is an unknown quantity <span class="math inline">\(u\)</span> that we can’t observe directly. However, we may obtain a collection of observations <span class="math inline">\(Y = (y_i, \cdots, y_p) \in \mathcal R^p\)</span>. Then the <strong>Forward Problem</strong> is <span class="math inline">\(\underset{\text{unknown}}{u \in Z} \to \underset{\text{observed}}{Y}\)</span>.</p>
<p>In a sense, when we talk about science, we are talking about use <span class="math inline">\(Y\)</span> to learn about <span class="math inline">\(u\)</span>. In practice, there are at least two “sources” of uncertainty associated to the picture: <span class="math inline">\(u \to Y\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(u\)</span> is unknown.</p></li>
<li><p>Observations are usually contaminated by some sort of noise so that in general <span class="math inline">\(Y\)</span> is not simply a function of the unknown <span class="math inline">\(u\)</span>. In statistics, we may model the “likelihood” or “probability” of observing <span class="math inline">\(Y\)</span> given a certain value of <span class="math inline">\(u\)</span>. This quantifies our uncertainty of the possible values <span class="math inline">\(Y\)</span> given the unknown <span class="math inline">\(u\)</span> through the probability mass function: <span class="math inline">\(P(Y = y, | u)\)</span> or through a density function: <span class="math inline">\(P(Y \in C | u) = \int_C p(y | u) dy\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="exm:ber" class="example"><strong>Example 9.1  </strong></span>Let <span class="math inline">\(u \in (0,1)\)</span> be the probability that the outcome of a certain biased coin flip is H. Suppose that this coin is flipped <span class="math inline">\(p\)</span> times and we get <span class="math inline">\(y_i = \begin{cases}1 &amp;i\text{-th is H}\\0 &amp; i\text{-th is T}\end{cases}\)</span>.</p>
<p>In this case, the likelihood is</p>
<p><span class="math display">\[
P(Y = y | u) = u^{\sum_{i=1}^p y_i} (1-u)^{p - \sum_{i=1}^p y_i}, ~y_i \in \{0,1\}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:gau" class="example"><strong>Example 9.2  </strong></span></p>
<ul>
<li><p>Regression: <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span> (Hidden regression function). <span class="math inline">\(y_i = u(x_i) + \varepsilon_i\)</span>, <span class="math inline">\(\varepsilon_i \sim N(0, \sigma^2)\)</span>.<br />
<span class="math display">\[
p(y|u) = \frac{1}{(\sqrt{2 \pi \sigma^2})^p} exp(-\frac{||y - (u(x_1), \cdots, u(x_n)||^2}{2\sigma^2})
\]</span></p></li>
<li><p>Classification with logistic model: Given <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span>, define: <span class="math inline">\(q_i = \frac{exp(u(x_i))}{1+exp(u(x_i))}\)</span> for <span class="math inline">\(x_1, \cdots, x_p\)</span>. Then, <span class="math inline">\(y_i = \begin{cases}1 &amp;\text{with prob } q_i\\0 &amp; \text{with prob } 1-q_i\end{cases}\)</span>. Thus,<br />
<span class="math display">\[
P(y | u) = \prod_{i=1}^p q_i^{y_i} (1- q_i)^{1-y_i} = \prod_{i=1}^p \frac{exp(y_iu(x_i))}{1+exp(u(x_i))}
\]</span></p></li>
</ul>
</div>
<div class="example">
<p><span id="exm:DE" class="example"><strong>Example 9.3  (Density Estimation) </strong></span>Let <span class="math inline">\(\mathcal X = \mathcal R^d\)</span> and suppose that <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span> is an unknown density function (<span class="math inline">\(u(x) \geq 0\)</span>, <span class="math inline">\(\forall x\)</span>, and <span class="math inline">\(\int_{\mathcal R^d} u(x) dx = 1\)</span>), <span class="math inline">\(y_1, \cdots, y_p \underset{i.i.d}{\sim} u\)</span>. Thus, the likelihood of <span class="math inline">\(Y=(y_1, \cdots, y_p)\)</span> is:<br />
<span class="math display">\[
p(y|u) = u(y_1) \cdots u(y_p)
\]</span></p>
</div>
<p>So far we have talked about the distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(u\)</span> (Forward Problem). This models the uncertainty of our “measurement” if we had access to <span class="math inline">\(u\)</span>. However, how do we learn <span class="math inline">\(u\)</span> from <span class="math inline">\(Y\)</span> (Inverse Problem)? Moreover, how do we quantify the uncertainty of the unknown <span class="math inline">\(u\)</span> before observing <span class="math inline">\(Y\)</span>, and after observing <span class="math inline">\(Y\)</span>?</p>
<div id="bayes-perspective" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Bayes Perspective<a href="uncertainty-quantification.html#bayes-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayes Rule: <span class="math inline">\(p(u | y) \propto p(y | u) \pi(u)\)</span>. For the moment, we consider the case where <span class="math inline">\(u \in \mathcal R^d\)</span>. Suppose <span class="math inline">\(\pi(u)\)</span> is then interpreted as a density in <span class="math inline">\(\mathcal R^d\)</span>. The posterior is usually also a density in <span class="math inline">\(\mathcal R^d\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 9.4  (Example <a href="uncertainty-quantification.html#exm:ber">9.1</a> Continued) </strong></span>Let <span class="math inline">\(u \in (0,1)\)</span>, the likelihood is <span class="math inline">\(Bernoulli(u):\)</span></p>
<p><span class="math display">\[
P(Y = y | u) = u^{\sum_{i=1}^p y_i} (1-u)^{p - \sum_{i=1}^p y_i}, ~y_i \in \{0,1\},
\]</span></p>
<p>the prior is <span class="math inline">\(Beta(\alpha, \beta)\)</span>:</p>
<p><span class="math display">\[
\pi(u) = \frac{u^{\alpha-1}(1-u)^{\beta-1}}{\Gamma(\alpha, \beta)}, ~\alpha, \beta &gt;0.
\]</span></p>
<p>Then the posterior is</p>
<p><span class="math display">\[
p(u | y) \propto p(y |u) \pi(u) \propto u^{\sum_{i=1}^p y_i + \alpha - 1} (1-u)^{p - \sum_{i=1}^p y_i + \beta - 1},
\]</span></p>
<p>that is, given <span class="math inline">\(Y=y\)</span>, <span class="math inline">\(u\)</span> is <span class="math inline">\(Beta(\sum_{i=1}^p y_i + \alpha, p - \sum_{i=1}^p y_i + \beta)\)</span></p>
</div>
<div class="example">
<p><span id="exm:gaucon" class="example"><strong>Example 9.5  (Example <a href="uncertainty-quantification.html#exm:gau">9.2</a> Continued) </strong></span>Let <span class="math inline">\(u \in \mathcal R^d\)</span> and <span class="math inline">\(x_1, \cdots, x_p \in \mathcal R^d\)</span>, the likelihood is:</p>
<p><span class="math display">\[
p(y|u) = \frac{1}{(\sqrt{2 \pi \sigma^2})^p} exp(-\frac{||y - (u(x_1), \cdots, u(x_n)||^2}{2\sigma^2}),
\]</span></p>
<p>the prior is</p>
<p><span class="math display">\[
\pi(u) = \frac{1}{(\sqrt{2 \pi \lambda^{-1}})^p} exp(-\frac{\lambda}{2} ||u||^2_{\mathcal R^d}), ~i.e. ~ u \sim N(\vec 0, \frac{1}{\lambda} I).
\]</span></p>
<p>Let <span class="math inline">\(X = (x_1, \cdots, x_p)^T_{p \times d}\)</span>. Then the posterior is</p>
<p><span class="math display">\[
\begin{aligned}
p(u | y) &amp;\propto p(y |u) \pi(u) \\
&amp;\propto exp \Big(&lt;\frac{X^Ty}{\sigma^2}, u&gt; - \frac{1}{2} &lt;(\lambda I + \frac{X^TX}{\sigma^2})u, u&gt;\Big)\\
&amp;= N(\mu, \Sigma),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mu = \frac{1}{\sigma^2} (\lambda I + \frac{1}{\sigma^2} (X^TX))^{-1} X^Ty\)</span> and <span class="math inline">\(\Sigma= (\lambda I + \frac{1}{\sigma^2} (X^TX))^{-1}\)</span>.</p>
</div>
<p>With the posterior, we can then compute the following:</p>
<ol style="list-style-type: decimal">
<li>Posterior Mean:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\hat u_{PM} = E[u_i | y] &amp;= \frac{\int_{\mathcal R^d} u_i p(y | u) \pi(u) du_1 \cdots du_d}{\int_{\mathcal R^d} p(y | u) \pi(u) du_1 \cdots du_d}\\
&amp;= \frac{\int_{\mathcal R^d} u_i p(y | u) \pi(u) du_1 \cdots du_d}{Z(y)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Z(y)\)</span> is the normalization constant.</p>
<p>Then, based on the observations (and model), we estimate the i-th coordinate of the unknown as <span class="math inline">\(E[u_i | y]\)</span>.</p>
<p>We could also estimate <span class="math inline">\(u_i^2 cos(u_j)\)</span> with: <span class="math inline">\(E[u_i^2 cos(u_j) | y] = \frac{\int_{\mathcal R^d} u_i^2cos(u_j) p(y | u) \pi(u) du_1 \cdots du_d}{Z(y)}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Posterior Covariance: <span class="math inline">\(Cov(u_i, u_j | y)\)</span> or <span class="math inline">\(Var(u_i|y) = E[u_i^2 | y] - (E[u|y])^2\)</span>. A measure of how certain we are about the estimation of <span class="math inline">\(u_i\)</span>.</p></li>
<li><p>Map (Maximum a posterior):</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
u^* &amp;= \underset{u}{\operatorname{arg max}} \pi(u) p(y | u)\\
&amp;= \underset{u}{\operatorname{arg max}} log(\pi(u)) + log(p(y | u))\\
&amp;= \underset{u}{\operatorname{arg min}} -log(\pi(u)) - log(p(y | u))
\end{aligned}
\]</span></p>
<p>Both 1 and 2 are based on being able to take expectation w.r.t. posterior. 3 on the other hand, is not related to expectation but rather to optimization.</p>
<div class="example">
<p><span id="exm:gauconcon" class="example"><strong>Example 9.6  (Example <a href="uncertainty-quantification.html#exm:gaucon">9.5</a> Continued) </strong></span>We have:</p>
<ul>
<li><p><span class="math inline">\(E[u_i |y] = \mu_i\)</span></p></li>
<li><p><span class="math inline">\(Cov(u_iu_j | y) = \Sigma_{ij}\)</span></p></li>
<li><p>The Map is determined by:<br />
<span class="math display">\[
\begin{aligned}
u^* &amp;= \underset{u}{\operatorname{arg min}} -log(p(u | y))\\
&amp;= \underset{u}{\operatorname{arg min}} -log(\pi(u)) - log(p(y | u))\\
&amp;= \underset{u}{\operatorname{arg min}} \frac{\lambda}{2} ||u||^2 + \frac{1}{\sigma^2} \sum_{j=1}^p (y_i - &lt;u, x_i&gt;)^2,
\end{aligned}
\]</span></p></li>
</ul>
<p>which is just the ridge regression.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remark</em>. </span><span class="math display">\[
u^* = \mu
\]</span></p>
<p>This can be seen by direct optimization or by noticing that the Map of a Gaussian random vector coincides with its mean vector.</p>
<p>The posterior can be interpreted as a different kind of regularization:</p>
<p><span class="math display">\[
p(u | y) \propto \underset{\text{Fidality}}{p(y | u)} ~\underset{\text{Regularizer}}{\pi(u)}
\]</span></p>
</div>
<p>Back to <strong>Example <a href="uncertainty-quantification.html#exm:gauconcon">9.6</a></strong>, when <span class="math inline">\(u | y\)</span> is <span class="math inline">\(N(\mu, \Sigma)\)</span>, we have formulas for <span class="math inline">\(E[u_i | y]\)</span> and <span class="math inline">\(Cov(u_i, u_j)\)</span>. However, something like <span class="math inline">\(E[u_1cos(u_2) exp(u_3) | y]\)</span> would be quite impossible to compute. Nevertheless, we can use Monte-Carlo to approximate this. The idea is that we know what the posterior is. Thus, we can try to sample <span class="math inline">\(u^1, \cdots, u^K \sim N_d(\mu, \Sigma)\)</span>. Then consider the empirical average: <span class="math inline">\(\frac{1}{K} \sum{j=1}^K u_1^j cos(u_2^j) exp(u_3^j)\)</span>. That is, if we can sample from the posterior, we can numerically approximate <span class="math inline">\(E[G(u) | y] \approx \frac{1}{K} \sum_{j=1}^K G(u^j)\)</span>, where <span class="math inline">\(u^1, \cdots, u^K \underset{i.i.d}{\sim} Posterior\)</span>.</p>
<p>If it is difficult to sample from the posterior, we can use MCMC (Monte Carlo Markov Chain).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularized-empirical-risk-minimization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/08-Bayes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
