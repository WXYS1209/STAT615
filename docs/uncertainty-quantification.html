<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Uncertainty Quantification | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Uncertainty Quantification | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Uncertainty Quantification | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularized-empirical-risk-minimization.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hat-eta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat \eta_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#erm-with-kernels"><i class="fa fa-check"></i><b>8.2</b> ERM with Kernels</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#comparison"><i class="fa fa-check"></i><b>8.2.1</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#regularity"><i class="fa fa-check"></i><b>8.3</b> Regularity</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html"><i class="fa fa-check"></i><b>9</b> Uncertainty Quantification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#bayes-perspective"><i class="fa fa-check"></i><b>9.1</b> Bayes Perspective</a></li>
<li class="chapter" data-level="9.2" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#gaussians-in-mathcal-rd"><i class="fa fa-check"></i><b>9.2</b> Gaussians in <span class="math inline">\(\mathcal R^d\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#diagonalization-of-sigma-and-sampling-for-n0-sigma"><i class="fa fa-check"></i><b>9.3</b> Diagonalization of <span class="math inline">\(\Sigma\)</span> and Sampling for <span class="math inline">\(N(0, \Sigma)\)</span></a></li>
<li class="chapter" data-level="9.4" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#stochastic-processes-and-gaussian-processes"><i class="fa fa-check"></i><b>9.4</b> Stochastic Processes and Gaussian Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="uncertainty-quantification" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Uncertainty Quantification<a href="uncertainty-quantification.html#uncertainty-quantification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Imagine there is an unknown quantity <span class="math inline">\(u\)</span> that we can’t observe directly. However, we may obtain a collection of observations <span class="math inline">\(Y = (y_i, \cdots, y_p) \in \mathcal R^p\)</span>. Then the <strong>Forward Problem</strong> is <span class="math inline">\(\underset{\text{unknown}}{u \in Z} \to \underset{\text{observed}}{Y}\)</span>.</p>
<p>In a sense, when we talk about science, we are talking about use <span class="math inline">\(Y\)</span> to learn about <span class="math inline">\(u\)</span>. In practice, there are at least two “sources” of uncertainty associated to the picture: <span class="math inline">\(u \to Y\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(u\)</span> is unknown.</p></li>
<li><p>Observations are usually contaminated by some sort of noise so that in general <span class="math inline">\(Y\)</span> is not simply a function of the unknown <span class="math inline">\(u\)</span>. In statistics, we may model the “likelihood” or “probability” of observing <span class="math inline">\(Y\)</span> given a certain value of <span class="math inline">\(u\)</span>. This quantifies our uncertainty of the possible values <span class="math inline">\(Y\)</span> given the unknown <span class="math inline">\(u\)</span> through the probability mass function: <span class="math inline">\(P(Y = y, | u)\)</span> or through a density function: <span class="math inline">\(P(Y \in C | u) = \int_C p(y | u) dy\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="exm:ber" class="example"><strong>Example 9.1  </strong></span>Let <span class="math inline">\(u \in (0,1)\)</span> be the probability that the outcome of a certain biased coin flip is H. Suppose that this coin is flipped <span class="math inline">\(p\)</span> times and we get <span class="math inline">\(y_i = \begin{cases}1 &amp;i\text{-th is H}\\0 &amp; i\text{-th is T}\end{cases}\)</span>.</p>
<p>In this case, the likelihood is</p>
<p><span class="math display">\[
P(Y = y | u) = u^{\sum_{i=1}^p y_i} (1-u)^{p - \sum_{i=1}^p y_i}, ~y_i \in \{0,1\}
\]</span></p>
</div>
<div class="example">
<p><span id="exm:gau" class="example"><strong>Example 9.2  </strong></span></p>
<ul>
<li><p>Regression: <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span> (Hidden regression function). <span class="math inline">\(y_i = u(x_i) + \varepsilon_i\)</span>, <span class="math inline">\(\varepsilon_i \sim N(0, \sigma^2)\)</span>.<br />
<span class="math display">\[
p(y|u) = \frac{1}{(\sqrt{2 \pi \sigma^2})^p} exp(-\frac{||y - (u(x_1), \cdots, u(x_n)||^2}{2\sigma^2})
\]</span></p></li>
<li><p>Classification with logistic model: Given <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span>, define: <span class="math inline">\(q_i = \frac{exp(u(x_i))}{1+exp(u(x_i))}\)</span> for <span class="math inline">\(x_1, \cdots, x_p\)</span>. Then, <span class="math inline">\(y_i = \begin{cases}1 &amp;\text{with prob } q_i\\0 &amp; \text{with prob } 1-q_i\end{cases}\)</span>. Thus,<br />
<span class="math display">\[
P(y | u) = \prod_{i=1}^p q_i^{y_i} (1- q_i)^{1-y_i} = \prod_{i=1}^p \frac{exp(y_iu(x_i))}{1+exp(u(x_i))}
\]</span></p></li>
</ul>
</div>
<div class="example">
<p><span id="exm:DE" class="example"><strong>Example 9.3  (Density Estimation) </strong></span>Let <span class="math inline">\(\mathcal X = \mathcal R^d\)</span> and suppose that <span class="math inline">\(u: \mathcal X \to \mathcal R\)</span> is an unknown density function (<span class="math inline">\(u(x) \geq 0\)</span>, <span class="math inline">\(\forall x\)</span>, and <span class="math inline">\(\int_{\mathcal R^d} u(x) dx = 1\)</span>), <span class="math inline">\(y_1, \cdots, y_p \underset{i.i.d}{\sim} u\)</span>. Thus, the likelihood of <span class="math inline">\(Y=(y_1, \cdots, y_p)\)</span> is:<br />
<span class="math display">\[
p(y|u) = u(y_1) \cdots u(y_p)
\]</span></p>
</div>
<p>So far we have talked about the distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(u\)</span> (Forward Problem). This models the uncertainty of our “measurement” if we had access to <span class="math inline">\(u\)</span>. However, how do we learn <span class="math inline">\(u\)</span> from <span class="math inline">\(Y\)</span> (Inverse Problem)? Moreover, how do we quantify the uncertainty of the unknown <span class="math inline">\(u\)</span> before observing <span class="math inline">\(Y\)</span>, and after observing <span class="math inline">\(Y\)</span>?</p>
<div id="bayes-perspective" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Bayes Perspective<a href="uncertainty-quantification.html#bayes-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayes Rule: <span class="math inline">\(p(u | y) \propto p(y | u) \pi(u)\)</span>. For the moment, we consider the case where <span class="math inline">\(u \in \mathcal R^d\)</span>. Suppose <span class="math inline">\(\pi(u)\)</span> is then interpreted as a density in <span class="math inline">\(\mathcal R^d\)</span>. The posterior is usually also a density in <span class="math inline">\(\mathcal R^d\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 9.4  (Example <a href="uncertainty-quantification.html#exm:ber">9.1</a> Continued) </strong></span>Let <span class="math inline">\(u \in (0,1)\)</span>, the likelihood is <span class="math inline">\(Bernoulli(u):\)</span></p>
<p><span class="math display">\[
P(Y = y | u) = u^{\sum_{i=1}^p y_i} (1-u)^{p - \sum_{i=1}^p y_i}, ~y_i \in \{0,1\},
\]</span></p>
<p>the prior is <span class="math inline">\(Beta(\alpha, \beta)\)</span>:</p>
<p><span class="math display">\[
\pi(u) = \frac{u^{\alpha-1}(1-u)^{\beta-1}}{\Gamma(\alpha, \beta)}, ~\alpha, \beta &gt;0.
\]</span></p>
<p>Then the posterior is</p>
<p><span class="math display">\[
p(u | y) \propto p(y |u) \pi(u) \propto u^{\sum_{i=1}^p y_i + \alpha - 1} (1-u)^{p - \sum_{i=1}^p y_i + \beta - 1},
\]</span></p>
<p>that is, given <span class="math inline">\(Y=y\)</span>, <span class="math inline">\(u\)</span> is <span class="math inline">\(Beta(\sum_{i=1}^p y_i + \alpha, p - \sum_{i=1}^p y_i + \beta)\)</span></p>
</div>
<div class="example">
<p><span id="exm:gaucon" class="example"><strong>Example 9.5  (Example <a href="uncertainty-quantification.html#exm:gau">9.2</a> Continued) </strong></span>Let <span class="math inline">\(u \in \mathcal R^d\)</span> and <span class="math inline">\(x_1, \cdots, x_p \in \mathcal R^d\)</span>, the likelihood is:</p>
<p><span class="math display">\[
p(y|u) = \frac{1}{(\sqrt{2 \pi \sigma^2})^p} exp(-\frac{||y - (u(x_1), \cdots, u(x_n)||^2}{2\sigma^2}),
\]</span></p>
<p>the prior is</p>
<p><span class="math display">\[
\pi(u) = \frac{1}{(\sqrt{2 \pi \lambda^{-1}})^p} exp(-\frac{\lambda}{2} ||u||^2_{\mathcal R^d}), ~i.e. ~ u \sim N(\vec 0, \frac{1}{\lambda} I).
\]</span></p>
<p>Let <span class="math inline">\(X = (x_1, \cdots, x_p)^T_{p \times d}\)</span>. Then the posterior is</p>
<p><span class="math display">\[
\begin{aligned}
p(u | y) &amp;\propto p(y |u) \pi(u) \\
&amp;\propto exp \Big(&lt;\frac{X^Ty}{\sigma^2}, u&gt; - \frac{1}{2} &lt;(\lambda I + \frac{X^TX}{\sigma^2})u, u&gt;\Big)\\
&amp;= N(\mu, \Sigma),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mu = \frac{1}{\sigma^2} (\lambda I + \frac{1}{\sigma^2} (X^TX))^{-1} X^Ty\)</span> and <span class="math inline">\(\Sigma= (\lambda I + \frac{1}{\sigma^2} (X^TX))^{-1}\)</span>.</p>
</div>
<p>With the posterior, we can then compute the following:</p>
<ol style="list-style-type: decimal">
<li>Posterior Mean:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\hat u_{PM} = E[u_i | y] &amp;= \frac{\int_{\mathcal R^d} u_i p(y | u) \pi(u) du_1 \cdots du_d}{\int_{\mathcal R^d} p(y | u) \pi(u) du_1 \cdots du_d}\\
&amp;= \frac{\int_{\mathcal R^d} u_i p(y | u) \pi(u) du_1 \cdots du_d}{Z(y)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Z(y)\)</span> is the normalization constant.</p>
<p>Then, based on the observations (and model), we estimate the i-th coordinate of the unknown as <span class="math inline">\(E[u_i | y]\)</span>.</p>
<p>We could also estimate <span class="math inline">\(u_i^2 cos(u_j)\)</span> with: <span class="math inline">\(E[u_i^2 cos(u_j) | y] = \frac{\int_{\mathcal R^d} u_i^2cos(u_j) p(y | u) \pi(u) du_1 \cdots du_d}{Z(y)}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Posterior Covariance: <span class="math inline">\(Cov(u_i, u_j | y)\)</span> or <span class="math inline">\(Var(u_i|y) = E[u_i^2 | y] - (E[u|y])^2\)</span>. A measure of how certain we are about the estimation of <span class="math inline">\(u_i\)</span>.</p></li>
<li><p>Map (Maximum a posterior):</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
u^* &amp;= \underset{u}{\operatorname{arg max}} \pi(u) p(y | u)\\
&amp;= \underset{u}{\operatorname{arg max}} log(\pi(u)) + log(p(y | u))\\
&amp;= \underset{u}{\operatorname{arg min}} -log(\pi(u)) - log(p(y | u))
\end{aligned}
\]</span></p>
<p>Both 1 and 2 are based on being able to take expectation w.r.t. posterior. 3 on the other hand, is not related to expectation but rather to optimization.</p>
<div class="example">
<p><span id="exm:gauconcon" class="example"><strong>Example 9.6  (Example <a href="uncertainty-quantification.html#exm:gaucon">9.5</a> Continued) </strong></span>We have:</p>
<ul>
<li><p><span class="math inline">\(E[u_i |y] = \mu_i\)</span></p></li>
<li><p><span class="math inline">\(Cov(u_iu_j | y) = \Sigma_{ij}\)</span></p></li>
<li><p>The Map is determined by:<br />
<span class="math display">\[
\begin{aligned}
u^* &amp;= \underset{u}{\operatorname{arg min}} -log(p(u | y))\\
&amp;= \underset{u}{\operatorname{arg min}} -log(\pi(u)) - log(p(y | u))\\
&amp;= \underset{u}{\operatorname{arg min}} \frac{\lambda}{2} ||u||^2 + \frac{1}{\sigma^2} \sum_{j=1}^p (y_i - &lt;u, x_i&gt;)^2,
\end{aligned}
\]</span></p></li>
</ul>
<p>which is just the ridge regression.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-45" class="remark"><em>Remark</em>. </span><span class="math display">\[
u^* = \mu
\]</span></p>
<p>This can be seen by direct optimization or by noticing that the Map of a Gaussian random vector coincides with its mean vector.</p>
<p>The posterior can be interpreted as a different kind of regularization:</p>
<p><span class="math display">\[
p(u | y) \propto \underset{\text{Fidality}}{p(y | u)} ~\underset{\text{Regularizer}}{\pi(u)}
\]</span></p>
</div>
<p>Back to <strong>Example <a href="uncertainty-quantification.html#exm:gauconcon">9.6</a></strong>, when <span class="math inline">\(u | y\)</span> is <span class="math inline">\(N(\mu, \Sigma)\)</span>, we have formulas for <span class="math inline">\(E[u_i | y]\)</span> and <span class="math inline">\(Cov(u_i, u_j)\)</span>. However, something like <span class="math inline">\(E[u_1cos(u_2) exp(u_3) | y]\)</span> would be quite impossible to compute. Nevertheless, we can use Monte-Carlo to approximate this. The idea is that we know what the posterior is. Thus, we can try to sample <span class="math inline">\(u^1, \cdots, u^K \sim N_d(\mu, \Sigma)\)</span>. Then consider the empirical average: <span class="math inline">\(\frac{1}{K} \sum{j=1}^K u_1^j cos(u_2^j) exp(u_3^j)\)</span>. That is, if we can sample from the posterior, we can numerically approximate <span class="math inline">\(E[G(u) | y] \approx \frac{1}{K} \sum_{j=1}^K G(u^j)\)</span>, where <span class="math inline">\(u^1, \cdots, u^K \underset{i.i.d}{\sim} Posterior\)</span>.</p>
<p>If it is difficult to sample from the posterior, we can use MCMC (Monte Carlo Markov Chain).</p>
</div>
<div id="gaussians-in-mathcal-rd" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Gaussians in <span class="math inline">\(\mathcal R^d\)</span><a href="uncertainty-quantification.html#gaussians-in-mathcal-rd" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 9.1  </strong></span>A Gaussian random vector <span class="math inline">\(Z\)</span> with mean zero and covariance <span class="math inline">\(\Sigma \in \mathcal R_{d\times d}\)</span> (positive definite matrix) is a random vector with density (in <span class="math inline">\(\mathcal R^d\)</span>):</p>
<p><span class="math display">\[
p(z) = \frac{1}{\sqrt{(2 \pi)^d ~det(\Sigma)}} \cdot exp(-\frac{1}{2} &lt; \Sigma^{-1} z, z&gt;),
\]</span></p>
<p>and we write <span class="math inline">\(Z \sim N(0, \Sigma)\)</span>.</p>
</div>
<p>We can characterize this distribution via its characteristic function: <span class="math inline">\(\phi_Z(v):= E[e^{\textbf{i} &lt;Z, v&gt;}]_{V \in \mathcal R^d} = e^{-\frac{1}{2} &lt;\Sigma v, v&gt;}\)</span>.</p>
<p>Notice that we can see <span class="math inline">\(Z\)</span> as a collection of real valued random variables: <span class="math inline">\(Z = (Z_1, \cdots, Z_d)\)</span>, namely the coordinates of <span class="math inline">\(Z\)</span>. For convenience let’s write <span class="math inline">\(Z\)</span> in the following form: <span class="math inline">\(\{Z_x\}_{x \in \mathcal X}\)</span>, where <span class="math inline">\(\mathcal X\)</span> is the set <span class="math inline">\(\mathcal X = \{1, \cdots, d\}\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-47" class="proposition"><strong>Proposition 9.1  </strong></span>Let <span class="math inline">\(\mathcal X = \{1, \cdots, d\}\)</span>, <span class="math inline">\(\{Z_x\}_{x \in \mathcal X}\)</span> a collection of real valued random variables is a Gaussian iff <span class="math inline">\(\forall\)</span> (finite) subset <span class="math inline">\(\mathcal X&#39; \subseteq \mathcal X\)</span>, the collection <span class="math inline">\(\{Z_x\}_{x \in \mathcal X&#39;}\)</span> is a Gaussian.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-48" class="proof"><em>Proof</em>. </span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>”: It is obvious since we can pick <span class="math inline">\(\mathcal X&#39; = \mathcal X\)</span>.</p>
<p>“<span class="math inline">\(\Rightarrow\)</span>”: For simplicity, suppose <span class="math inline">\(\mathcal X&#39; = \{1, \cdots, m\}\)</span>, <span class="math inline">\(m \leq d\)</span>. Let <span class="math inline">\(\tilde Z = \{Z_x\}_{x \in \mathcal X&#39;}\)</span>, <span class="math inline">\(w \in \mathcal R^m\)</span>. Then we have:</p>
<p><span class="math display">\[
\begin{aligned}
E[e^{\textbf{i} &lt;\tilde Z, w&gt;_{\mathcal R^m}}] &amp;= E[e^{\textbf{i} &lt;\tilde Z,uw&gt;_{\mathcal R^m}}]\\
&amp;(u = (w_1, \cdots, w_m, 0, \cdots, 0))\\
&amp;= e^{-\frac{1}{2} &lt; \Sigma u, u &gt;_{\mathcal R^d}}\\
&amp;= e^{-\frac{1}{2} &lt; \tilde \Sigma w, w &gt;_{\mathcal R^m}},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\tilde \Sigma \in \mathcal R^{m \times m}\)</span> and <span class="math inline">\(\tilde \Sigma_{ij} = \Sigma_{ij}\)</span>, <span class="math inline">\(i,j = 1,\cdots,m\)</span>.</p>
<p>Since the characteristic function of <span class="math inline">\(\tilde Z\)</span> is that of a Gaussian in <span class="math inline">\(\mathcal R^m\)</span>, we have <span class="math inline">\(\tilde Z \sim N_m(0, \tilde \Sigma)\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-49" class="proposition"><strong>Proposition 9.2  </strong></span>Let <span class="math inline">\(Z\)</span> be a Gaussian random vector in <span class="math inline">\(\mathcal R^d: Z \sim N_d(0, \Sigma)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(A \in \mathcal R^{m \times d}\)</span> is a matrix, then <span class="math inline">\(U = AZ\)</span> is a Gaussian in <span class="math inline">\(\mathcal R^m\)</span> and <span class="math inline">\(U \sim N_m(0, A\Sigma A^T)\)</span>.</p></li>
<li><p>If <span class="math inline">\(Z \sim N_d(0, \Sigma)\)</span>, <span class="math inline">\(\tilde Z \sim N_m(0, \tilde \Sigma)\)</span>, and <span class="math inline">\(Z, \tilde Z\)</span> are independent, then <span class="math inline">\(Z, \tilde Z \sim N_{d+m}(0, \left(\begin{matrix}\Sigma&amp;0\\0&amp;\tilde \Sigma\end{matrix}\right))\)</span>.</p></li>
<li><p><span class="math inline">\(Z\)</span> is a Gaussian in <span class="math inline">\(\mathcal R^d\)</span> iff every linear combination of its coordinates is univariate (单元的) Gaussian:</p></li>
</ol>
<p><span class="math display">\[
Z = (Z_1, \cdots, Z_d) \sim N_d \Leftrightarrow \forall a_1, \cdots, a_d \in \mathcal R, ~\sum_{j=1}^d a_j Z_j \sim N_1.
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(Z \sim N_d(0, \Sigma)\)</span>, and let <span class="math inline">\(v, \tilde v \in \mathcal R^d\)</span> be two vectors, then</li>
</ol>
<p><span class="math display">\[
Cov(&lt;Z, v&gt;, &lt;Z, \tilde v&gt;) = &lt;\Sigma v, \tilde v&gt; = &lt;\Sigma \tilde v, v&gt;
\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(Z = (Z_1, \cdots, Z_d)\)</span> is Gaussian with cov. matrix <span class="math inline">\(\Sigma\)</span>, then the coordinates of <span class="math inline">\(Z\)</span> are independent random variables iff <span class="math inline">\(\Sigma\)</span> is a diagonal matrix.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-50" class="proof"><em>Proof</em>. </span></p>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
E[e^{\textbf{i}&lt;w,U&gt;_{\mathcal R^m}}] &amp;= E[e^{\textbf{i}&lt;w,AZ&gt;_{\mathcal R^m}}]\\
&amp;\underset{&lt;A,B&gt; = tr(A^TB)}{=} E[e^{\textbf{i}&lt;A^Tw,Z&gt;_{\mathcal R^m}}]\\
&amp;= e^{-\frac{1}{2} &lt; \Sigma (A^Tw), A^Tw&gt;_{\mathcal R^d}}\\
&amp;= e^{-\frac{1}{2} &lt; A \Sigma A^Tw, w&gt;_{\mathcal R^d}}
\end{aligned}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>“<span class="math inline">\(\Rightarrow\)</span>”: Let <span class="math inline">\(U = \sum_{j=1}^d a_j Z_j\)</span> be a linear combination of the coordinates of <span class="math inline">\(Z\)</span>. By 1, we have <span class="math inline">\(U=AZ\)</span>, where <span class="math inline">\(A = (a_1, \cdots, a_d)\)</span>, is a Gaussian.</li>
</ol>
<p>“<span class="math inline">\(\Leftarrow\)</span>”: Let <span class="math inline">\(a \in \mathcal R^d\)</span>, with <span class="math inline">\(a = (a_1, \cdots, a_d)\)</span>. Then, <span class="math inline">\(E[e^{\textbf{i}&lt;a,Z&gt;}] = E[e^{\textbf{i} \sum_{j=1}^d a_j Z_j}] \underset{\sum_{j=1}^d a_j Z_j \sim N(0, \sigma_a^2)}{=} e^{-\frac{1}{2} \sigma_a^2}\)</span>, where <span class="math inline">\(\sigma_a^2 = Var(\sum_{j=1}^d a_j Z_j) = &lt;\Sigma a, a&gt;_{\mathcal R^d}\)</span>, <span class="math inline">\(\Sigma\)</span> is the covariance matrix of <span class="math inline">\(Z = (Z_1, \cdots, Z_d)\)</span>. Thus, <span class="math inline">\(E[e^{\textbf{i}&lt;a,Z&gt;}] = e^{-\frac{1}{2} &lt;\Sigma a, a&gt;_{\mathcal R^d}}\)</span>, <span class="math inline">\(\forall a \in \mathcal R^d\)</span>, which means that <span class="math inline">\(Z \sim N_d\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
Cov(&lt;Z, v&gt;, &lt;Z, \tilde v&gt;) &amp;= Cov(\sum_{i=1}^n Z_i v_i, \sum_{j=1}^n Z_j, \tilde v_j)\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n v_i \tilde v_j Cov(Z_i, Z_j)\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n (\Sigma_{ij}v_i \tilde v_j)\\
&amp;= &lt;\Sigma v, \tilde v&gt; = &lt;\Sigma \tilde v, v&gt;.
\end{aligned}
\]</span></p>
</div>
</div>
<div id="diagonalization-of-sigma-and-sampling-for-n0-sigma" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Diagonalization of <span class="math inline">\(\Sigma\)</span> and Sampling for <span class="math inline">\(N(0, \Sigma)\)</span><a href="uncertainty-quantification.html#diagonalization-of-sigma-and-sampling-for-n0-sigma" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-51" class="theorem"><strong>Theorem 9.1  </strong></span>If <span class="math inline">\(\Sigma \in \mathcal R^{d \times d}\)</span> is symmetric positive semi definite, then there are vectors <span class="math inline">\(v_1, \cdots, v_d \in \mathcal R^d\)</span> and non-negative <span class="math inline">\(\lambda_1 \geq \cdots \geq \lambda_d \geq 0\)</span> s.t <span class="math inline">\(\Sigma v_i = \lambda_i vi_i\)</span> and <span class="math inline">\(&lt;v_i, v_j&gt;_{\mathcal R^d} = \begin{cases}1 &amp; i=j\\0 &amp; i \neq j\end{cases}\)</span>.</p>
<p>In short: <span class="math inline">\(\Sigma\)</span> induces an orthonormal basis for <span class="math inline">\(\mathcal R^d\)</span> formed of eigen vectors of <span class="math inline">\(\Sigma\)</span>. Moreover, all of <span class="math inline">\(\Sigma\)</span>’s eigen values are non-negative.</p>
</div>
<ul>
<li>How to sample from <span class="math inline">\(Z \sim N_d(0, \Sigma)\)</span>:<br />
Since <span class="math inline">\(Z \in \mathcal R^d\)</span>, we notice that <span class="math inline">\(Z = \sum_{i=1}^d &lt;Z, v_i&gt;_{\mathcal R^d} v_i\)</span>, where <span class="math inline">\(\{v_i\}\)</span> is an orthonormal basis for <span class="math inline">\(\mathcal R^d\)</span> and are fixed. We then have <span class="math inline">\(&lt;Z, v_i&gt; \sim N_1(0, &lt;\Sigma v_i, v_i&gt;) = N_1(0, \lambda_i &lt;v_i, v_i&gt;) = N_1(0, \lambda_i)\)</span>.<br />
Moreover, for <span class="math inline">\(i \neq j\)</span>:<br />
<span class="math display">\[
\begin{aligned}
Cov(&lt;Z, v_i&gt;, &lt;Z, v_j&gt;) &amp;= &lt;\Sigma v_i, v_j&gt;\\
&amp;= &lt;\lambda_i v_i, v_j&gt;\\
&amp;= \lambda &lt;v_i, v_j&gt;\\
&amp;= 0
\end{aligned}
\]</span><br />
which means that the random variables <span class="math inline">\(&lt;Z, v_1&gt;,\cdots, &lt;Z,v_d&gt;\)</span> are independent.<br />
So, we can write: <span class="math inline">\(Z = \sum_{i=1}^d \sqrt{\lambda_i}(\frac{&lt;Z, v_i&gt;}{\sqrt{\lambda_i}}) v_i\ = \sum_{i=1}^d \sqrt{\lambda_i}\xi_i v_i\)</span>, where <span class="math inline">\(\xi_1, \cdots, \xi_d\)</span> are i.i.d <span class="math inline">\(N(0,1)\)</span> random variables.</li>
</ul>
<p>Thus, to sample from <span class="math inline">\(N_d(0, \Sigma)\)</span>, we can do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Find the eigen pairs <span class="math inline">\(\{(\lambda_i, v_i)\}_{i=1,\cdots,d}\)</span> for <span class="math inline">\(\Sigma\)</span>.</p></li>
<li><p>Sample <span class="math inline">\(\xi_1, \cdots, \xi_d \sim N(0,1)\)</span>.</p></li>
<li><p>Set <span class="math inline">\(Z = \sum_{i=1}^d \sqrt{\lambda_i}\xi_i v_i\)</span>.</p></li>
</ol>
<div class="remark">
<p><span id="unlabeled-div-52" class="remark"><em>Remark</em>. </span>The above is precisely what we need to sample form Gaussian in more general settings. In particular, for Gaussians in certain RKHSs.</p>
</div>
</div>
<div id="stochastic-processes-and-gaussian-processes" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Stochastic Processes and Gaussian Processes<a href="uncertainty-quantification.html#stochastic-processes-and-gaussian-processes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:sp" class="definition"><strong>Definition 9.2  (Stochastic Processes) </strong></span>Let <span class="math inline">\(\mathcal X\)</span> be a set. A stochastic process over <span class="math inline">\(\mathcal X\)</span> is a collection of random variables <span class="math inline">\(\{Z_x\}_{x \in \mathcal X}\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:gp" class="definition"><strong>Definition 9.3  (Gaussian Processes) </strong></span>A Gaussian Process over <span class="math inline">\(\mathcal X\)</span> is a stochastic process <span class="math inline">\(\{Z_x\}_{x \in \mathcal X}\)</span> with the property that <span class="math inline">\(\forall\)</span> finite subset <span class="math inline">\(\mathcal X&#39;\)</span> of <span class="math inline">\(\mathcal X\)</span>, the vector <span class="math inline">\(\{Z_x\}_{x \in \mathcal X&#39;}\)</span> is a Gaussian.</p>
</div>
<div class="definition">
<p><span id="def:cf" class="definition"><strong>Definition 9.4  (Covariance Funciton) </strong></span>Suppose <span class="math inline">\(Z = \{Z_x\}_{x \in \mathcal X}\)</span> is a Gaussian Process over <span class="math inline">\(\mathcal X\)</span>. We define <span class="math inline">\(Z\)</span>’s covariance function as:</p>
<p><span class="math display">\[
K: \mathcal X \times \mathcal X \to \mathcal R\\
K(x, \tilde x):= Cov(Z_x, Z_{\tilde x})
\]</span></p>
<p>We will write <span class="math inline">\(Z \sim N(0, K)\)</span> for simplicity.</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-53" class="proposition"><strong>Proposition 9.3  </strong></span>The covariance function of a Gaussian process over <span class="math inline">\(\mathcal X\)</span> is a kernel over <span class="math inline">\(\mathcal X\)</span>:</p>
<p><span class="math inline">\(x_1, \cdots, x_n \in \mathcal X\)</span>, <span class="math inline">\(\{Z_{x_1}, \cdots, Z_{x_n}\}\)</span> is a n-dimensional Gaussian. Thus <span class="math inline">\(\Sigma\)</span> is positive semi definite and <span class="math inline">\(\Sigma_{ij} = K(x_i, x_j)\)</span>.</p>
</div>
<p>Let <span class="math inline">\(K: \mathcal R^d \times \mathcal R^d \to \mathcal R\)</span> be a continuous kernel, for which <span class="math inline">\(|K(x, \tilde x)| \leq M\)</span>, where <span class="math inline">\(M\)</span> is a constant. Let <span class="math inline">\(p: \mathcal R^d \to \mathcal R\)</span> be a density function.</p>
<p>Consider the following operator:</p>
<p><span class="math display">\[
T_K: f \in \mathcal L^2(p) \to \int_{\mathcal R^d} K(\cdot, x) f(x) dx,
\]</span></p>
<p>where <span class="math inline">\(\mathcal L^2(p) = \{f: \int_{\mathcal R^d} f^2 p(x)dx &lt; \infty\}\)</span>. In particular, for <span class="math inline">\(f \in \mathcal L^2(p)\)</span>, <span class="math inline">\(T_K f\)</span> is a function on <span class="math inline">\(\mathcal R^d\)</span>, defined by <span class="math inline">\(T_K f(\tilde x):= \int_{\mathcal R^d} K(\tilde x, x) f(x) p(x) dx\)</span>, <span class="math inline">\(\tilde x \in \mathcal R^d\)</span>.</p>
<div class="definition">
<p><span id="def:ep" class="definition"><strong>Definition 9.5  (Eigen Values and Eigen Functions) </strong></span><span class="math inline">\(\lambda \in \mathcal R\)</span> is said to be an eigen value of <span class="math inline">\(T_K\)</span> if <span class="math inline">\(\exists f \in \mathcal L^2(p)\)</span>, s.t <span class="math inline">\(T_Kf = \lambda f\)</span>.</p>
<p>In other words, if <span class="math inline">\(\lambda f(\cdot) = \int_{\mathcal R^d} K(\cdot, \tilde x) f(\tilde x) p(\tilde x) d \tilde x\)</span>, we call <span class="math inline">\((\lambda, f)\)</span> and eigen pair.</p>
</div>
<div class="theorem">
<p><span id="thm:mt" class="theorem"><strong>Theorem 9.2  (Mercer's Theorem) </strong></span>There exists a sequence of eigen pairs <span class="math inline">\(\{(\lambda_k, \phi_k)\}_{k \in \mathcal N}\)</span> of <span class="math inline">\(T_K\)</span> s.t</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(&lt;\phi_k, \phi_l)&gt;_{\mathcal L^2(p)} = \int_{\mathcal R^d} \phi_k(x) \phi_l(x) p(x) dx = \begin{cases}1 &amp;k=l\\0&amp;o.w.\end{cases}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal L^2(p) = \{\sum_{i=1}^\infty a_i \phi_i: \sum_{i=1}^\infty a_i^2 &lt; \infty\}\)</span> and <span class="math inline">\(f \in \mathcal L^2(p)\)</span> can be represented as <span class="math inline">\(\sum_{i=1}^\infty &lt;f, \phi_i&gt;_{\mathcal L^2(p)} \phi_i\)</span> and the inner product can be written as <span class="math inline">\(&lt;f,g&gt;_{\mathcal L^2(p)} = \sum_{i=1}^\infty &lt;f, \phi_i&gt;_{\mathcal L^2(p)} &lt; g, \phi_i&gt;_{\mathcal L^2(p)}\)</span>.</p></li>
<li><p>The numbers <span class="math inline">\(\lambda_1, \lambda_2, \cdots\)</span> satisfy <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \cdots \geq 0\)</span> and <span class="math inline">\(\lambda_n \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p></li>
<li><p>The kernel <span class="math inline">\(K\)</span> can be written as <span class="math inline">\(K(x, \tilde x) = \sum_{i=1}^\infty \lambda_i \phi_i(x) \phi_i(\tilde x)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-54" class="proof"><em>Proof</em>. </span></p>
<ol start="4" style="list-style-type: decimal">
<li>First, note that <span class="math inline">\(K(\cdot, x) \in \mathcal L^2(p)\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\int_{\mathcal R^d} (K(\tilde x, x))^2 p(\tilde x) d\tilde x &amp;\leq \int_{\mathcal R^d} [\sqrt{K(\tilde x, \tilde x)} \sqrt{K(x,x)}]^2 p(\tilde x) d\tilde x\\
&amp;= K(x,x) \int_{\mathcal R^d} K(\tilde x, \tilde x) p(\tilde x) d\tilde x\\
&amp;&lt; \infty
\end{aligned}
\]</span></p>
<p>In particular, <span class="math inline">\(K(\cdot, x) = \sum_{i=1}^\infty &lt;K(\cdot,x), \phi_i&gt;_{\mathcal L^2(p)} \phi_i(\cdot)\)</span>. Thus, <span class="math inline">\(K(\tilde x, x) = \sum_{i=1}^\infty &lt;K(\cdot,x), \phi_i&gt;_{\mathcal L^2(p)} \phi_i(\tilde x)\)</span>. We just need to show that <span class="math inline">\(&lt;K(\cdot,x), \phi_i&gt;_{\mathcal L^2(p)} = \lambda_i \phi_i(x)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
&lt;K(\cdot,x), \phi_i&gt;_{\mathcal L^2(p)} &amp;= \int_{\mathcal R^d} K(\tilde x, x) \phi_i(\tilde x) p(\tilde x) d \tilde x\\
&amp;= T_K \phi_i(x)\\
&amp;= \lambda_i \phi_i(x)
\end{aligned}
\]</span></p>
</div>
<ul>
<li>How to define a Gaussian Process over <span class="math inline">\(\mathcal X\)</span> with covariance function <span class="math inline">\(K\)</span>? How to do sampling?<br />
Assuming <span class="math inline">\(K\)</span> satisfies the condition for Mercer’s Theorem for some density <span class="math inline">\(p\)</span>, we consider the eigen pairs <span class="math inline">\(\{(\lambda_i, \phi_i)\}\)</span> and do the following:</li>
</ul>
<p>Let <span class="math inline">\(\xi_1, \cdots\)</span> be i.i.d <span class="math inline">\(N(0,1)\)</span> random variables. We define a random function: <span class="math inline">\(Z = \sum_{i=1}^\infty \sqrt{\lambda_i} \xi_i \phi_i\)</span>. In particular, <span class="math inline">\(Z(x) = \sum_{i=1}^\infty \sqrt{\lambda_i} \xi_i \phi_i(x_i)\)</span>.</p>
<p>It follows that,</p>
<p><span class="math display">\[
\begin{aligned}
Cov(Z(x), Z(\tilde x)) &amp;= E[\sum_{i=1}^\infty \sum_{j=1}^\infty \sqrt{\lambda_i} \sqrt{\lambda_j} \xi_1 \xi_j \phi_i(\tilde x)\phi_i(x)]\\
&amp;= \sum_{i=1}^\infty \sum_{j=1}^\infty \sqrt{\lambda_i} \sqrt{\lambda_j} \phi_i(\tilde x)\phi_i(x)E[\xi_1 \xi_j]\\
&amp;\underset{E[\xi_i \xi_j] = 1_{i=j}}{=} \sum_{i=1}^\infty\lambda_i \phi_i(x) \phi_i(\tilde x)\\
&amp;= K(x, \tilde x)
\end{aligned}
\]</span></p>
<div class="theorem">
<p><span id="thm:kle" class="theorem"><strong>Theorem 9.3  (Karhunen-Loewe Expansion) </strong></span>The representation: <span class="math inline">\(Z = \sum_{i=1}^\infty \sqrt{\lambda_i} \xi_i \phi_i\)</span> for <span class="math inline">\(Z \sim N(0, K)\)</span> is known as Karhunen-Loewe Expansion.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-55" class="example"><strong>Example 9.7  </strong></span>Let <span class="math inline">\(\mathcal X = [0,1]\)</span>, <span class="math inline">\(K(s,t) = min\{s,t\} - st\)</span> is a kernel. Consider <span class="math inline">\(p(t) = 1\)</span>, <span class="math inline">\(t \in [0,1]\)</span>. Consider the operator: <span class="math inline">\(T_K: f \in \mathcal L^2(p) \to T_Kf \in \mathcal L^2(p)\)</span>, where <span class="math inline">\(T_K f(\cdot) = \int_0^1 K(\cdot, s) f(s) ds\)</span>.</p>
<p>Let us try to find the eigen pairs of <span class="math inline">\(T_K\)</span>. That is,</p>
<p><span class="math display">\[
\begin{aligned}
\lambda \phi(t) &amp;= \int_0^1 K(t,s) \phi(s) ds\\
&amp;= \int_0^1 (min\{s,t\} - st) \phi(s) ds\\
&amp;= \int_0^1 min\{s,t\} \phi(s) ds - t\int_0^1 s \phi(s) ds\\
&amp;= \int_0^t s\phi(s) ds + t\int_t^1 \phi(s) ds - t\int_0^1 s \phi(s) ds\\
&amp;:= \int_0^t s\phi(s) ds + t\int_t^1 \phi(s) ds - tC
\end{aligned}
\]</span></p>
<p>Take derivative w.r.t. <span class="math inline">\(t\)</span> to get</p>
<p><span class="math display">\[
\begin{aligned}
\lambda \phi&#39;(t) &amp;= t\phi(t) + [\int_t^1 \phi(s) ds - t\phi(t)] - C\\
&amp; = \int_t^1 \phi(s) ds - C
\end{aligned}
\]</span></p>
<p>Take derivative w.t.t. <span class="math inline">\(t\)</span> again to derive:</p>
<p><span class="math display">\[
\lambda \phi&#39;&#39;(t) = - \phi(t)
\]</span></p>
<p>For reasons that will soon become apparent (<span class="math inline">\(\phi \in \mathcal H\)</span>), we must have:</p>
<p><span class="math display">\[
\phi(0) = 0 = \phi(1)
\]</span></p>
<p>Thus, we have the equation:</p>
<p><span class="math display">\[
\begin{cases}
\lambda \phi&#39;&#39;(t) + \phi(t) = 0, ~\forall t \in (0,1)\\
\phi(0) = 0\\
\phi(1) = 0
\end{cases}
\]</span></p>
<p>Thus, <span class="math inline">\(\phi\)</span> has to take the form:</p>
<p><span class="math display">\[\phi(t) = A cos(\frac{t}{\sqrt{\lambda}}) + B sin(\frac{t}{\sqrt \lambda})\]</span></p>
<p>However, we need to make sure <span class="math inline">\(\phi(0) = 0 = \phi(1)\)</span>, which means that <span class="math inline">\(0 = \phi(0) = A\)</span>, and <span class="math inline">\(0 = \phi(1) = B sin(\frac{1}{\sqrt \lambda})\)</span>, which forces <span class="math inline">\(\frac{1}{\sqrt \lambda}\)</span> to be a multiple of <span class="math inline">\(\pi\)</span>.</p>
<p>In other words: <span class="math inline">\(\frac{1}{\sqrt \lambda} = k\pi\)</span>, i.e. <span class="math inline">\(\lambda = \frac{1}{k^2 \pi^2}\)</span>, <span class="math inline">\(k \in \mathcal N\)</span>.</p>
<p>Thus, we have</p>
<p><span class="math display">\[
\begin{cases}
\lambda_k = \frac{1}{k^2 \pi^2}\\
\phi_k = C_k sin (k\pi t)
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(C_k\)</span> satisfies <span class="math inline">\(\int_0^1 \phi^2(x) dx = 1\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-56" class="remark"><em>Remark</em>. </span>Let’s go back to the setting of Mercer’s Theorem:</p>
<ul>
<li><p><span class="math inline">\(K\)</span> a continuous, bounded kernel.</p></li>
<li><p><span class="math inline">\(p\)</span> a density.</p></li>
<li><p><span class="math inline">\(\{(\lambda_k, \phi_k)\}_{k \in \mathcal N}\)</span> are eigen pairs of <span class="math inline">\(T_K\)</span>.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathcal L^2(p)\)</span> can be alternatively written as: <span class="math inline">\(\mathcal L^2(p) = \{\sum_{i=1}^\infty : \sum_{i=1}^\infty a_i^2 &lt; \infty\}\)</span>.</p></li>
<li><p>The RKHS <span class="math inline">\(\mathcal H\)</span> of <span class="math inline">\(K\)</span> is defined as</p></li>
</ol>
<p><span class="math display">\[
\mathcal H := \{\sum_{i=1}^\infty b_i K(\cdot, x_i): \{x_i\} \subseteq \mathcal X, \{b_i\} \subseteq \mathcal R, \text{ s.t } \sum_{j=1}^\infty \sum_{i=1}^\infty K(x_i, x_j) b_i b_j &lt; \infty\}
\]</span></p>
<p>As</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^\infty b_i K(\cdot, x_i) &amp;= \sum_{i=1}^\infty b_i \sum_{k=1}^\infty \lambda_k \phi_k(\cdot) \phi_k(x_i)\\
&amp;= \sum_{k=1}^\infty \lambda_k (\sum_{i=1}^\infty b_i\phi_k(x_i)) \phi_k(\cdot)\\
&amp;= \sum_{k=1}^\infty &lt;\sum_{i=1}^\infty b_i K(\cdot, x_i), \phi_k&gt;_{\mathcal L^2(p)} \phi_k(\cdot)\\
&amp;:= \sum_{k=1}^\infty a_k \phi_k(\cdot),
\end{aligned}
\]</span></p>
<p>we can write it as:</p>
<p><span class="math display">\[\mathcal H := \{\sum_{k=1}^\infty a_k \phi_k(\cdot): \{x_i\} \subseteq \mathcal X, \{b_i\} \subseteq \mathcal R, \text{ s.t } \sum_{i=1}^\infty \frac{a_i^2}{\lambda_i} &lt; \infty.\]</span></p>
<p>In particular,</p>
<ul>
<li><p><span class="math inline">\(\mathcal H \subseteq \mathcal L^2(p)\)</span></p></li>
<li><p><span class="math inline">\(\phi_i \in \mathcal H\)</span>, <span class="math inline">\(\forall i\)</span>.</p></li>
<li><p><span class="math inline">\(Z = \sum_{i=1}^\infty \sqrt{\lambda_i} \xi_i \phi_i \notin \mathcal H\)</span>, although it does belong to <span class="math inline">\(\mathcal L^2(p)\)</span>.</p></li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularized-empirical-risk-minimization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/08-Bayes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
