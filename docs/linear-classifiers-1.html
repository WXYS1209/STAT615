<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Linear Classifiers | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Linear Classifiers | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Linear Classifiers | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-classifiers.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hateta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat{\eta}_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#hard-margin-svm"><i class="fa fa-check"></i><b>6.3.2</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="6.3.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#soft-margin-svm"><i class="fa fa-check"></i><b>6.3.3</b> Soft Margin SVM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html"><i class="fa fa-check"></i><b>7</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#lda-or-qda-1"><i class="fa fa-check"></i><b>7.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="7.2" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#logistic-regression-1"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.3" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#perceptrons-and-svms-1"><i class="fa fa-check"></i><b>7.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#perceptrons-1"><i class="fa fa-check"></i><b>7.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#hard-margin-svm-1"><i class="fa fa-check"></i><b>7.3.2</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.3.3" data-path="linear-classifiers-1.html"><a href="linear-classifiers-1.html#soft-margin-svm-1"><i class="fa fa-check"></i><b>7.3.3</b> Soft Margin SVM</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-classifiers-1" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Linear Classifiers<a href="linear-classifiers-1.html#linear-classifiers-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="math inline">\(\mathcal F = \{\text{all linear classifiers}\}\)</span>, <span class="math inline">\(\mathcal X = R^d\)</span>, <span class="math inline">\(\mathcal Y = \{-1,1\}\)</span> or <span class="math inline">\(\{1,\cdots,K\}\)</span>. Consider mainly on binary case.</p>
<p>Let <span class="math inline">\(\beta \in R^d\)</span> and <span class="math inline">\(\beta_0 \in R\)</span>, <span class="math inline">\(\mathcal H_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 = 0\}\)</span> (hyperplane), <span class="math inline">\(\mathcal H^+_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 \geq 0\}\)</span> and <span class="math inline">\(\mathcal H^-_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 &lt; 0\}\)</span> (half plane).</p>
<p><span class="math display">\[
\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}
\]</span></p>
<ul>
<li>Question: How to find a linear classifier based on <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>.<br />
A: Three different ways to tune <span class="math inline">\(\beta, \beta_0\)</span> from data: LDA (linear discriminant analysis) or QDA (quadratic discriminant analysis), Logistic Regression, Perceptrons and SVMs.</li>
</ul>
<div id="lda-or-qda-1" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> LDA or QDA<a href="linear-classifiers-1.html#lda-or-qda-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\((X,Y) \sim \rho\)</span>, where <span class="math inline">\(P(Y=k) = \omega_k, \rho_{X|Y}(X | Y=k) = N(\mu_k, \Sigma_k)\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes}(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \frac{\rho_{X|Y}(x | Y=k) \cdot \omega_k}{\rho_X(x)}\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [\rho_{X|Y}(x | Y=k) \cdot \omega_k]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [log\Big (\rho_{X|Y}(x | Y=k)\Big ) + log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [-log\Big (\frac{1}{(2\pi)^{d/2} (det(\Sigma_k))^{1/2}} \Big ) + \frac{1}{2} &lt;\Sigma_k^{-1} (x-\mu_k), (x-\mu_k)&gt; -  log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} \delta_k(x)
\end{aligned}
\]</span></p>
<p>Observation: <span class="math inline">\(\delta_k(x)\)</span> is quadratic and convex in <span class="math inline">\(x\)</span>.</p>
<p><strong>QDA</strong>: What if we only have <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate <span class="math inline">\(\mu_k, \omega_k, \Sigma_k\)</span>, <span class="math inline">\(k=1,\cdots, K\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 7.1  </strong></span><span class="math display">\[
\hat \mu_k := \frac{\sum_{i ~s.t. ~y_i=k} x_i}{\#\{x_i ~s.t. ~y_i=k\} ~(:= N_k)}
\]</span></p>
<p><span class="math display">\[
(\hat \Sigma_k)_{lm} := (\frac{1}{N_k} \sum_{i ~s.t. ~y_i=k} x_{il}x_{im} - \hat \mu_{kl} \hat \mu_{km}), \text{ where } l=1,\cdots,d, ~m=1,\cdots,d.
\]</span></p>
<p><span class="math display">\[
\hat \omega_k = \frac{N_k}{n}
\]</span></p>
<p><span class="math display">\[
\hat \delta_k(x) \text{ same as } \delta_k \text{ but with } \hat{} \text{ everywhere}
\]</span></p>
</div>
<p><strong>LDA</strong>: What if we had assumed <span class="math inline">\(\Sigma_1 = \Sigma_2 = \cdots = \Sigma_K = \Sigma\)</span>?</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [\frac{1}{2} &lt;\Sigma^{-1}x, x&gt; + &lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [&lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} l_k(x), ~(l_k(x) \text{ is linear})
\end{aligned}
\]</span></p>
<p>We can estimate <span class="math inline">\(\mu_k ,\omega_k\)</span> by <span class="math inline">\(\hat \mu_k ,\hat \omega_k\)</span>, and <span class="math inline">\(\Sigma\)</span> with the full data set.</p>
</div>
<div id="logistic-regression-1" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Logistic Regression<a href="linear-classifiers-1.html#logistic-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\(\vec \beta_k \in R^d\)</span>, <span class="math inline">\(\beta_{0k} \in R\)</span>, and <span class="math inline">\((X,Y)\)</span> satisfies: <span class="math inline">\(P(Y=k | X=x) = \frac{exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, <span class="math inline">\(P(Y=K | X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Let <span class="math inline">\(\varphi_k(x) := exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})\)</span> and <span class="math inline">\(\varphi_K(x):=1\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Then we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes} (x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \varphi_k(x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} log(\varphi_k(x))
\end{aligned}
\]</span></p>
<p>What if we only have observed <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate the parameters.</p>
<div class="example">
<p><span id="exm:mle" class="example"><strong>Example 7.2  (MLE) </strong></span>Given the data, find the best parameters (the ones maximizing the likelihood of the observations), i.e.</p>
<p><span class="math display">\[
\{(\vec \beta_k^*, \beta_{0k}^*)\} = \underset{\{(\vec \beta_k, \beta_{0k})\}_{k=1,\cdots,K-1}}{\operatorname{max}} \prod_{i=1}^n P(Y = y_i | X = x_i)
\]</span></p>
</div>
</div>
<div id="perceptrons-and-svms-1" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Perceptrons and SVMs<a href="linear-classifiers-1.html#perceptrons-and-svms-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{-1, 1\}\)</span>,
<span class="math inline">\((x_i, y_i)_{i=1,\cdots,n}\)</span>,
<span class="math inline">\((\vec \beta, \beta_0)\)</span>,
<span class="math inline">\(\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}\)</span>,
<span class="math inline">\(\sigma(\vec \beta, \beta_0) := \sum_{i \in \mathcal M_{\vec \beta, \beta_0}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span>,
where <span class="math inline">\(\mathcal M_{\vec \beta, \beta_0} = \{i \text{ s.t. } \mathfrak f_{\vec \beta, \beta_0} (x_i) \neq y_i\}\)</span></p>
<div id="perceptrons-1" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Perceptrons<a href="linear-classifiers-1.html#perceptrons-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\((\vec \beta^*, \beta_0^*) := \underset{(\vec \beta, \beta_{0})}{\operatorname{min}} \sigma(\vec \beta, \beta_0)\)</span>, then the perceptron classifier is <span class="math inline">\(\mathfrak f_{\vec \beta^*, \beta_0^*} (x)\)</span>. There exist many solutions of perceptron problems but some hyperplanes seem to be more robust:</p>
<div class="example">
<p><span id="exm:perceptron" class="example"><strong>Example 7.3  </strong></span><img src="_main_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Hyperplane 2 seems to be more robust.</p>
</div>
</div>
<div id="hard-margin-svm-1" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Hard Margin SVM<a href="linear-classifiers-1.html#hard-margin-svm-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s suppose that <span class="math inline">\((x_i, y_i)_{i=1,\cdots,n}\)</span> is linearly separable (for motivation for now). Then there exists at least one <span class="math inline">\(\vec \beta, \beta_0\)</span> s.t. <span class="math inline">\(\mathcal M_{\vec \beta, \beta_0} = \emptyset\)</span>. What we want is to find</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\vec \beta, \beta_{0})}{\operatorname{max}} &amp;~margin(\vec \beta, \beta_0)\\
s.t. &amp;~\mathcal M_{\vec \beta, \beta_0} = \emptyset
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(margin(\vec \beta, \beta_0):= min\{C^+_{\vec \beta, \beta_0}, C^-_{\vec \beta, \beta_0}\}\)</span>,
<span class="math inline">\(C^+_{\vec \beta, \beta_0} = \underset{x_i \text{ s.t. } y_i = 1}{\operatorname{min}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span> and
<span class="math inline">\(C^-_{\vec \beta, \beta_0} = \underset{x_i \text{ s.t. } y_i = -1}{\operatorname{min}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span>.</p>
<p>Let <span class="math inline">\(\tilde \beta = \frac{\vec \beta}{|| \vec \beta ||}\)</span>, <span class="math inline">\(\tilde \beta_0 = \frac{\beta_0}{|| \vec \beta ||}\)</span>, where <span class="math inline">\(||\tilde \beta || = 1\)</span>. Then we have <span class="math inline">\(\mathcal H_{\tilde \beta, \tilde \beta_0} = \mathcal H_{\vec \beta, \beta_0}\)</span>, <span class="math inline">\(\mathcal H^+_{\tilde \beta, \tilde \beta_0} = \mathcal H^+_{\vec \beta, \beta_0}\)</span>, and <span class="math inline">\(\mathcal H^-_{\tilde \beta, \tilde \beta_0} = \mathcal H^-_{\vec \beta, \beta_0}\)</span>. Thus, the <em>Geometric Formulation of SVM</em> is:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\vec \beta, \beta_{0})}{\operatorname{max}} &amp;~margin(\tilde \beta, \tilde \beta_0)\\
s.t. &amp;~\mathcal M_{\tilde \beta, \tilde \beta_0} = \emptyset\\
&amp;~||\tilde \beta|| = 1
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(dist(x, \mathcal H_{\tilde \beta, \tilde \beta_0}) = | &lt;x, \tilde \beta&gt; + \tilde \beta_0|\)</span>,
we have <span class="math inline">\(margin(\tilde \beta, \tilde \beta_0) = \underset{i=1.\cdots,n}{\operatorname{min}} | &lt;x_i, \tilde \beta&gt; + \tilde \beta_0|\)</span>.</p>
<p>As <span class="math inline">\(\mathcal M_{\tilde \beta, \tilde \beta_0} = \emptyset\)</span>,
we have <span class="math inline">\(sign(&lt;x_i, \tilde \beta&gt; + \tilde \beta_0) = y_i\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>Then, <span class="math inline">\(y_i (&lt;x_i, \tilde \beta&gt; + \tilde \beta_0) \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span>.</p>
<p>Thus, <span class="math inline">\(| &lt;x_i, \tilde \beta&gt; + \tilde \beta_0| = y_i (&lt;x_i, \tilde \beta&gt; + \tilde \beta_0)\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>As a result, the margin becomes:</p>
<p><span class="math display">\[
\begin{aligned}
m &amp;= \underset{i=1.\cdots,n}{\operatorname{min}} y_i(&lt;x_i, \tilde \beta&gt; + \tilde \beta_0)\\
&amp;\qquad \qquad \Updownarrow\\
1 &amp;= \underset{i=1.\cdots,n}{\operatorname{min}} y_i(&lt;x_i, \frac{\tilde \beta}{m}&gt; + \frac{\tilde \beta_0}{m})
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\beta = \frac{\tilde \beta}{m}\)</span>, <span class="math inline">\(\beta_0 = \frac{\tilde \beta_0}{m}\)</span>,
then we also have <span class="math inline">\(1 \leq y_i(&lt;x_i, \beta&gt; + \beta_0)\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>As <span class="math inline">\(|| \beta || = || \frac{\tilde \beta}{m} || = \frac{1}{m}\)</span>,
we have <span class="math inline">\(m = \frac{1}{|| \beta ||}\)</span>.</p>
<p>Thus, the geometric formulation of SVM becomes:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\beta, \beta_{0})}{\operatorname{max}} &amp;~\frac{1}{|| \beta ||}\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n\\
&amp;\qquad \qquad \Updownarrow\\
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~|| \beta ||\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n\\
&amp;\qquad \qquad \Updownarrow\\
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~|| \beta ||^2\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>which is the <em>Convex Optimization Formulation of SVM</em>.</p>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>SVM problem is fessible only when data set is linearly separable.</p></li>
<li><p>SVM problem is a convex optimization problem.</p></li>
</ol>
</div>
<hr />
<p><strong>Duality for Convex Optimization</strong>:</p>
<p>Primal problem <em>(P)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{z\in R^s}{\operatorname{min}} &amp;~f(z)\\
s.t. &amp;~h_i(z) \leq 0 \text{ (constraints) }~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>We first introduce the notion of Lagrangian:</p>
<p><span class="math display">\[
\mathcal L(z, \lambda) = f(z) + \sum_{i=1}^n \lambda_i h_i(z)
\]</span></p>
<p>where <span class="math inline">\(\lambda \in R^n\)</span>, <span class="math inline">\(n\)</span> is the number of constraints and <span class="math inline">\(\lambda = (\lambda_1, \cdots, \lambda_n)\)</span> is the vector of Lagrangian multipliers.</p>
<p>Let <span class="math inline">\(g(h) = \underset{z\in R^s}{\operatorname{min}} ~\mathcal L(z, \lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> is fixed, then we have the <em>Dual Problem of (P) ((D))</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda}{\operatorname{max}} &amp;~g(\lambda)\\
s.t. &amp;~\lambda_i \geq 0 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Let’s suppose that <span class="math inline">\(f:R^s \to R\)</span> and <span class="math inline">\(h_i:R^s \to R\)</span> are all differentiable and convex functions. Then,</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda \geq 0}{\operatorname{max}} g(\lambda) &amp;= \underset{\lambda \geq 0}{\operatorname{max}} \underset{z\in R^s}{\operatorname{min}} \mathcal L(z,\lambda)\\
&amp;= \underset{z\in R^s}{\operatorname{min}} \underset{\lambda \geq 0}{\operatorname{max}} \mathcal L(z,\lambda)\\
&amp;= \underset{z\in R^s}{\operatorname{min}} f(z) ~(\text{suppose min and max can be swapped})\\
&amp;\qquad s.t. ~h_i(z) \leq 0 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>For the equation mentioned above:</p>
<p><span class="math display">\[
\underset{\lambda \geq 0}{\operatorname{max}} \mathcal L(z,\lambda) = \underset{\lambda \geq 0}{\operatorname{max}} f(z) + \sum_{i=1}^n \lambda_i h_i(z) = \begin{cases}\infty&amp; \exists i, \text{ s.t. } ~h_i(z) &gt; 0\\f(z) &amp; \forall i, ~h_i(z) \leq 0\end{cases}
\]</span></p>
<div class="theorem">
<p><span id="thm:KKT" class="theorem"><strong>Theorem 7.1  (Karush-Kuhn-Tucker) </strong></span></p>
<ul>
<li><p>Suppose <span class="math inline">\(\exists \tilde z\)</span>, s.t. <span class="math inline">\(h_i(\tilde z) \leq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots, n\)</span> (Slatter’s condition)</p></li>
<li><p>Suppose <span class="math inline">\(f, h_i\)</span> are differentiable and convex functions.</p></li>
</ul>
<p>Then, <span class="math inline">\(\forall z^*\)</span> solution to <em>(P)</em>, <span class="math inline">\(\exists \lambda^*\)</span> solution to <em>(D)</em>, s.t.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\vec 0 ~(\in R^s) = \nabla f(z^*) + \sum_{i=1}^n \lambda_i^* \nabla h_i(z^*)\)</span> (Stationarity)</p></li>
<li><p><span class="math inline">\(h_i(z^*) \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Primal Feasibility)</p></li>
<li><p><span class="math inline">\(\lambda_i^* \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Dual Feasibility)</p></li>
<li><p><span class="math inline">\(\lambda_i^* h_i(z^*) = 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Complementary Slackness)</p></li>
</ol>
<p>Conversely, if <span class="math inline">\((z^*, \lambda^*)\)</span> satisfy 1-4, then <span class="math inline">\(z^*\)</span> is a solution to <em>(P)</em> and <span class="math inline">\(\lambda^*\)</span> is a solution to <em>(D)</em>.</p>
</div>
<hr />
<p>Back to <strong>SVM</strong> problem:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~\frac{|| \beta ||^2}{2}\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(z = (\beta, \beta_0)\)</span>, <span class="math inline">\(f(z) = \frac{|| \beta ||^2}{2}\)</span>, <span class="math inline">\(h_i(z) = 1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\)</span>.</p>
<p>Then we have <span class="math inline">\(\mathcal L(\beta, \beta_0, \lambda) = \frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\Big)\)</span> and <span class="math inline">\(g(\lambda) = \underset{(\beta, \beta_{0})}{\operatorname{min}} \{\frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\Big)\}\)</span></p>
<p>Case 1: If <span class="math inline">\(\sum_{i=1}^n \lambda_iy_i \neq 0\)</span>, <span class="math inline">\(g(\lambda) = -\infty\)</span></p>
<p>Case 2: If <span class="math inline">\(\sum_{i=1}^n \lambda_iy_i = 0\)</span>, <span class="math inline">\(g(\lambda) = \underset{\beta}{\operatorname{min}} \{\frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt;)\Big)\}\)</span>.</p>
<p>To find this, we can find the critical point for the above problem: <span class="math inline">\(\vec 0 = \beta - \sum_{i=1}^n \lambda_i y_i x_i\)</span>. Thus, plug the <span class="math inline">\(\beta\)</span> back to the above expression, we have: <span class="math inline">\(g(\lambda) = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j &lt;x_i, x_j&gt; + \sum_{i=1}^n \lambda_i\)</span>.</p>
<p>As a result, we have the Dual of SVM <em>(D)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda \geq 0}{\operatorname{max}} g(\lambda) &amp;= \underset{\lambda \in R^n}{\operatorname{max}} \{-\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j &lt;x_i, x_j&gt; + \sum_{i=1}^n \lambda_i \}\\
&amp;\qquad s.t. ~
\begin{cases}\sum_{i=1}^n \lambda_iy_i = 0\\
\lambda_i \geq 0
\end{cases}~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Now, using KKT conditions: let <span class="math inline">\(\lambda^*\)</span> be a solution to <em>(D)</em>. From the (Stationarity), we have <span class="math inline">\(\beta^* = \sum_{i=1}^n \lambda_i^* y_i x_i\)</span>. From the (Complementary Slackness), we have</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\lambda_i^*\Big(1-y_i(&lt;x_i, \beta^*&gt; + \beta_0^*)\Big) = 0,  ~\forall i\\
\underset{\text{choose } \hat{i} \text{ s.t. } \lambda^*_{\hat{i}} &gt; 0}{\Rightarrow} &amp;~ 1 - y_{\hat i} (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*) = 0\\
\Rightarrow &amp;~ 1 = y_{\hat i} (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*)\\
\underset{y_i^2=1}{\Rightarrow} &amp;~ y_{\hat i} = (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*)\\
\Rightarrow &amp;~ (y_{\hat i} - &lt;x_{\hat i}, \beta^*&gt;) = \beta_0^*
\end{aligned}
\]</span></p>
<p>Then, we go from <span class="math inline">\(\lambda^*\)</span> to <span class="math inline">\((\beta^*, \beta_0^*)\)</span> or <em>(D)</em> to <em>(P)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\beta^* &amp;= \sum_{i=1}^n \lambda_i^* y_i x_i\\
\beta_0^* &amp;= y_{\hat i} - &lt;x_{\hat i}, \beta^*&gt;\\
&amp;= y_{\hat i} - \sum_{j=1}^n \lambda_j^* y_j &lt;x_j, x_{\hat i}&gt;, \text{ where } \hat{i} \text{ s.t. } \lambda^*_{\hat{i}} &gt; 0
\end{aligned}
\]</span></p>
<p>One can show that the primal problem <em>(P)</em> has a unique solution as shown above (Use strictly convex to show uniqueness for <span class="math inline">\(\beta^*\)</span>).</p>
<p>Thus the classifier is:</p>
<p><span class="math display">\[
\begin{aligned}
l(x)&amp;=
\begin{cases}
1 &amp; &lt;\beta^*, x&gt; + \beta_0^* &gt; 0\\
-1 &amp; &lt;\beta^*, x&gt; + \beta_0^* &lt; 0
\end{cases}\\
&amp;=
\begin{cases}
1 &amp; \sum_{j=1}^n \lambda_j^* y_j (&lt;x_j, x&gt; - &lt;x_j, x_i&gt;) + y_i &gt; 0\\
-1 &amp; o.w.
\end{cases}, \text{ where }i \text{ s.t. } \lambda_i^*&gt;0\\
\end{aligned}
\]</span></p>
<p>which depends on <span class="math inline">\(x\)</span> and <span class="math inline">\(x_i\)</span> only through inner products. This is a crucial property that we will exploit to define a larger class of SVM classifiers.</p>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>Hard Margin SVM problem has a solution (primal problem is feasible) if data is linearly separable .</p></li>
<li><p>In KKT, primal problem is feasible iff dual problem is bounded (in which case both primal and dual have solution and it makes sense to talk about <span class="math inline">\(z^*\)</span> and <span class="math inline">\(\lambda^*\)</span>).</p></li>
</ol>
</div>
<div class="definition">
<p><span id="def:SV" class="definition"><strong>Definition 7.1  (Support Vectors) </strong></span>Under the assumptions, let <span class="math inline">\(\beta^*, \beta_0^*, \lambda^*\)</span> be the solution to <em>(P)</em>. We say <span class="math inline">\(x_i\)</span> is a support vector if <span class="math inline">\(y_i (&lt;x_i, \beta^*&gt; + \beta_0^*) = 1\)</span>.</p>
</div>
<p>In particular, the minimum distance between <span class="math inline">\(\{x_1, \cdots, x_n\}\)</span> and the optimal hyperplane <span class="math inline">\(\mathcal H_{\beta^*, \beta_0^*}\)</span> is achieved at the support vectors.</p>
<div class="theorem">
<p><span id="thm:SPofSVM" class="theorem"><strong>Theorem 7.2  (Stability Property of SVMs) </strong></span>Let <span class="math inline">\(x_i\)</span> be a training data point and not a support vector. Suppose <span class="math inline">\(x_i\)</span> is changed for a point <span class="math inline">\(\tilde x_i\)</span> s.t. <span class="math inline">\(y_i (&lt;x_i, \beta^*&gt; + \beta_0^*) \geq 1\)</span>. Then the solution for the new data set is the same for the original data set.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span><span class="math inline">\((\lambda^*, \beta_0^*, \beta^*)\)</span> from the original problem still satisfy KKT condition for new data set:</p>
<ol style="list-style-type: decimal">
<li><p>(Stationarity) As <span class="math inline">\(x_i\)</span> is not a support vector, we have <span class="math inline">\(\lambda^*_i = 0\)</span>. Thus, <span class="math inline">\(\beta^* = \sum_{j=1}^n \lambda_j^* y_j x_j = \sum_{j \neq i} \lambda_j^* y_j x_j + \lambda_i^* y_i x_i = \sum_{j \neq i} \lambda_j^* y_j x_j + \lambda_i^* y_i \tilde x_i\)</span>, which means that <span class="math inline">\(\beta^*\)</span> doesn’t change.</p></li>
<li><p>(Primal Feasibility) <span class="math inline">\(y_j (&lt;\beta^*, x_j&gt; + \beta_0^*) \geq 1\)</span>, <span class="math inline">\(\forall j \neq i\)</span> and <span class="math inline">\(y_i (&lt;\beta^*, x_j&gt; + \beta_0^*) \geq 1\)</span> by the assumption above.</p></li>
<li><p>(Dual Feasibility) <span class="math inline">\(\lambda_j \geq 0\)</span>, <span class="math inline">\(\forall j\)</span>.</p></li>
<li><p>(Complementary Slackness) As <span class="math inline">\(\lambda_i^* = 0\)</span>, <span class="math inline">\(\lambda_i^* (1-y_i(&lt;\tilde x_i, \beta^*&gt; + \beta_0^*)) = 0\)</span>.</p></li>
</ol>
<p>As we didn’t change any <span class="math inline">\(y_i\)</span>, we still have <span class="math inline">\(\sum_{j=1}^n \lambda_i^* y_i = 0\)</span>. As a result, <span class="math inline">\((\beta_0^*, \beta^*)\)</span> is still a solution for the primal problem for the new data set.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-5" class="remark"><em>Remark</em>. </span>The robustness associated to SVMs is markedly different to what happens with LDA or Logistic Regression, where outliers can change completely the decision boundaries. It depends only on support vectors.</p>
</div>
</div>
<div id="soft-margin-svm-1" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Soft Margin SVM<a href="linear-classifiers-1.html#soft-margin-svm-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What if the data set is not linearly separable?</p>
<div id="case-1-1" class="section level4 hasAnchor" number="7.3.3.1">
<h4><span class="header-section-number">7.3.3.1</span> Case 1<a href="linear-classifiers-1.html#case-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="_main_files/figure-html/smSVM1-1.png" width="672" /></p>
<p>The data set is not linearly separable mildly. This case will motivate the soft margin SVM (generalizes Hard Margin SVM).</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\beta_0, \beta, \xi_1, \cdots, \xi_n}{\operatorname{min}} &amp;~\frac{|| \beta ||^2}{2} + C \sum_{i=1}^n \xi_i\\
s.t. &amp;~
\begin{cases}
y_i(&lt;x_i, \beta&gt;) \geq 1 - \xi_i\\
\xi_i \geq 0
\end{cases}, ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a parameter of the problem: as <span class="math inline">\(C \nearrow \infty\)</span> we recover the hard margin problem (in case the data is linearly separable). However, for <span class="math inline">\(C \in (0, \infty)\)</span>, the problem is always feasible.</p>
</div>
<div id="case-2-1" class="section level4 hasAnchor" number="7.3.3.2">
<h4><span class="header-section-number">7.3.3.2</span> Case 2:<a href="linear-classifiers-1.html#case-2-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="_main_files/figure-html/smSVM2-1.png" width="672" /></p>
<p>The data set is not linearly separable but by a lot. After a transformation, we will be able to transform the data into a linearly separable data set.</p>
<p>Let <span class="math inline">\(\begin{aligned}\psi: R^2 &amp;\to \mathcal H( = R^3)\\(x,y) &amp;\to (x,y,x^2+y^2)\end{aligned}\)</span>. Then the data set <span class="math inline">\((\psi(x_1), y_1), \cdots, (\psi(x_n), y_n)\)</span> is now linearly separable, and we will work on the embedded space:</p>
<p><em>(P)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\beta_0 \in R, \beta \in \mathcal H}{\operatorname{min}} &amp;~\frac{|| \beta ||_{\mathcal H}^2}{2}\\
s.t. &amp;~y_i(&lt;\beta, \psi(x_i)&gt;_{\mathcal H}) \geq 1, ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p><em>(D)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda \in R^n}{\operatorname{max}} &amp;~ -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j &lt;\psi(x_i), \psi(x_j)&gt;_{\mathcal H} + \sum_{i=1}^n \lambda_i\\
&amp;s.t. ~
\begin{cases}\sum_{i=1}^n \lambda_iy_i = 0\\
\lambda_i \geq 0
\end{cases},~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<ul>
<li><p>From <span class="math inline">\(\lambda^*\)</span>, we can obtain <span class="math inline">\(\beta^* = \sum_{j=1}^n \lambda_j^* y_j \psi(x_j)\)</span></p></li>
<li><p>Given <span class="math inline">\(i\)</span> s.t. <span class="math inline">\(\lambda_i^* &gt; 0\)</span>, <span class="math inline">\(\beta_0^*\)</span> can then be computed: <span class="math inline">\(\beta_0^* = y_i - &lt;\beta^*, \psi(x_i)&gt;_{\mathcal H} = y_i - \sum_{j=1}^n \lambda_j^* y_j &lt;\psi(x_j), \psi(x_i)&gt;_{\mathcal H}\)</span>.</p></li>
<li><p>The classifier is (<span class="math inline">\(x \in R^d\)</span>):</p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
l_{\psi}(x):&amp;=
\begin{cases}
1 &amp; &lt;\beta^*, \psi(x)&gt;_{\mathcal H} + \beta_0^* &gt; 0\\
-1 &amp; &lt;\beta^*, \psi(x)&gt;_{\mathcal H} + \beta_0^* &lt; 0
\end{cases}\\
&amp;= \begin{cases}
1 &amp; \sum_{j=1}^n \lambda_j^* y_j (&lt;\psi(x_j), \psi(x)&gt;_{\mathcal H} - &lt;\psi(x_j), \psi(x_i)&gt;_{\mathcal H}) + y_i &gt; 0\\
-1 &amp; o.w.
\end{cases}\\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(i\)</span> s.t. <span class="math inline">\(\lambda_i^* &gt; 0\)</span>, and <span class="math inline">\(\lambda_j^*\)</span> only depends on <span class="math inline">\(&lt;\psi(x_j), \psi(x_l)&gt;_{\mathcal H}\)</span></p>
<p>Let <span class="math inline">\(K(x, \tilde x) = &lt;\psi(x), \psi(\tilde x)&gt;_{\mathcal H}\)</span> for arbitrary <span class="math inline">\(x, \tilde x \in R^d\)</span>. Then the only thing we need to build <span class="math inline">\(l_\psi\)</span> is the function <span class="math inline">\(K\)</span>. In particular, we don’t need <span class="math inline">\(\psi\)</span> explicitly. Thus, it is fair to write <span class="math inline">\(l_\psi = l_K\)</span>.</p>
<p>In particular, to produce more general SVM classifiers, we can either:</p>
<ol style="list-style-type: decimal">
<li><p>Pick a “Hilbert Space” <span class="math inline">\(\mathcal H\)</span> and define a map <span class="math inline">\(\psi: R^d \to \mathcal H\)</span>, then we can build the function <span class="math inline">\(K\)</span>, which is all we need to compute <span class="math inline">\(l_\psi\)</span>.</p></li>
<li><p>Pick a “Kernel” <span class="math inline">\(K\)</span> and produce the classifier <span class="math inline">\(l_K\)</span>.</p></li>
</ol>
<p>Question: If we start with a “Kernel” <span class="math inline">\(K\)</span>, can we implicitly picking <span class="math inline">\(\mathcal H\)</span> and <span class="math inline">\(\psi\)</span>?</p>
<p>Answer: Under fairly general condition, yes.</p>
<div class="definition">
<p><span id="def:kernel" class="definition"><strong>Definition 7.2  (Kernels) </strong></span>A kernel over a set <span class="math inline">\(\mathcal X\)</span> is a function <span class="math inline">\(K: \mathcal X \times \mathcal X \to R\)</span> with the following property:</p>
<p><span class="math inline">\(\forall n \in N\)</span> and <span class="math inline">\(\forall\)</span> sequence of points <span class="math inline">\(x_1, \cdots, x_n \in \mathcal X\)</span>, the matrix</p>
<p><span class="math display">\[
A:=
\left(
\begin{matrix} K(x_1,x_1) &amp; \cdots &amp; K(x_1, x_n)\\
\vdots &amp; \ddots &amp; \vdots\\
K(x_n ,x_1) &amp; \cdots &amp; K(x_n, x_n)
\end{matrix}
\right)
\]</span></p>
<p>is symmetric (<span class="math inline">\(A = A^T\)</span>) and positive semi-definite <span class="math inline">\(&lt;Av, v&gt;_{R^n} \geq 0\)</span>, <span class="math inline">\(\forall b \in R^n\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-6" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>Kernels must be symmetric functions: <span class="math inline">\(K(x,y) = K(y,x)\)</span>, <span class="math inline">\(\forall x,y \in \mathcal X\)</span>.</p></li>
<li><p>But there are symmetric functions that are not kernels. In particular, we need to check the positive semi-definiteness condition.</p></li>
</ol>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-7" class="proposition"><strong>Proposition 7.1  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Inner products are kernels: <span class="math inline">\(K(x, y) = &lt;x,y&gt;_{R^d}\)</span>.</p></li>
<li><p>If <span class="math inline">\(K_1: \mathcal X \times \mathcal X \to R\)</span> and <span class="math inline">\(K_2: \mathcal X \times \mathcal X \to R\)</span> are two kernels, then <span class="math inline">\(K(x,y):= a_1 K_1(x,y) + a_2 K_2(x,y)\)</span> is also a kernel, provided <span class="math inline">\(a_1, a_2\)</span> are two non-negative numbers.</p></li>
<li><p>If <span class="math inline">\(K: \mathcal X \times \mathcal X \to R\)</span> is a kernel and <span class="math inline">\(f:\mathcal X \to R\)</span> is a function, then <span class="math inline">\(\tilde K(x,y) := f(x) \cdot \f(y) \cdot K(x,y)\)</span> is also a kernel.</p></li>
<li><p>If <span class="math inline">\(K_1: \mathcal X \times \mathcal X \to R\)</span> and <span class="math inline">\(K_1: \mathcal X \times \mathcal X \to R\)</span> are two kernels, then <span class="math inline">\(K(x,y) := K_1(x,y) \cdot K_2(x,y)\)</span> is also a kernel.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>(of 1)</p>
<p>Let <span class="math inline">\(n \in N\)</span> and <span class="math inline">\(x_1, \cdots, x_n \in R^d\)</span> be arbitrary. Consider the matrix <span class="math inline">\(A \in R^{n \times n}\)</span> with entries <span class="math inline">\(A_{ij} = &lt;x_i, x_j&gt;_{R^d}\)</span>, then</p>
<ul>
<li><p>It is symmetric: <span class="math inline">\(A_{ij} = &lt;x_i, x_j&gt;_{R^d} = &lt;x_j, x_i&gt;_{R^d} = A_{ji}\)</span></p></li>
<li><p>It is positive semi-definite:<br />
Let <span class="math inline">\(v = (v_1, \cdots, v_n) \in R^n\)</span>. Then<br />
<span class="math display">\[
\begin{aligned}
&lt;Av, v&gt;_{R^d} &amp;= \sum_{i=1}^n \sum_{j=1}^n A_{ij} v_i v_j\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n &lt;x_i, x_j&gt;_{R^d} v_i v_j\\
&amp;= \sum_{i=1}^n \sum_{j=1}^n &lt;v_i x_i, v_j x_j&gt;_{R^d}\\
&amp;= &lt;\sum_{i=1}^n v_i x_i, \sum_{j=1}^n v_j x_j&gt;_{R^d}\\
&amp;= || \sum_{i=1}^n v_i x_i ||^2_{R_d}\\
&amp;\geq 0
\end{aligned}
\]</span></p></li>
</ul>
</div>
<p>By the above proof, we can build the kernel <span class="math inline">\(K(x,y) = &lt;\psi(x), \psi(y)&gt;_{\mathcal H}\)</span> given <span class="math inline">\(\mathcal H\)</span> and <span class="math inline">\(\psi :R^d \to \mathcal H\)</span>.</p>
<div class="corollary">
<p><span id="cor:unlabeled-div-9" class="corollary"><strong>Corollary 7.1  </strong></span>The following are kernels in <span class="math inline">\(R^d\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Inner product: <span class="math inline">\(K(x,y) = &lt;x,y&gt;_{R^d}\)</span>.</p></li>
<li><p>Polynomial: <span class="math inline">\(K(x,y) = a_0 + a_1 &lt;x,y&gt;_{R^d} + \cdots + a_l (&lt;x,y&gt;_{R^d})^l\)</span> as long as <span class="math inline">\(a_0, \cdots, a_l \geq 0\)</span>. (<span class="math inline">\((&lt;x,y&gt;_{R^d})^l\)</span> is a kernel)</p></li>
<li><p>Radial Basis Kernel or Gaussian Kernel: <span class="math inline">\(K(x,y) = \operatorname{exp} (-\frac{|| x-y ||^2}{\sigma})\)</span> for <span class="math inline">\(\sigma &gt; 0\)</span>.</p></li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-10" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>For every kernel we will have a different classification rule. Try the simplest classifiers first (linear classifiers or soft margin SVMs).</p></li>
<li><p>Any classifier that can be purely defined in terms of inner products can be kernelized.</p></li>
<li><p>For a given kernel, how do we know if the embedded data set <span class="math inline">\((\psi(x_1), y_1), \cdots, (\psi(x_n), y_n)\)</span> is linearly separable? In some cases it is always the case. But even if the data set is not linearly separable, we mat be in the Case 3 mentioned below, where we should use the kernelized version of SVMs.</p></li>
</ol>
</div>
</div>
<div id="case-3-1" class="section level4 hasAnchor" number="7.3.3.3">
<h4><span class="header-section-number">7.3.3.3</span> Case 3:<a href="linear-classifiers-1.html#case-3-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="_main_files/figure-html/smSVM3-1.png" width="672" /></p>
<p>Use the kernelized version of Soft Margin SVM.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-classifiers.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/06-SVM.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
