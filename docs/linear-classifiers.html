<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Classifiers | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Classifiers | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Classifiers | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="summary.html"/>
<link rel="next" href="support-vector-machine.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hat-eta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat \eta_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#erm-with-kernels"><i class="fa fa-check"></i><b>8.2</b> ERM with Kernels</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#comparison"><i class="fa fa-check"></i><b>8.2.1</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#regularity"><i class="fa fa-check"></i><b>8.3</b> Regularity</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html"><i class="fa fa-check"></i><b>9</b> Uncertainty Quantification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#bayes-perspective"><i class="fa fa-check"></i><b>9.1</b> Bayes Perspective</a></li>
<li class="chapter" data-level="9.2" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#gaussians-in-mathcal-rd"><i class="fa fa-check"></i><b>9.2</b> Gaussians in <span class="math inline">\(\mathcal R^d\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#diagonalization-of-sigma-and-sampling-for-n0-sigma"><i class="fa fa-check"></i><b>9.3</b> Diagonalization of <span class="math inline">\(\Sigma\)</span> and Sampling for <span class="math inline">\(N(0, \Sigma)\)</span></a></li>
<li class="chapter" data-level="9.4" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#stochastic-processes-and-gaussian-processes"><i class="fa fa-check"></i><b>9.4</b> Stochastic Processes and Gaussian Processes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-classifiers" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear Classifiers<a href="linear-classifiers.html#linear-classifiers" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="math inline">\(\mathcal F = \{\text{all linear classifiers}\}\)</span>, <span class="math inline">\(\mathcal X = R^d\)</span>, <span class="math inline">\(\mathcal Y = \{-1,1\}\)</span> or <span class="math inline">\(\{1,\cdots,K\}\)</span>. Consider mainly on binary case.</p>
<p>Let <span class="math inline">\(\beta \in R^d\)</span> and <span class="math inline">\(\beta_0 \in R\)</span>, <span class="math inline">\(\mathcal H_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 = 0\}\)</span> (hyperplane), <span class="math inline">\(\mathcal H^+_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 \geq 0\}\)</span> and <span class="math inline">\(\mathcal H^-_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 &lt; 0\}\)</span> (half plane).</p>
<p><span class="math display">\[
\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}
\]</span></p>
<ul>
<li>Question: How to find a linear classifier based on <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>.<br />
A: Three different ways to tune <span class="math inline">\(\beta, \beta_0\)</span> from data: LDA (linear discriminant analysis) or QDA (quadratic discriminant analysis), Logistic Regression, Perceptrons and SVMs.</li>
</ul>
<div id="lda-or-qda" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> LDA or QDA<a href="linear-classifiers.html#lda-or-qda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\((X,Y) \sim \rho\)</span>, where <span class="math inline">\(P(Y=k) = \omega_k, \rho_{X|Y}(X | Y=k) = N(\mu_k, \Sigma_k)\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes}(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \frac{\rho_{X|Y}(x | Y=k) \cdot \omega_k}{\rho_X(x)}\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [\rho_{X|Y}(x | Y=k) \cdot \omega_k]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [log\Big (\rho_{X|Y}(x | Y=k)\Big ) + log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [-log\Big (\frac{1}{(2\pi)^{d/2} (det(\Sigma_k))^{1/2}} \Big ) + \frac{1}{2} &lt;\Sigma_k^{-1} (x-\mu_k), (x-\mu_k)&gt; -  log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} \delta_k(x)
\end{aligned}
\]</span></p>
<p>Observation: <span class="math inline">\(\delta_k(x)\)</span> is quadratic and convex in <span class="math inline">\(x\)</span>.</p>
<p><strong>QDA</strong>: What if we only have <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate <span class="math inline">\(\mu_k, \omega_k, \Sigma_k\)</span>, <span class="math inline">\(k=1,\cdots, K\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 6.1  </strong></span><span class="math display">\[
\hat \mu_k := \frac{\sum_{i ~s.t. ~y_i=k} x_i}{\#\{x_i ~s.t. ~y_i=k\} ~(:= N_k)}
\]</span></p>
<p><span class="math display">\[
(\hat \Sigma_k)_{lm} := (\frac{1}{N_k} \sum_{i ~s.t. ~y_i=k} x_{il}x_{im} - \hat \mu_{kl} \hat \mu_{km}), \text{ where } l=1,\cdots,d, ~m=1,\cdots,d.
\]</span></p>
<p><span class="math display">\[
\hat \omega_k = \frac{N_k}{n}
\]</span></p>
<p><span class="math display">\[
\hat \delta_k(x) \text{ same as } \delta_k \text{ but with } \hat{} \text{ everywhere}
\]</span></p>
</div>
<p><strong>LDA</strong>: What if we had assumed <span class="math inline">\(\Sigma_1 = \Sigma_2 = \cdots = \Sigma_K = \Sigma\)</span>?</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [\frac{1}{2} &lt;\Sigma^{-1}x, x&gt; + &lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [&lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} l_k(x), ~(l_k(x) \text{ is linear})
\end{aligned}
\]</span></p>
<p>We can estimate <span class="math inline">\(\mu_k ,\omega_k\)</span> by <span class="math inline">\(\hat \mu_k ,\hat \omega_k\)</span>, and <span class="math inline">\(\Sigma\)</span> with the full data set.</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Logistic Regression<a href="linear-classifiers.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\(\vec \beta_k \in R^d\)</span>, <span class="math inline">\(\beta_{0k} \in R\)</span>, and <span class="math inline">\((X,Y)\)</span> satisfies: <span class="math inline">\(P(Y=k | X=x) = \frac{exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, <span class="math inline">\(P(Y=K | X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Let <span class="math inline">\(\varphi_k(x) := exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})\)</span> and <span class="math inline">\(\varphi_K(x):=1\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Then we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes} (x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \varphi_k(x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} log(\varphi_k(x))
\end{aligned}
\]</span></p>
<p>What if we only have observed <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate the parameters.</p>
<div class="example">
<p><span id="exm:mle" class="example"><strong>Example 6.2  (MLE) </strong></span>Given the data, find the best parameters (the ones maximizing the likelihood of the observations), i.e.</p>
<p><span class="math display">\[
\{(\vec \beta_k^*, \beta_{0k}^*)\} = \underset{\{(\vec \beta_k, \beta_{0k})\}_{k=1,\cdots,K-1}}{\operatorname{max}} \prod_{i=1}^n P(Y = y_i | X = x_i)
\]</span></p>
</div>
</div>
<div id="perceptrons-and-svms" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Perceptrons and SVMs<a href="linear-classifiers.html#perceptrons-and-svms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{-1, 1\}\)</span>,
<span class="math inline">\((x_i, y_i)_{i=1,\cdots,n}\)</span>,
<span class="math inline">\((\vec \beta, \beta_0)\)</span>,
<span class="math inline">\(\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}\)</span>,
<span class="math inline">\(\sigma(\vec \beta, \beta_0) := \sum_{i \in \mathcal M_{\vec \beta, \beta_0}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span>,
where <span class="math inline">\(\mathcal M_{\vec \beta, \beta_0} = \{i \text{ s.t. } \mathfrak f_{\vec \beta, \beta_0} (x_i) \neq y_i\}\)</span></p>
<div id="perceptrons" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Perceptrons<a href="linear-classifiers.html#perceptrons" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\((\vec \beta^*, \beta_0^*) := \underset{(\vec \beta, \beta_{0})}{\operatorname{min}} \sigma(\vec \beta, \beta_0)\)</span>, then the perceptron classifier is <span class="math inline">\(\mathfrak f_{\vec \beta^*, \beta_0^*} (x)\)</span>. There exist many solutions of perceptron problems but some hyperplanes seem to be more robust:</p>
<div class="example">
<p><span id="exm:perceptron" class="example"><strong>Example 6.3  </strong></span><img src="figures/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hyperplane 2 seems to be more robust.</p>
</div>
</div>
<div id="svms" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> SVMs<a href="linear-classifiers.html#svms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>See next chapter.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="summary.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machine.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-LC.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
