<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Classifiers | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Classifiers | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Classifiers | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="summary.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hateta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat{\eta}_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-classifiers" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear Classifiers<a href="linear-classifiers.html#linear-classifiers" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="math inline">\(\mathcal F = \{\text{all linear classifiers}\}\)</span>, <span class="math inline">\(\mathcal X = R^d\)</span>, <span class="math inline">\(\mathcal Y = \{-1,1\}\)</span> or <span class="math inline">\(\{1,\cdots,K\}\)</span>. Consider mainly on binary case.</p>
<p>Let <span class="math inline">\(\beta \in R^d\)</span> and <span class="math inline">\(\beta_0 \in R\)</span>, <span class="math inline">\(\mathcal H_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 = 0\}\)</span> (hyperplane), <span class="math inline">\(\mathcal H^+_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 \geq 0\}\)</span> and <span class="math inline">\(\mathcal H^-_{\beta, \beta_0} = \{x\in R^d: &lt;\beta, x&gt; + \beta_0 &lt; 0\}\)</span> (half plane).</p>
<p><span class="math display">\[
\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}
\]</span></p>
<ul>
<li>Question: How to find a linear classifier based on <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>.<br />
A: Three different ways to tune <span class="math inline">\(\beta, \beta_0\)</span> from data: LDA (linear discriminant analysis) or QDA (quadratic discriminant analysis), Logistic Regression, Perceptrons and SVMs.</li>
</ul>
<div id="lda-or-qda" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> LDA or QDA<a href="linear-classifiers.html#lda-or-qda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\((X,Y) \sim \rho\)</span>, where <span class="math inline">\(P(Y=k) = \omega_k, \rho_{X|Y}(X | Y=k) = N(\mu_k, \Sigma_k)\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes}(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \frac{\rho_{X|Y}(x | Y=k) \cdot \omega_k}{\rho_X(x)}\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [\rho_{X|Y}(x | Y=k) \cdot \omega_k]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} [log\Big (\rho_{X|Y}(x | Y=k)\Big ) + log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [-log\Big (\frac{1}{(2\pi)^{d/2} (det(\Sigma_k))^{1/2}} \Big ) + \frac{1}{2} &lt;\Sigma_k^{-1} (x-\mu_k), (x-\mu_k)&gt; -  log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} \delta_k(x)
\end{aligned}
\]</span></p>
<p>Observation: <span class="math inline">\(\delta_k(x)\)</span> is quadratic and convex in <span class="math inline">\(x\)</span>.</p>
<p><strong>QDA</strong>: What if we only have <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate <span class="math inline">\(\mu_k, \omega_k, \Sigma_k\)</span>, <span class="math inline">\(k=1,\cdots, K\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 6.1  </strong></span><span class="math display">\[
\hat \mu_k := \frac{\sum_{i ~s.t. ~y_i=k} x_i}{\#\{x_i ~s.t. ~y_i=k\} ~(:= N_k)}
\]</span></p>
<p><span class="math display">\[
(\hat \Sigma_k)_{lm} := (\frac{1}{N_k} \sum_{i ~s.t. ~y_i=k} x_{il}x_{im} - \hat \mu_{kl} \hat \mu_{km}), \text{ where } l=1,\cdots,d, ~m=1,\cdots,d.
\]</span></p>
<p><span class="math display">\[
\hat \omega_k = \frac{N_k}{n}
\]</span></p>
<p><span class="math display">\[
\hat \delta_k(x) \text{ same as } \delta_k \text{ but with } \hat{} \text{ everywhere}
\]</span></p>
</div>
<p><strong>LDA</strong>: What if we had assumed <span class="math inline">\(\Sigma_1 = \Sigma_2 = \cdots = \Sigma_K = \Sigma\)</span>?</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f(x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [\frac{1}{2} &lt;\Sigma^{-1}x, x&gt; + &lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmin}} [&lt;\Sigma^{-1}x, \mu_k&gt; + \frac{1}{2} &lt;\Sigma^{-1}\mu_k, \mu_k&gt; - log(\omega_k)]\\
&amp;:= \underset{k=1,\cdots, K}{\operatorname{argmin}} l_k(x), ~(l_k(x) \text{ is linear})
\end{aligned}
\]</span></p>
<p>We can estimate <span class="math inline">\(\mu_k ,\omega_k\)</span> by <span class="math inline">\(\hat \mu_k ,\hat \omega_k\)</span>, and <span class="math inline">\(\Sigma\)</span> with the full data set.</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Logistic Regression<a href="linear-classifiers.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{1, \cdots, K\}\)</span>, <span class="math inline">\(\vec \beta_k \in R^d\)</span>, <span class="math inline">\(\beta_{0k} \in R\)</span>, and <span class="math inline">\((X,Y)\)</span> satisfies: <span class="math inline">\(P(Y=k | X=x) = \frac{exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, <span class="math inline">\(P(Y=K | X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} exp(&lt;x,\vec \beta_l&gt; + \beta_{0l})}\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Let <span class="math inline">\(\varphi_k(x) := exp(&lt;x,\vec \beta_k&gt; + \beta_{0k})\)</span> and <span class="math inline">\(\varphi_K(x):=1\)</span>, where <span class="math inline">\(k=1,\cdots,K-1\)</span>. Then we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathfrak f_{Bayes} (x) &amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} P(Y=k | X=x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} \varphi_k(x)\\
&amp;= \underset{k=1,\cdots, K}{\operatorname{argmax}} log(\varphi_k(x))
\end{aligned}
\]</span></p>
<p>What if we only have observed <span class="math inline">\((x_i, y_i)\)</span>, <span class="math inline">\(i=1,\cdots,n\)</span>? We use the observations to estimate the parameters.</p>
<div class="example">
<p><span id="exm:mle" class="example"><strong>Example 6.2  (MLE) </strong></span>Given the data, find the best parameters (the ones maximizing the likelihood of the observations), i.e.</p>
<p><span class="math display">\[
\{(\vec \beta_k^*, \beta_{0k}^*)\} = \underset{\{(\vec \beta_k, \beta_{0k})\}_{k=1,\cdots,K-1}}{\operatorname{max}} \prod_{i=1}^n P(Y = y_i | X = x_i)
\]</span></p>
</div>
</div>
<div id="perceptrons-and-svms" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Perceptrons and SVMs<a href="linear-classifiers.html#perceptrons-and-svms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathcal Y = \{-1, 1\}\)</span>,
<span class="math inline">\((x_i, y_i)_{i=1,\cdots,n}\)</span>,
<span class="math inline">\((\vec \beta, \beta_0)\)</span>,
<span class="math inline">\(\mathfrak f_{\beta, \beta_0}(x):=\begin{cases}1 &amp; &lt;\beta,x&gt; + \beta_0 \geq 0 (\Leftrightarrow x \in H^+_{\beta,\beta_0})\\- 1&amp; &lt;\beta,x&gt; + \beta_0 &lt; 0(\Leftrightarrow x \in H^-_{\beta,\beta_0})\end{cases}\)</span>,
<span class="math inline">\(\sigma(\vec \beta, \beta_0) := \sum_{i \in \mathcal M_{\vec \beta, \beta_0}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span>,
where <span class="math inline">\(\mathcal M_{\vec \beta, \beta_0} = \{i \text{ s.t. } \mathfrak f_{\vec \beta, \beta_0} (x_i) \neq y_i\}\)</span></p>
<p><strong>Perceptrons</strong>: Let <span class="math inline">\((\vec \beta^*, \beta_0^*) := \underset{(\vec \beta, \beta_{0})}{\operatorname{min}} \sigma(\vec \beta, \beta_0)\)</span>, then the perceptron classifier is <span class="math inline">\(\mathfrak f_{\vec \beta^*, \beta_0^*} (x)\)</span>. There exist many solutions of perceptron problems but some hyperplanes seem to be more robust:</p>
<div class="example">
<p><span id="exm:perceptron" class="example"><strong>Example 6.3  </strong></span><img src="figures/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hyperplane 2 seems to be more robust.</p>
</div>
<p><strong>SVMs</strong>: Let’s suppose that <span class="math inline">\((x_i, y_i)_{i=1,\cdots,n}\)</span> is linearly separable (for motivation for now). Then there exists at least one <span class="math inline">\(\vec \beta, \beta_0\)</span> s.t. <span class="math inline">\(\mathcal M_{\vec \beta, \beta_0} = \emptyset\)</span>. What we want is to find</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\vec \beta, \beta_{0})}{\operatorname{max}} &amp;~margin(\vec \beta, \beta_0)\\
s.t. &amp;~\mathcal M_{\vec \beta, \beta_0} = \emptyset
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(margin(\vec \beta, \beta_0):= min\{C^+_{\vec \beta, \beta_0}, C^-_{\vec \beta, \beta_0}\}\)</span>,
<span class="math inline">\(C^+_{\vec \beta, \beta_0} = \underset{x_i \text{ s.t. } y_i = 1}{\operatorname{min}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span> and
<span class="math inline">\(C^-_{\vec \beta, \beta_0} = \underset{x_i \text{ s.t. } y_i = -1}{\operatorname{min}} dist(x_i, \mathcal H_{\vec \beta, \beta_0})\)</span>.</p>
<p>Let <span class="math inline">\(\tilde \beta = \frac{\vec \beta}{|| \vec \beta ||}\)</span>, <span class="math inline">\(\tilde \beta_0 = \frac{\beta_0}{|| \vec \beta ||}\)</span>, where <span class="math inline">\(||\tilde \beta || = 1\)</span>. Then we have <span class="math inline">\(\mathcal H_{\tilde \beta, \tilde \beta_0} = \mathcal H_{\vec \beta, \beta_0}\)</span>, <span class="math inline">\(\mathcal H^+_{\tilde \beta, \tilde \beta_0} = \mathcal H^+_{\vec \beta, \beta_0}\)</span>, and <span class="math inline">\(\mathcal H^-_{\tilde \beta, \tilde \beta_0} = \mathcal H^-_{\vec \beta, \beta_0}\)</span>. Thus, the <em>Geometric Formulation of SVM</em> is:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\vec \beta, \beta_{0})}{\operatorname{max}} &amp;~margin(\tilde \beta, \tilde \beta_0)\\
s.t. &amp;~\mathcal M_{\tilde \beta, \tilde \beta_0} = \emptyset\\
&amp;~||\tilde \beta|| = 1
\end{aligned}
\]</span></p>
<p>As <span class="math inline">\(dist(x, \mathcal H_{\tilde \beta, \tilde \beta_0}) = | &lt;x, \tilde \beta&gt; + \tilde \beta_0|\)</span>,
we have <span class="math inline">\(margin(\tilde \beta, \tilde \beta_0) = \underset{i=1.\cdots,n}{\operatorname{min}} | &lt;x_i, \tilde \beta&gt; + \tilde \beta_0|\)</span>.</p>
<p>As <span class="math inline">\(\mathcal M_{\tilde \beta, \tilde \beta_0} = \emptyset\)</span>,
we have <span class="math inline">\(sign(&lt;x_i, \tilde \beta&gt; + \tilde \beta_0) = y_i\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>Then, <span class="math inline">\(y_i (&lt;x_i, \tilde \beta&gt; + \tilde \beta_0) \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span>.</p>
<p>Thus, <span class="math inline">\(| &lt;x_i, \tilde \beta&gt; + \tilde \beta_0| = y_i (&lt;x_i, \tilde \beta&gt; + \tilde \beta_0)\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>As a result, the margin becomes:</p>
<p><span class="math display">\[
\begin{aligned}
m &amp;= \underset{i=1.\cdots,n}{\operatorname{min}} y_i(&lt;x_i, \tilde \beta&gt; + \tilde \beta_0)\\
&amp;\qquad \qquad \Updownarrow\\
1 &amp;= \underset{i=1.\cdots,n}{\operatorname{min}} y_i(&lt;x_i, \frac{\tilde \beta}{m}&gt; + \frac{\tilde \beta_0}{m})
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\beta = \frac{\tilde \beta}{m}\)</span>, <span class="math inline">\(\beta_0 = \frac{\tilde \beta_0}{m}\)</span>,
then we also have <span class="math inline">\(1 \leq y_i(&lt;x_i, \beta&gt; + \beta_0)\)</span>, <span class="math inline">\(\forall i=1, \cdots, n\)</span>.</p>
<p>As <span class="math inline">\(|| \beta || = || \frac{\tilde \beta}{m} || = \frac{1}{m}\)</span>,
we have <span class="math inline">\(m = \frac{1}{|| \beta ||}\)</span>.</p>
<p>Thus, the geometric formulation of SVM becomes:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\beta, \beta_{0})}{\operatorname{max}} &amp;~\frac{1}{|| \beta ||}\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n\\
&amp;\qquad \qquad \Updownarrow\\
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~|| \beta ||\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n\\
&amp;\qquad \qquad \Updownarrow\\
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~|| \beta ||^2\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>which is the <em>Convex Optimization Formulation of SVM</em>.</p>
<div class="remark">
<p><span id="unlabeled-div-27" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>SVM problem is fessible only when data set is linearly separable.</p></li>
<li><p>SVM problem is a convex optimization problem.</p></li>
</ol>
</div>
<p><strong>Duality for Convex Optimization</strong>:</p>
<p>Primal problem <em>(P)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{z\in R^s}{\operatorname{min}} &amp;~f(z)\\
s.t. &amp;~h_i(z) \leq 0 \text{ (constraints) }~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>We first introduce the notion of Lagrangian:</p>
<p><span class="math display">\[
\mathcal L(z, \lambda) = f(z) + \sum_{i=1}^n \lambda_i h_i(z)
\]</span></p>
<p>where <span class="math inline">\(\lambda \in R^n\)</span>, <span class="math inline">\(n\)</span> is the number of constraints and <span class="math inline">\(\lambda = (\lambda_1, \cdots, \lambda_n)\)</span> is the vector of Lagrangian multipliers.</p>
<p>Let <span class="math inline">\(g(h) = \underset{z\in R^s}{\operatorname{min}} ~\mathcal L(z, \lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> is fixed, then we have the <em>Dual Problem of (P) ((D))</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda}{\operatorname{max}} &amp;~g(\lambda)\\
s.t. &amp;~\lambda_i \geq 0 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Let’s suppose that <span class="math inline">\(f:R^s \to R\)</span> and <span class="math inline">\(h_i:R^s \to R\)</span> are all differentiable and convex functions. Then,</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda \geq 0}{\operatorname{max}} g(\lambda) &amp;= \underset{\lambda \geq 0}{\operatorname{max}} \underset{z\in R^s}{\operatorname{min}} \mathcal L(z,\lambda)\\
&amp;= \underset{z\in R^s}{\operatorname{min}} \underset{\lambda \geq 0}{\operatorname{max}} \mathcal L(z,\lambda)\\
&amp;= \underset{z\in R^s}{\operatorname{min}} f(z) ~(\text{suppose min and max can be swapped})\\
&amp;\qquad s.t. ~h_i(z) \leq 0 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>For the equation mentioned above:</p>
<p><span class="math display">\[
\underset{\lambda \geq 0}{\operatorname{max}} \mathcal L(z,\lambda) = \underset{\lambda \geq 0}{\operatorname{max}} f(z) + \sum_{i=1}^n \lambda_i h_i(z) = \begin{cases}\infty&amp; \exists i, \text{ s.t. } ~h_i(z) &gt; 0\\f(z) &amp; \forall i, ~h_i(z) \leq 0\end{cases}
\]</span></p>
<div class="theorem">
<p><span id="thm:KKT" class="theorem"><strong>Theorem 6.1  (Karush-Kuhn-Tucker) </strong></span></p>
<ul>
<li><p>Suppose <span class="math inline">\(\exists \tilde z\)</span>, s.t. <span class="math inline">\(h_i(\tilde z) \leq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots, n\)</span> (Slatter’s condition)</p></li>
<li><p>Suppose <span class="math inline">\(f, h_i\)</span> are differentiable and convex functions.</p></li>
</ul>
<p>Then, <span class="math inline">\(\forall z^*\)</span> solution to <em>(P)</em>, <span class="math inline">\(\exists \lambda^*\)</span> solution to <em>(D)</em>, s.t.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\vec 0 ~(\in R^s) = \nabla f(z^*) + \sum_{i=1}^n \lambda_i^* \nabla h_i(z^*)\)</span> (Stationarity)</p></li>
<li><p><span class="math inline">\(h_i(z^*) \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Primal Feasibility)</p></li>
<li><p><span class="math inline">\(\lambda_i^* \geq 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Dual Feasibility)</p></li>
<li><p><span class="math inline">\(\lambda_i^* h_i(z^*) = 0\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span> (Complementary Slackness)</p></li>
</ol>
<p>Conversely, if <span class="math inline">\((z^*, \lambda^*)\)</span> satisfy 1-4, then <span class="math inline">\(z^*\)</span> is a solution to <em>(P)</em> and <span class="math inline">\(\lambda^*\)</span> is a solution to <em>(D)</em>.</p>
</div>
<p>Back to <strong>SVM</strong> problem:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{(\beta, \beta_{0})}{\operatorname{min}} &amp;~\frac{|| \beta ||^2}{2}\\
s.t. &amp;~y_i(&lt;x_i, \beta&gt; + \beta_0) \geq 1 ~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(z = (\beta, \beta_0)\)</span>, <span class="math inline">\(f(z) = \frac{|| \beta ||^2}{2}\)</span>, <span class="math inline">\(h_i(z) = 1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\)</span>.</p>
<p>Then we have <span class="math inline">\(\mathcal L(\beta, \beta_0, \lambda) = \frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\Big)\)</span> and <span class="math inline">\(g(\lambda) = \underset{(\beta, \beta_{0})}{\operatorname{min}} \{\frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt; + \beta_0)\Big)\}\)</span></p>
<p>Case 1: If <span class="math inline">\(\sum_{i=1}^n \lambda_iy_i \neq 0\)</span>, <span class="math inline">\(g(\lambda) = -\infty\)</span></p>
<p>Case 2: If <span class="math inline">\(\sum_{i=1}^n \lambda_iy_i = 0\)</span>, <span class="math inline">\(g(\lambda) = \underset{\beta}{\operatorname{min}} \{\frac{|| \beta ||^2}{2} + \sum_{i=1}^n \lambda_i \Big(1 - y_i(&lt;x_i, \beta&gt;)\Big)\}\)</span>.</p>
<p>To find this, we can find the critical point for the above problem: <span class="math inline">\(\vec 0 = \beta - \sum_{i=1}^n \lambda_i y_i x_i\)</span>. Thus, plug the <span class="math inline">\(\beta\)</span> back to the above expression, we have: <span class="math inline">\(g(\lambda) = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j &lt;x_i, x_j&gt; + \sum_{i=1}^n \lambda_i\)</span>.</p>
<p>As a result, we have the Dual of SVM <em>(D)</em>:</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\lambda \geq 0}{\operatorname{max}} g(\lambda) &amp;= \underset{\lambda \in R^n}{\operatorname{max}} \{-\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j &lt;x_i, x_j&gt; + \sum_{i=1}^n \lambda_i \}\\
&amp;\qquad s.t. ~
\begin{cases}\sum_{i=1}^n \lambda_iy_i = 0\\
\lambda_i \geq 0
\end{cases}~\forall i=1,\cdots,n
\end{aligned}
\]</span></p>
<p>Now, using KKT conditions: let <span class="math inline">\(\lambda^*\)</span> be a solution to <em>(D)</em>. From the (Stationarity), we have <span class="math inline">\(\beta^* = \sum_{i=1}^n \lambda_i^* y_i x_i\)</span>. From the (Complementary Slackness), we have</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\lambda_i^*\Big(1-y_i(&lt;x_i, \beta^*&gt; + \beta_0^*)\Big) = 0,  ~\forall i\\
\underset{\text{choose } \hat{i} \text{ s.t. } \lambda^*_{\hat{i}} &gt; 0}{\Rightarrow} &amp;~ 1 - y_{\hat i} (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*) = 0\\
\Rightarrow &amp;~ 1 = y_{\hat i} (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*)\\
\underset{y_i^2=1}{\Rightarrow} &amp;~ y_{\hat i} = (&lt;x_{\hat i}, \beta^*&gt; + \beta_0^*)\\
\Rightarrow &amp;~ (y_{\hat i} - &lt;x_{\hat i}, \beta^*&gt;) = \beta_0^*
\end{aligned}
\]</span></p>
<p>Then, we go from <span class="math inline">\(\lambda^*\)</span> to <span class="math inline">\((\beta^*, \beta_0^*)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\beta^* &amp;= \sum_{i=1}^n \lambda_i^* y_i x_i\\
\beta_0^* &amp;= y_{\hat i} - &lt;x_{\hat i}, \beta^*&gt;, \text{ where } \hat{i} \text{ s.t. } \lambda^*_{\hat{i}} &gt; 0
\end{aligned}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="summary.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-LC.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
