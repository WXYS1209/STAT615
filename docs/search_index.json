[["uncertainty-quantification.html", "Chapter 9 Uncertainty Quantification 9.1 Bayes Perspective 9.2 Gaussians in \\(\\mathcal R^d\\) 9.3 Diagonalization of \\(\\Sigma\\) and Sampling for \\(N(0, \\Sigma)\\) 9.4 Stochastic Processes and Gaussian Processes", " Chapter 9 Uncertainty Quantification Imagine there is an unknown quantity \\(u\\) that we can’t observe directly. However, we may obtain a collection of observations \\(Y = (y_i, \\cdots, y_p) \\in \\mathcal R^p\\). Then the Forward Problem is \\(\\underset{\\text{unknown}}{u \\in Z} \\to \\underset{\\text{observed}}{Y}\\). In a sense, when we talk about science, we are talking about use \\(Y\\) to learn about \\(u\\). In practice, there are at least two “sources” of uncertainty associated to the picture: \\(u \\to Y\\): \\(u\\) is unknown. Observations are usually contaminated by some sort of noise so that in general \\(Y\\) is not simply a function of the unknown \\(u\\). In statistics, we may model the “likelihood” or “probability” of observing \\(Y\\) given a certain value of \\(u\\). This quantifies our uncertainty of the possible values \\(Y\\) given the unknown \\(u\\) through the probability mass function: \\(P(Y = y, | u)\\) or through a density function: \\(P(Y \\in C | u) = \\int_C p(y | u) dy\\). Example 9.1 Let \\(u \\in (0,1)\\) be the probability that the outcome of a certain biased coin flip is H. Suppose that this coin is flipped \\(p\\) times and we get \\(y_i = \\begin{cases}1 &amp;i\\text{-th is H}\\\\0 &amp; i\\text{-th is T}\\end{cases}\\). In this case, the likelihood is \\[ P(Y = y | u) = u^{\\sum_{i=1}^p y_i} (1-u)^{p - \\sum_{i=1}^p y_i}, ~y_i \\in \\{0,1\\} \\] Example 9.2 Regression: \\(u: \\mathcal X \\to \\mathcal R\\) (Hidden regression function). \\(y_i = u(x_i) + \\varepsilon_i\\), \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). \\[ p(y|u) = \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^p} exp(-\\frac{||y - (u(x_1), \\cdots, u(x_n)||^2}{2\\sigma^2}) \\] Classification with logistic model: Given \\(u: \\mathcal X \\to \\mathcal R\\), define: \\(q_i = \\frac{exp(u(x_i))}{1+exp(u(x_i))}\\) for \\(x_1, \\cdots, x_p\\). Then, \\(y_i = \\begin{cases}1 &amp;\\text{with prob } q_i\\\\0 &amp; \\text{with prob } 1-q_i\\end{cases}\\). Thus, \\[ P(y | u) = \\prod_{i=1}^p q_i^{y_i} (1- q_i)^{1-y_i} = \\prod_{i=1}^p \\frac{exp(y_iu(x_i))}{1+exp(u(x_i))} \\] Example 9.3 (Density Estimation) Let \\(\\mathcal X = \\mathcal R^d\\) and suppose that \\(u: \\mathcal X \\to \\mathcal R\\) is an unknown density function (\\(u(x) \\geq 0\\), \\(\\forall x\\), and \\(\\int_{\\mathcal R^d} u(x) dx = 1\\)), \\(y_1, \\cdots, y_p \\underset{i.i.d}{\\sim} u\\). Thus, the likelihood of \\(Y=(y_1, \\cdots, y_p)\\) is: \\[ p(y|u) = u(y_1) \\cdots u(y_p) \\] So far we have talked about the distribution of \\(Y\\) given \\(u\\) (Forward Problem). This models the uncertainty of our “measurement” if we had access to \\(u\\). However, how do we learn \\(u\\) from \\(Y\\) (Inverse Problem)? Moreover, how do we quantify the uncertainty of the unknown \\(u\\) before observing \\(Y\\), and after observing \\(Y\\)? 9.1 Bayes Perspective Bayes Rule: \\(p(u | y) \\propto p(y | u) \\pi(u)\\). For the moment, we consider the case where \\(u \\in \\mathcal R^d\\). Suppose \\(\\pi(u)\\) is then interpreted as a density in \\(\\mathcal R^d\\). The posterior is usually also a density in \\(\\mathcal R^d\\). Example 9.4 (Example 9.1 Continued) Let \\(u \\in (0,1)\\), the likelihood is \\(Bernoulli(u):\\) \\[ P(Y = y | u) = u^{\\sum_{i=1}^p y_i} (1-u)^{p - \\sum_{i=1}^p y_i}, ~y_i \\in \\{0,1\\}, \\] the prior is \\(Beta(\\alpha, \\beta)\\): \\[ \\pi(u) = \\frac{u^{\\alpha-1}(1-u)^{\\beta-1}}{\\Gamma(\\alpha, \\beta)}, ~\\alpha, \\beta &gt;0. \\] Then the posterior is \\[ p(u | y) \\propto p(y |u) \\pi(u) \\propto u^{\\sum_{i=1}^p y_i + \\alpha - 1} (1-u)^{p - \\sum_{i=1}^p y_i + \\beta - 1}, \\] that is, given \\(Y=y\\), \\(u\\) is \\(Beta(\\sum_{i=1}^p y_i + \\alpha, p - \\sum_{i=1}^p y_i + \\beta)\\) Example 9.5 (Example 9.2 Continued) Let \\(u \\in \\mathcal R^d\\) and \\(x_1, \\cdots, x_p \\in \\mathcal R^d\\), the likelihood is: \\[ p(y|u) = \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^p} exp(-\\frac{||y - (u(x_1), \\cdots, u(x_n)||^2}{2\\sigma^2}), \\] the prior is \\[ \\pi(u) = \\frac{1}{(\\sqrt{2 \\pi \\lambda^{-1}})^p} exp(-\\frac{\\lambda}{2} ||u||^2_{\\mathcal R^d}), ~i.e. ~ u \\sim N(\\vec 0, \\frac{1}{\\lambda} I). \\] Let \\(X = (x_1, \\cdots, x_p)^T_{p \\times d}\\). Then the posterior is \\[ \\begin{aligned} p(u | y) &amp;\\propto p(y |u) \\pi(u) \\\\ &amp;\\propto exp \\Big(&lt;\\frac{X^Ty}{\\sigma^2}, u&gt; - \\frac{1}{2} &lt;(\\lambda I + \\frac{X^TX}{\\sigma^2})u, u&gt;\\Big)\\\\ &amp;= N(\\mu, \\Sigma), \\end{aligned} \\] where \\(\\mu = \\frac{1}{\\sigma^2} (\\lambda I + \\frac{1}{\\sigma^2} (X^TX))^{-1} X^Ty\\) and \\(\\Sigma= (\\lambda I + \\frac{1}{\\sigma^2} (X^TX))^{-1}\\). With the posterior, we can then compute the following: Posterior Mean: \\[ \\begin{aligned} \\hat u_{PM} = E[u_i | y] &amp;= \\frac{\\int_{\\mathcal R^d} u_i p(y | u) \\pi(u) du_1 \\cdots du_d}{\\int_{\\mathcal R^d} p(y | u) \\pi(u) du_1 \\cdots du_d}\\\\ &amp;= \\frac{\\int_{\\mathcal R^d} u_i p(y | u) \\pi(u) du_1 \\cdots du_d}{Z(y)} \\end{aligned} \\] where \\(Z(y)\\) is the normalization constant. Then, based on the observations (and model), we estimate the i-th coordinate of the unknown as \\(E[u_i | y]\\). We could also estimate \\(u_i^2 cos(u_j)\\) with: \\(E[u_i^2 cos(u_j) | y] = \\frac{\\int_{\\mathcal R^d} u_i^2cos(u_j) p(y | u) \\pi(u) du_1 \\cdots du_d}{Z(y)}\\) Posterior Covariance: \\(Cov(u_i, u_j | y)\\) or \\(Var(u_i|y) = E[u_i^2 | y] - (E[u|y])^2\\). A measure of how certain we are about the estimation of \\(u_i\\). Map (Maximum a posterior): \\[ \\begin{aligned} u^* &amp;= \\underset{u}{\\operatorname{arg max}} \\pi(u) p(y | u)\\\\ &amp;= \\underset{u}{\\operatorname{arg max}} log(\\pi(u)) + log(p(y | u))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} -log(\\pi(u)) - log(p(y | u)) \\end{aligned} \\] Both 1 and 2 are based on being able to take expectation w.r.t. posterior. 3 on the other hand, is not related to expectation but rather to optimization. Example 9.6 (Example 9.5 Continued) We have: \\(E[u_i |y] = \\mu_i\\) \\(Cov(u_iu_j | y) = \\Sigma_{ij}\\) The Map is determined by: \\[ \\begin{aligned} u^* &amp;= \\underset{u}{\\operatorname{arg min}} -log(p(u | y))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} -log(\\pi(u)) - log(p(y | u))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} \\frac{\\lambda}{2} ||u||^2 + \\frac{1}{\\sigma^2} \\sum_{j=1}^p (y_i - &lt;u, x_i&gt;)^2, \\end{aligned} \\] which is just the ridge regression. Remark. \\[ u^* = \\mu \\] This can be seen by direct optimization or by noticing that the Map of a Gaussian random vector coincides with its mean vector. The posterior can be interpreted as a different kind of regularization: \\[ p(u | y) \\propto \\underset{\\text{Fidality}}{p(y | u)} ~\\underset{\\text{Regularizer}}{\\pi(u)} \\] Back to Example 9.6, when \\(u | y\\) is \\(N(\\mu, \\Sigma)\\), we have formulas for \\(E[u_i | y]\\) and \\(Cov(u_i, u_j)\\). However, something like \\(E[u_1cos(u_2) exp(u_3) | y]\\) would be quite impossible to compute. Nevertheless, we can use Monte-Carlo to approximate this. The idea is that we know what the posterior is. Thus, we can try to sample \\(u^1, \\cdots, u^K \\sim N_d(\\mu, \\Sigma)\\). Then consider the empirical average: \\(\\frac{1}{K} \\sum{j=1}^K u_1^j cos(u_2^j) exp(u_3^j)\\). That is, if we can sample from the posterior, we can numerically approximate \\(E[G(u) | y] \\approx \\frac{1}{K} \\sum_{j=1}^K G(u^j)\\), where \\(u^1, \\cdots, u^K \\underset{i.i.d}{\\sim} Posterior\\). If it is difficult to sample from the posterior, we can use MCMC (Monte Carlo Markov Chain). 9.2 Gaussians in \\(\\mathcal R^d\\) Definition 9.1 A Gaussian random vector \\(Z\\) with mean zero and covariance \\(\\Sigma \\in \\mathcal R_{d\\times d}\\) (positive definite matrix) is a random vector with density (in \\(\\mathcal R^d\\)): \\[ p(z) = \\frac{1}{\\sqrt{(2 \\pi)^d ~det(\\Sigma)}} \\cdot exp(-\\frac{1}{2} &lt; \\Sigma^{-1} z, z&gt;), \\] and we write \\(Z \\sim N(0, \\Sigma)\\). We can characterize this distribution via its characteristic function: \\(\\phi_Z(v):= E[e^{\\textbf{i} &lt;Z, v&gt;}]_{V \\in \\mathcal R^d} = e^{-\\frac{1}{2} &lt;\\Sigma v, v&gt;}\\). Notice that we can see \\(Z\\) as a collection of real valued random variables: \\(Z = (Z_1, \\cdots, Z_d)\\), namely the coordinates of \\(Z\\). For convenience let’s write \\(Z\\) in the following form: \\(\\{Z_x\\}_{x \\in \\mathcal X}\\), where \\(\\mathcal X\\) is the set \\(\\mathcal X = \\{1, \\cdots, d\\}\\). Proposition 9.1 Let \\(\\mathcal X = \\{1, \\cdots, d\\}\\), \\(\\{Z_x\\}_{x \\in \\mathcal X}\\) a collection of real valued random variables is a Gaussian iff \\(\\forall\\) (finite) subset \\(\\mathcal X&#39; \\subseteq \\mathcal X\\), the collection \\(\\{Z_x\\}_{x \\in \\mathcal X&#39;}\\) is a Gaussian. Proof. “\\(\\Leftarrow\\)”: It is obvious since we can pick \\(\\mathcal X&#39; = \\mathcal X\\). “\\(\\Rightarrow\\)”: For simplicity, suppose \\(\\mathcal X&#39; = \\{1, \\cdots, m\\}\\), \\(m \\leq d\\). Let \\(\\tilde Z = \\{Z_x\\}_{x \\in \\mathcal X&#39;}\\), \\(w \\in \\mathcal R^m\\). Then we have: \\[ \\begin{aligned} E[e^{\\textbf{i} &lt;\\tilde Z, w&gt;_{\\mathcal R^m}}] &amp;= E[e^{\\textbf{i} &lt;\\tilde Z,uw&gt;_{\\mathcal R^m}}]\\\\ &amp;(u = (w_1, \\cdots, w_m, 0, \\cdots, 0))\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\Sigma u, u &gt;_{\\mathcal R^d}}\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\tilde \\Sigma w, w &gt;_{\\mathcal R^m}}, \\end{aligned} \\] where \\(\\tilde \\Sigma \\in \\mathcal R^{m \\times m}\\) and \\(\\tilde \\Sigma_{ij} = \\Sigma_{ij}\\), \\(i,j = 1,\\cdots,m\\). Since the characteristic function of \\(\\tilde Z\\) is that of a Gaussian in \\(\\mathcal R^m\\), we have \\(\\tilde Z \\sim N_m(0, \\tilde \\Sigma)\\). Proposition 9.2 Let \\(Z\\) be a Gaussian random vector in \\(\\mathcal R^d: Z \\sim N_d(0, \\Sigma)\\). If \\(A \\in \\mathcal R^{m \\times d}\\) is a matrix, then \\(U = AZ\\) is a Gaussian in \\(\\mathcal R^m\\) and \\(U \\sim N_m(0, A\\Sigma A^T)\\). If \\(Z \\sim N_d(0, \\Sigma)\\), \\(\\tilde Z \\sim N_m(0, \\tilde \\Sigma)\\), and \\(Z, \\tilde Z\\) are independent, then \\(Z, \\tilde Z \\sim N_{d+m}(0, \\left(\\begin{matrix}\\Sigma&amp;0\\\\0&amp;\\tilde \\Sigma\\end{matrix}\\right))\\). \\(Z\\) is a Gaussian in \\(\\mathcal R^d\\) iff every linear combination of its coordinates is univariate (单元的) Gaussian: \\[ Z = (Z_1, \\cdots, Z_d) \\sim N_d \\Leftrightarrow \\forall a_1, \\cdots, a_d \\in \\mathcal R, ~\\sum_{j=1}^d a_j Z_j \\sim N_1. \\] Suppose that \\(Z \\sim N_d(0, \\Sigma)\\), and let \\(v, \\tilde v \\in \\mathcal R^d\\) be two vectors, then \\[ Cov(&lt;Z, v&gt;, &lt;Z, \\tilde v&gt;) = &lt;\\Sigma v, \\tilde v&gt; = &lt;\\Sigma \\tilde v, v&gt; \\] Suppose \\(Z = (Z_1, \\cdots, Z_d)\\) is Gaussian with cov. matrix \\(\\Sigma\\), then the coordinates of \\(Z\\) are independent random variables iff \\(\\Sigma\\) is a diagonal matrix. Proof. \\[ \\begin{aligned} E[e^{\\textbf{i}&lt;w,U&gt;_{\\mathcal R^m}}] &amp;= E[e^{\\textbf{i}&lt;w,AZ&gt;_{\\mathcal R^m}}]\\\\ &amp;\\underset{&lt;A,B&gt; = tr(A^TB)}{=} E[e^{\\textbf{i}&lt;A^Tw,Z&gt;_{\\mathcal R^m}}]\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\Sigma (A^Tw), A^Tw&gt;_{\\mathcal R^d}}\\\\ &amp;= e^{-\\frac{1}{2} &lt; A \\Sigma A^Tw, w&gt;_{\\mathcal R^d}} \\end{aligned} \\] “\\(\\Rightarrow\\)”: Let \\(U = \\sum_{j=1}^d a_j Z_j\\) be a linear combination of the coordinates of \\(Z\\). By 1, we have \\(U=AZ\\), where \\(A = (a_1, \\cdots, a_d)\\), is a Gaussian. “\\(\\Leftarrow\\)”: Let \\(a \\in \\mathcal R^d\\), with \\(a = (a_1, \\cdots, a_d)\\). Then, \\(E[e^{\\textbf{i}&lt;a,Z&gt;}] = E[e^{\\textbf{i} \\sum_{j=1}^d a_j Z_j}] \\underset{\\sum_{j=1}^d a_j Z_j \\sim N(0, \\sigma_a^2)}{=} e^{-\\frac{1}{2} \\sigma_a^2}\\), where \\(\\sigma_a^2 = Var(\\sum_{j=1}^d a_j Z_j) = &lt;\\Sigma a, a&gt;_{\\mathcal R^d}\\), \\(\\Sigma\\) is the covariance matrix of \\(Z = (Z_1, \\cdots, Z_d)\\). Thus, \\(E[e^{\\textbf{i}&lt;a,Z&gt;}] = e^{-\\frac{1}{2} &lt;\\Sigma a, a&gt;_{\\mathcal R^d}}\\), \\(\\forall a \\in \\mathcal R^d\\), which means that \\(Z \\sim N_d\\). \\[ \\begin{aligned} Cov(&lt;Z, v&gt;, &lt;Z, \\tilde v&gt;) &amp;= Cov(\\sum_{i=1}^n Z_i v_i, \\sum_{j=1}^n Z_j, \\tilde v_j)\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n v_i \\tilde v_j Cov(Z_i, Z_j)\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n (\\Sigma_{ij}v_i \\tilde v_j)\\\\ &amp;= &lt;\\Sigma v, \\tilde v&gt; = &lt;\\Sigma \\tilde v, v&gt;. \\end{aligned} \\] 9.3 Diagonalization of \\(\\Sigma\\) and Sampling for \\(N(0, \\Sigma)\\) Theorem 9.1 If \\(\\Sigma \\in \\mathcal R^{d \\times d}\\) is symmetric positive semi definite, then there are vectors \\(v_1, \\cdots, v_d \\in \\mathcal R^d\\) and non-negative \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_d \\geq 0\\) s.t \\(\\Sigma v_i = \\lambda_i vi_i\\) and \\(&lt;v_i, v_j&gt;_{\\mathcal R^d} = \\begin{cases}1 &amp; i=j\\\\0 &amp; i \\neq j\\end{cases}\\). In short: \\(\\Sigma\\) induces an orthonormal basis for \\(\\mathcal R^d\\) formed of eigen vectors of \\(\\Sigma\\). Moreover, all of \\(\\Sigma\\)’s eigen values are non-negative. How to sample from \\(Z \\sim N_d(0, \\Sigma)\\): Since \\(Z \\in \\mathcal R^d\\), we notice that \\(Z = \\sum_{i=1}^d &lt;Z, v_i&gt;_{\\mathcal R^d} v_i\\), where \\(\\{v_i\\}\\) is an orthonormal basis for \\(\\mathcal R^d\\) and are fixed. We then have \\(&lt;Z, v_i&gt; \\sim N_1(0, &lt;\\Sigma v_i, v_i&gt;) = N_1(0, \\lambda_i &lt;v_i, v_i&gt;) = N_1(0, \\lambda_i)\\). Moreover, for \\(i \\neq j\\): \\[ \\begin{aligned} Cov(&lt;Z, v_i&gt;, &lt;Z, v_j&gt;) &amp;= &lt;\\Sigma v_i, v_j&gt;\\\\ &amp;= &lt;\\lambda_i v_i, v_j&gt;\\\\ &amp;= \\lambda &lt;v_i, v_j&gt;\\\\ &amp;= 0 \\end{aligned} \\] which means that the random variables \\(&lt;Z, v_1&gt;,\\cdots, &lt;Z,v_d&gt;\\) are independent. So, we can write: \\(Z = \\sum_{i=1}^d \\sqrt{\\lambda_i}(\\frac{&lt;Z, v_i&gt;}{\\sqrt{\\lambda_i}}) v_i\\ = \\sum_{i=1}^d \\sqrt{\\lambda_i}\\xi_i v_i\\), where \\(\\xi_1, \\cdots, \\xi_d\\) are i.i.d \\(N(0,1)\\) random variables. Thus, to sample from \\(N_d(0, \\Sigma)\\), we can do the following: Find the eigen pairs \\(\\{(\\lambda_i, v_i)\\}_{i=1,\\cdots,d}\\) for \\(\\Sigma\\). Sample \\(\\xi_1, \\cdots, \\xi_d \\sim N(0,1)\\). Set \\(Z = \\sum_{i=1}^d \\sqrt{\\lambda_i}\\xi_i v_i\\). Remark. The above is preciselt what we need to sample form Gaussian in more general settings. In particular, for Gaussians in certain RKHSs. 9.4 Stochastic Processes and Gaussian Processes Definition 9.2 (Stochastic Processes) Let \\(\\mathcal X\\) be a set. A stochastic process over \\(\\mathcal X\\) is a collection of random variables \\(\\{Z_x\\}_{x \\in \\mathcal X}\\). Definition 9.3 (Gaussian Processes) A Gaussian Process over \\(\\mathcal X\\) is a stochastic process \\(\\{Z_x\\}_{x \\in \\mathcal X}\\) with the property that \\(\\forall\\) finite subset \\(\\mathcal X&#39;\\) of \\(\\mathcal X\\), the vector \\(\\{Z_x\\}_{x \\in \\mathcal X&#39;}\\) is a Gaussian. Definition 9.4 (Covariance Funciton) Suppose \\(Z = \\{Z_x\\}_{x \\in \\mathcal X}\\) is a Gaussian Process over \\(\\mathcal X\\). We define \\(Z\\)’s covariance function as: \\[ K: \\mathcal X \\times \\mathcal X \\to \\mathcal R\\\\ K(x, \\tilde x):= Cov(Z_x, Z_{\\tilde x}) \\] We will write \\(Z \\sim N(0, K)\\) for simplicity. Proposition 9.3 The covariance function of a Gaussian process over \\(\\mathcal X\\) is a kernel over \\(\\mathcal X\\): \\(x_1, \\cdots, x_n \\in \\mathcal X\\), \\(\\{Z_{x_1}, \\cdots, Z_{x_n}\\}\\) is a n-dimensional Gaussian. Thus \\(\\Sigma\\) is positive semi definite and \\(\\Sigma_{ij} = K(x_i, x_j)\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
