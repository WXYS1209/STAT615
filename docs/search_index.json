[["index.html", "STAT 615 Note Book Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " STAT 615 Note Book Peter Wang 2023-05-09 Chapter 1 About 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: "],["classification-problem-bayes-classifier.html", "Chapter 2 Classification Problem &amp; Bayes Classifier 2.1 Notation 2.2 Classification Problem 2.3 Other Topics", " Chapter 2 Classification Problem &amp; Bayes Classifier 2.1 Notation \\(\\mathcal X\\) = input space; \\(\\mathcal Y\\) = output space \\(x\\) = feature vector, input, data point; \\(y\\) = label \\((X, Y)\\) = random variable. Supervised Learning: From given data set \\((x_1,y_1), \\cdots, (x_n,y_n)\\), find \\(\\mathfrak f\\), such that \\(\\mathfrak f: ~x (\\in \\mathcal X) \\to y (\\in \\mathcal Y)\\). \\((x_1,y_1), \\cdots, (x_n,y_n)\\) come from a distribution of input-output pairs. Distribution: \\(\\rho\\), is a probability distribution over \\(\\mathcal X \\times \\mathcal Y\\), the “Ground Truth”. 2.2 Classification Problem Given \\(\\mathfrak f : \\mathcal X \\to \\mathcal Y\\), compute \\(P_{(X,Y) \\sim \\rho}(\\mathfrak f(X) \\neq Y)\\), which is average misclassification error (AME). 2.2.1 Goal To build the “best” possible classifier, that is find \\(\\mathfrak f\\) that makes the AME as small as possible. 2.2.2 Questions Does there exist a “best” classifier (relative to AME)? Theorem 2.1 (Bayes Classifier) Given \\(\\rho\\) distribution over \\(\\mathcal X \\times \\mathcal Y\\), \\(l\\) = 0-1 loss; \\(\\mathcal Y\\) = {0,1} (binary problem); \\(\\mathfrak f_B(x)\\) := \\(\\begin{cases}1 &amp; if ~P_{(X,Y) \\in \\rho}(Y=1 \\mid X=x) \\geq P_{(X,Y) \\in \\rho}(Y=0 \\mid X=x)\\\\0 &amp; o.w.\\end{cases}\\). Then \\(\\mathfrak f_B\\) minimizes the AME. Proof. \\(\\forall ~\\mathfrak f: \\mathcal X \\to \\{0,1\\}\\), we have \\[ \\begin{aligned} P(\\mathfrak f(X) \\neq Y) &amp;= E[1_{\\mathfrak f(X) \\neq Y}]\\\\ &amp;= E\\Big[E[1_{\\mathfrak f(X) \\neq Y} \\mid X]\\Big]\\\\ &amp;= E\\Big[1_{\\mathfrak f(X) \\neq 1} \\cdot P(Y=1 \\mid X) + 1_{\\mathfrak f(X) \\neq 0} \\cdot P(Y=0 \\mid X)\\Big]\\\\ \\end{aligned} \\] When \\(P(Y=1 \\mid X) \\geq P(Y=0 \\mid X)\\), we have \\[ \\begin{aligned} 1_{\\mathfrak f(X) \\neq 1} \\cdot P(Y=1 \\mid X) + 1_{\\mathfrak f(X) \\neq 0} \\cdot P(Y=0 \\mid X) &amp;\\geq 1_{\\mathfrak f(X) \\neq 1} \\cdot P(Y=0 \\mid X) + 1_{\\mathfrak f(X) \\neq 0} \\cdot P(Y=0 \\mid X)\\\\ &amp;= P(Y=0 \\mid X) \\cdot (1_{\\mathfrak f(X) \\neq 1} + 1_{\\mathfrak f(X) \\neq 0})\\\\ &amp;= P(Y=0 \\mid X)\\\\ &amp;=_{\\mathfrak f_B(X) = 1} ~P(\\mathfrak f_B(X) \\neq Y \\mid X) \\end{aligned} \\] When \\(P(Y=0 \\mid X) &gt; P(Y=1 \\mid X)\\), we similarly have \\[ \\begin{aligned} 1_{\\mathfrak f(X) \\neq 1} \\cdot P(Y=1 \\mid X) + 1_{\\mathfrak f(X) \\neq 0} \\cdot P(Y=0 \\mid X) &gt; P(\\mathfrak f_B(X) \\neq Y \\mid X) \\end{aligned} \\] As a result, we have \\[ \\begin{aligned} P(\\mathfrak f(X) \\neq Y) &amp;\\geq E[P(\\mathfrak f_B(X) \\neq Y \\mid X)]\\\\ &amp;= E\\Big[E[1_{\\mathfrak f_B(X) \\neq Y} \\mid X]\\Big]\\\\ &amp;= P(\\mathfrak f_B(X) \\neq Y) \\end{aligned} \\] which means that \\(\\mathfrak f_B\\) minimizes the AME. Can we construct this “best” classifier (if we only observe the data \\((x_1, y_1), \\cdots, (x_n, y_n)\\))? If we can not build this classifier from the observed data, then what can we do in that case? 2.2.3 The 0-1 Loss \\[\\begin{aligned}l:\\{1, \\cdots, k\\} \\times \\{1, \\cdots, k\\} &amp;\\to \\mathcal R\\\\(\\mathcal Y, \\mathcal Y&#39;) &amp;\\to \\mathcal R\\end{aligned}; \\quad l(y,y&#39;) = \\begin{cases}1 &amp; if ~y\\neq y&#39;\\\\0 &amp; if ~y = y&#39;\\end{cases}.\\] Relative to this loss function, and relative to the distribution \\(\\rho\\), we can define the risk of a given classifier \\(\\mathfrak f: \\mathcal X \\to \\mathcal Y = \\{1, \\cdots, k\\}\\), \\[ \\begin{aligned} R(\\mathfrak f) :&amp;= E_{(X, Y) \\in \\rho}[l(\\mathfrak f(X), Y)] \\\\ &amp;= P(\\mathfrak f(X) \\neq Y). \\end{aligned} \\] To find the “best” classifier is to solve \\(min_{\\mathfrak f} R(\\mathfrak f)\\). Example 2.1 \\(y_1\\) = stop sign, \\(y_2\\) = 50 mph, \\(y_3\\) = 40 mph. The classifier classifies \\(\\mathfrak f: x \\to y_2\\), when \\((x,y)\\) was such that \\(y\\) = stop sign. Then the 0-1 loss is \\(l(\\mathfrak f(x), y) = l(y_2, y_1) = 1\\). Predicting 50 mph, when \\(y\\) = stop sign seems to be worse than predicting 40 mph, when \\(y\\) = 50 mph. Thus other loss function may be better. 2.2.4 Observations \\(\\mathfrak f_B\\) depends on \\(\\rho\\): if \\((X,Y) \\sim \\rho&#39;\\), you can not expect \\(\\mathfrak f_B\\) constructed from \\(\\rho\\) to do well at classifying \\((X,Y)\\). Bayes Rule: \\[ P(Y=1 \\mid X=x) = \\frac{\\rho_{X\\mid Y=1}(x) \\cdot P(Y=1)}{\\rho_X(x)} \\] Thus, \\[\\begin{aligned}P(Y=1 \\mid X=x) \\geq P(Y=0 \\mid X=x) &amp;\\Leftrightarrow \\rho_{X\\mid Y=1}(x) \\cdot P(Y=1) \\geq \\rho_{X\\mid Y=0}(x) \\cdot P(Y=0)\\\\ &amp;\\Leftrightarrow P(X=x, Y=1) \\geq P(X=x, Y=0)\\\\ &amp;(\\text{useful when the input space } \\mathcal X \\text{ is discrete}) \\end{aligned} \\] In general there are multiple solutions to the problem. However, all of them have the form of \\(\\mathfrak f_B\\). \\(R^*_B = min_{\\mathfrak f} ~P(\\mathfrak f(X) \\neq Y) = P(\\mathfrak f_B(X) \\neq Y)\\) is Bayes Risk, which indicates how accurate the classifier is. Notice that regardless of what \\(\\rho\\) is, \\(R^*_B \\leq \\frac{1}{2}\\). Both \\(\\mathfrak f_B\\) and \\(R^*_B\\) depend on \\(\\rho\\). But also if we were to change the loss function, the formula for \\(\\mathfrak f_B\\) and the value of \\(R^*_B\\) would change. 2.2.5 Examples Example 2.2 \\(\\mathcal X = \\{a,b,c,d,e,f\\};~ \\mathcal Y = \\{0,1\\};~ \\rho\\) is a distribution over \\(\\mathcal X \\times \\mathcal Y\\). a b c d e f 0 0.1 0.2 0.5 0.3 0.8 0.9 1 0.9 0.3 0.1 0.3 0.1 0.3 \\(P(X=x, Y=y) = \\frac{q(x,y)}{M}\\), where \\(q(\\cdot, \\cdot)\\) is element of the above matrix, and \\(M\\) is the normalization constant. According to the Bayes Rule, we have: \\[ \\begin{aligned} \\mathfrak f_B(x) &amp;= \\begin{cases} 1 &amp; if ~ P(X=x, Y=1) \\geq P(X=x, Y=0)\\\\ 0 &amp; o.w. \\end{cases}\\\\ &amp;= \\begin{cases} 1 &amp;if ~ q(x,1) \\geq q(x,0)\\\\ 0 &amp; o.w. \\end{cases} \\end{aligned} \\] Thus, a b c d e f \\(\\mathfrak f_B\\) 1 1 0 1 (0 is also OK) 0 0 Example 2.3 (Mixture of Gaussians Model) \\(\\mathcal X = \\mathcal R\\); \\(\\mathcal Y = \\{0,1\\}\\); \\((X, Y) \\sim \\rho\\), \\(P(Y=1) = \\omega_1\\), \\(P(Y=0) = \\omega_0\\); \\(X \\mid Y=1 \\sim N(\\mu_1, \\sigma_1^2)\\), \\(X \\mid Y=0 \\sim N(\\mu_0, \\sigma_0^2)\\). According to the Bayes Rule, we have: \\[ \\begin{aligned} P(Y=1 \\mid X=x) \\geq P(Y=0 \\mid X=x) &amp;\\Leftrightarrow \\frac{\\rho_1(x) \\cdot P(Y=1)}{\\rho(x)} \\geq \\frac{\\rho_0(x) \\cdot P(Y=0)}{\\rho(x)}\\\\ &amp;\\Leftrightarrow \\rho_1(x) \\cdot \\omega_1 \\geq \\rho_0(x) \\cdot \\omega_0\\\\ &amp;\\Leftrightarrow \\omega_1 \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma_1} ~exp[-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}] \\geq \\omega_0 \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma_0} ~exp[-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}]\\\\ &amp;\\Leftrightarrow log(\\frac{\\omega_1}{\\sigma_1}) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\geq log(\\frac{\\omega_0}{\\sigma_0}) - \\frac{(x-\\mu_0)^2}{2\\sigma_0^2} \\end{aligned} \\] where \\(\\rho_1(x) = P(X=x \\mid Y=1)\\), \\(\\rho_0(x) = P(X=x \\mid Y=0)\\), \\(\\rho(\\cdot)\\) is the marginal density of \\(X\\). 2.3 Other Topics “Weak Classifier” or “Probabilistic Classifier” \\(\\mathfrak g: \\mathcal X \\to [0,1]\\). \\(\\mathfrak g(x)\\) = likelihood that we are going to label 1. \\(min_{\\mathfrak g} ~E[\\mid \\mathfrak g(x) - Y \\mid]\\). Theorem 2.2 …… "],["plug-in-classifiers-similarity-classifier.html", "Chapter 3 Plug-in Classifiers (Similarity Classifier) 3.1 Notions of Consistency for Families of Binary Classifier 3.2 Functions of \\(\\hat \\eta_n\\) 3.3 Consistency 3.4 Strong Consistency", " Chapter 3 Plug-in Classifiers (Similarity Classifier) For binary classification with 0-1 loss: \\(\\mathfrak f_B(x)\\) = \\(\\begin{cases}1 &amp; if ~P_{(X,Y) \\in \\rho}(Y=1 \\mid X=x) \\geq P_{(X,Y) \\in \\rho}(Y=0 \\mid X=x)\\\\0 &amp; o.w.\\end{cases}\\) is Motivate “plug-in” or similarity classifier; \\(\\mathfrak f_B \\in \\underset{\\mathfrak f: \\mathcal X \\to \\{0,1\\}}{\\operatorname{argmin}} ~ P(\\mathfrak f(X) \\neq Y)\\) is Motivate ERM (Empirical Risk Minimization). For this chapter, we will talk about the first one. Define \\(\\eta(x) := P(Y=1 \\mid X=x)\\), then we can rewrite \\(\\mathfrak f_B(x)\\) as: \\(\\mathfrak f_B(x) = \\begin{cases}1 &amp; \\eta(x) \\geq \\frac{1}{2}\\\\0 &amp; o.w.\\end{cases}.\\) Idea: Replace \\(\\eta(x)\\) with \\(\\hat \\eta_n(x)\\) that is built from our training data \\((x_1,y_1), \\cdots, (x_n,y_n)\\): \\(\\hat{\\mathfrak f}_n(x) = \\begin{cases}1 &amp; \\hat \\eta_n(x) \\geq \\frac{1}{2}\\\\0 &amp; o.w.\\end{cases}.\\) Lemma 3.1 Let \\(\\mathfrak f\\) be an arbitrary binary classifier. Then \\[ 0 \\leq R(\\mathfrak f) - R_B^* = E\\Big[\\mid 2 \\eta(X) - 1 \\mid \\cdot 1_{\\mathfrak f(X) \\neq \\mathfrak f_B(X)}\\Big] \\leq 2 E\\Big[ \\mid \\eta(X) - \\alpha(X) \\mid \\Big], \\] where we suppose \\(\\mathfrak f(X) = \\begin{cases}1 &amp; \\alpha(X) \\geq \\frac{1}{2}\\\\0 &amp; o.w.\\end{cases}.\\) Proof. As \\[ \\begin{aligned} E[1_{\\mathfrak f(X) \\neq Y} - 1_{\\mathfrak f_B(X) \\neq Y} \\mid X] &amp;= (1_{\\mathfrak f(X) \\neq 1} - 1_{\\mathfrak f _B(X) \\neq 1}) \\cdot P(Y=1 \\mid X) + (1_{\\mathfrak f(X) \\neq 0} - 1_{\\mathfrak f _B(X) \\neq 0}) \\cdot P(Y=0 \\mid X)\\\\ &amp;= (1_{\\mathfrak f(X) \\neq 1} - 1_{\\mathfrak f _B(X) \\neq 1}) \\cdot \\eta(X) + (1_{\\mathfrak f(X) \\neq 0} - 1_{\\mathfrak f _B(X) \\neq 0}) \\cdot (1-\\eta(X))\\\\ &amp;= \\begin{cases} 0 &amp; if ~ \\mathfrak f(X) = \\mathfrak f_B(X) = 1\\\\ 0 &amp; if ~ \\mathfrak f(X) = \\mathfrak f_B(X) = 0\\\\ \\mid 1-2\\eta(X) \\mid &amp; if ~ \\mathfrak f(X) =1,~ \\mathfrak f_B(X) = 0\\\\ \\mid 2\\eta(X)-1 \\mid &amp; if ~ \\mathfrak f(X) =0,~ \\mathfrak f_B(X) = 1\\\\ \\end{cases}\\\\ &amp;= \\mid 2\\eta(X)-1 \\mid \\cdot 1_{\\mathfrak f(X) \\neq \\mathfrak f_B(X)}, \\end{aligned} \\] we have: \\[ \\begin{aligned} R(\\mathfrak f) - R_B^* &amp;= E\\Big[E[1_{\\mathfrak f(X) \\neq Y} - 1_{\\mathfrak f_B(X) \\neq Y} \\mid X] \\Big]\\\\ &amp;= E\\Big[\\mid 2 \\eta(X) - 1 \\mid \\cdot 1_{\\mathfrak f(X) \\neq \\mathfrak f_B(X)}\\Big]. \\end{aligned} \\] Moreover, Case1: \\(\\mathfrak f(X) \\neq \\mathfrak f_B(X)\\), \\(\\eta(X) &lt; \\frac{1}{2}\\) In this case, \\(\\mathfrak f_B(X) = 0\\) and \\(\\mathfrak f(X) = 1\\), which means that \\(\\alpha(X) \\geq \\frac{1}{2}\\). So, \\(2 \\mid \\eta(X) - \\frac{1}{2} \\mid \\leq 2 \\mid \\eta(X) - \\alpha(X) \\mid\\). Case 2: \\(\\mathfrak f(X) \\neq \\mathfrak f_B(X)\\), \\(\\eta(X) \\geq \\frac{1}{2}\\) Similarly, we have \\(2 \\mid \\frac{1}{2} - \\eta(X) \\mid \\leq 2 \\mid \\eta(X) - \\alpha(X) \\mid\\). Thus, \\(E\\Big[\\mid 2 \\eta(X) - 1 \\mid \\cdot 1_{\\mathfrak f(X) \\neq \\mathfrak f_B(X)}\\Big] \\leq 2E\\Big[\\mid \\eta(X) -\\alpha(X) \\mid \\cdot 1_{\\mathfrak f(X) \\neq \\mathfrak f_B(X)}\\Big].\\) Corollary 3.1 If \\(\\hat{\\mathfrak f}_n(X) = \\begin{cases}1&amp; if~ \\hat \\eta_n(X) \\geq 1/2 \\\\0 &amp; o.w.\\end{cases}\\), \\(\\hat \\eta_n(X) = \\hat \\eta_n (X; (x_1, y_1), \\cdots, (x_n, y_n))\\), then \\[ E[R(\\hat{\\mathfrak f}_n)] - R^*_B \\leq 2 E\\Big[\\mid \\eta(X) - \\hat \\eta_n(X) \\mid\\Big]. \\] Remark. For \\(\\hat{\\mathfrak f}_n\\), \\(\\eta(X)\\), and \\(\\hat \\eta_n(X)\\), we have \\((X, Y) \\sim \\rho\\) and \\((x_1, y_1), \\cdots, (x_n, y_n) \\sim \\rho\\). Proof. Given \\((x_1, y_1), \\cdots, (x_n, y_n)\\), according to Lemma 3.1, we have: \\(R(\\hat{\\mathfrak f}_n) - R^*_B \\leq 2 E\\Big[\\mid \\eta(X) - \\hat \\eta_n(X) \\mid\\Big]\\). In general, we are interested in building classifier from observation \\((x_1, y_1), \\cdots, (x_n, y_n)\\): \\(\\hat {\\mathfrak f}_n: ~\\mathcal X \\to \\{0,1\\}\\), \\(\\hat {\\mathfrak f}_n (X) \\in \\{0,1\\}\\). Example 3.1 (Dependency on Data) Given \\((x_1, y_1), \\cdots, (x_n, y_n)\\), \\[ \\hat \\eta_n(X) := \\frac{\\sum_{i=1}^n Y_i exp(-\\mid X_i - X \\mid ^2 / r_n^2)}{\\sum_{i=1}^n exp(-\\mid X_i - X \\mid ^2 / r_n^2)}. \\] Then we have: \\[ \\hat {\\mathfrak f}_n(X; (x_1, y_1), \\cdots, (x_n, y_n)) = \\begin{cases} 1 &amp; if ~\\hat \\eta_n(X) \\geq \\frac{1}{2}\\\\ 0 &amp; o.w. \\end{cases} \\] Remark. \\(\\hat \\eta_n(X) \\in [0,1]\\). Goal: To build \\(\\hat{\\mathfrak f}_n\\) in such a way that \\(R(\\hat{\\mathfrak f}_n) - R^*_B\\) goes to 0 as \\(n \\to \\infty\\). 3.1 Notions of Consistency for Families of Binary Classifier We say that \\(\\{\\hat{\\mathfrak f}_n\\}_{n\\in N}\\) is consistent for the distribution \\(\\rho\\), if we have: \\[ \\underset{n \\to \\infty}{\\operatorname{lim}} (E_{(x_1, y_1), \\cdots, (x_n, y_n)} [R(\\hat{\\mathfrak f}_n)] - R^*_B) = 0 \\] Remark. \\((x_1, y_1), \\cdots, (x_n, y_n) \\underset{i.i.d}{\\sim} \\rho\\). \\(R^*_B = \\underset{\\mathfrak f}{\\operatorname{min}} P_{(X,Y) \\sim \\rho} [\\mathfrak f(X) \\neq Y]\\). \\(R(\\hat{\\mathfrak f}_n) = P_{(X,Y) \\sim \\rho} [\\hat{\\mathfrak f}_n(X) \\neq Y]\\). We say that \\(\\{\\hat{\\mathfrak f}_n\\}_{n\\in N}\\) is strongly consistent for the distribution \\(\\rho\\), if we have: \\[ \\underset{n \\to \\infty}{\\operatorname{lim}} (R(\\hat{\\mathfrak f}_n) - R^*_B) = 0, ~\\text{with probability 1 (almost sure convergence).} \\] Example 3.2 (Conceptual Explaination) Assume we have M students.Then for student m, we have: \\[ \\begin{cases} (x_1^m, y_1^m), \\cdots, (x_n^m, y_n^m) \\underset{i.i.d}{\\sim} \\rho\\\\ \\text{The plug-in classifier}: ~\\hat{\\mathfrak f}_n^m (\\cdot ~;(x_1^m, y_1^m), \\cdots, (x_n^m, y_n^m))\\\\ \\text{Risk: }R(\\hat{\\mathfrak f}_n^m) = P_{(X,Y) \\sim \\rho} [\\hat{\\mathfrak f}_n^m(X) \\neq Y] \\end{cases}. \\] Q1 (Consistency): Suppose I collect all risks that the class computed: \\(R(\\hat{\\mathfrak f}_n^1), \\cdots, R(\\hat{\\mathfrak f}_n^M)\\), and suppose I average them: \\(\\frac{1}{M} \\sum_{m=1}^M R(\\hat{\\mathfrak f}_n^m) \\approx E_{(x_1, y_1), \\cdots, (x_n, y_n)} [R(\\hat{\\mathfrak f}_n)]\\). Is it true that as \\(n \\to \\infty\\), this average risk goes to \\(R^*_B\\). Q2 (Strong Consistency): Is it true that for each \\(m\\), we have that \\(R(\\hat{\\mathfrak f}_n)\\) converges to \\(R^*_B\\) as \\(n \\to \\infty\\). We say that a family \\(\\{\\hat{\\mathfrak f}_n\\}_n\\) is universal (strong) consistent if \\(\\{\\hat{\\mathfrak f}_n\\}_n\\) is (strong) consistent for all \\(\\rho\\). Example 3.3 (Universality in Statistical Inference) Let \\(\\mu\\) be a probability distribution over \\(R\\) with well defined first moment \\(\\theta := E_{Z \\sim \\mu} (Z)\\). The goal is to build an estimator \\(\\hat \\theta_n(z_1, \\cdots, z_n)\\) that is consistent for \\(\\theta\\). \\(\\hat \\theta_n^1(z_1, \\cdots, z_n) = \\frac{1}{n} \\sum z_i ~(\\text{Sample Mean})\\), is universal strong consistent. \\(z_1, \\cdots, z_n \\underset{i.i.d}{\\sim} \\mu\\) for arbitrary \\(\\mu\\), by Law of Large Number (Strong LLN), we know that with probability 1, \\((\\hat \\theta_n^1(z_1, \\cdots, z_n) - \\theta) \\to 0\\) as \\(n \\to \\infty\\). \\(\\hat \\theta_n^2(z_1, \\cdots, z_n) = \\text{Sample Median}\\), is not universal consistent. If \\(\\mu\\) is a distribution such that its median is not equal to its mean, then it is not true that \\((\\hat \\theta_n^2(z_1, \\cdots, z_n) - \\theta) \\to 0\\) as \\(n \\to \\infty\\). However, \\(\\hat \\theta_n^2(z_1, \\cdots, z_n)\\) is consistent for \\(\\mu = N(1,1)\\) whose mean and median are the same. For the plug-in classifiers, studying (strong) consistency reduce to understanding the following two questions: Does \\(E_{(X,Y) \\sim \\rho} \\Big[\\mid \\eta(X) - \\hat \\eta_n(X) \\mid \\Big]\\) converge to 0 as \\(n \\to \\infty\\), with probability 1? If yes, \\(\\{\\hat{\\mathfrak f}_n\\}_n\\) is strong consistent for \\(\\rho\\). Does \\(E_{(x_1,y_1), \\cdots, (x_n,y_n), (X,Y) \\sim \\rho} \\Big[\\mid \\eta(X) - \\hat \\eta_n(X) \\mid \\Big]\\) converge to 0 as \\(n \\to \\infty\\)? If yes, \\(\\{\\hat{\\mathfrak f}_n\\}_n\\) is consistent for \\(\\rho\\). 3.2 Functions of \\(\\hat \\eta_n\\) \\[ \\hat \\eta_n(X; (x_1,y_1), \\cdots, (x_n,y_n)) = \\sum_{i=1}^n y_i \\omega_{in} (X; x_1, \\cdots, x_n) \\] where \\(\\omega_{in} (X; x_1, \\cdots, x_n)\\) is weight that we give to each point \\((x_i, y_i)\\) and \\(\\sum_{i=1}^n \\omega_{in} (X; x_1, \\cdots, x_n) = 1\\). In other words, to determine \\(\\hat \\eta_n(X)\\), we consider weighted averages of the labels \\(y_1, \\cdots, y_n\\). Intuitively the \\(i\\)s for which \\(\\omega_{in}\\) are larger should be ones for which \\(x_i\\) is more similar to \\(X\\). Example 3.4 (Kernel-Based Weights) \\(\\mathcal X = R^d\\), select a length-scale \\(r &gt; 0\\). \\[ \\omega_{in}(X; x_1, \\cdots, x_n) = \\frac{exp(- \\frac{|| X_i - X || ^2}{2r^2}) }{\\sum_{i=1}^n exp(-\\frac{|| X_i - X || ^2}{2r^2})}. \\] Points \\(x_i\\) for which \\(|| X - x_i || \\leq r\\) will be give higher weight than points that are such that \\(|| X - x_i || \\geq r\\). Example 3.5 (Histogram-Based Weights) \\(\\mathcal X = R^d\\), select a length-scale \\(h &gt; 0\\). For \\(d=2\\), we have: Introduce pre-weights: Provided there is at least one \\(x_i\\) in the same cell as \\(X\\): \\[ \\tilde \\omega_{in}(X; x_1, \\cdots, x_n) := \\begin{cases} 1 &amp; \\text{if } x_i \\text{ and } X ~\\text{belong to the same cell}\\\\ 0 &amp; o.w. \\end{cases} \\] Otherwise we define: \\[ \\tilde \\omega_{in}(X; x_1, \\cdots, x_n) := \\frac{1}{n} \\] Then we have: \\[ \\omega_{in}(X; x_1, \\cdots, x_n) = \\frac{\\tilde \\omega_{in}(X; x_1, \\cdots, x_n)}{\\sum_{j=1}^n \\tilde \\omega_{jn}(X; x_1, \\cdots, x_n)} \\] Example 3.6 (k-NN Based Weights) \\(k \\in [1,n]\\) \\[ \\tilde \\omega_{in}(X; x_1, \\cdots, x_n) = \\begin{cases} 1 &amp; \\text{if } x_i \\text{ is among the k-closest points to } X\\\\ 0 &amp; o.w. \\end{cases} \\] Then we have: \\[ \\omega_{in}(X; x_1, \\cdots, x_n) = \\frac{\\tilde \\omega_{in}(X; x_1, \\cdots, x_n)}{\\sum_{j=1}^n \\tilde \\omega_{jn}(X; x_1, \\cdots, x_n)} \\] There may be ties, which will be discussed later. 3.3 Consistency Theorem 3.1 (Stone's Theorem) Let \\(\\omega_{in}(X; x_1, \\cdots, x_n)\\) be such that following three conditions hold: \\(\\exists c&gt;0\\) such that \\(\\forall \\text{non-negative function } g: R^d \\to R\\), with \\(E_{(X,y) \\sim \\rho} [g(X)] &lt; \\infty\\): \\(E_{(X, x_1, \\cdots, x_n)} [\\sum_{i=1}^n \\omega_{in} (X) g(x_i)] \\leq c ~E_X [g(X)]\\); \\(\\forall a &gt; 0\\), \\(\\underset{n \\to \\infty}{\\operatorname{lim}} E_{(X, x_1, \\cdots, x_n)} [\\sum_{i=1}^n \\omega_{in}(X) ~1_{|x_i - X| &gt; a}] = 0\\) (weights are only relevant or large for \\(x_i\\) close to \\(X\\)); \\(\\underset{n \\to \\infty}{\\operatorname{lim}} E_{(X, x_1, \\cdots, x_n)} [\\underset{i=1,\\cdots, n}{\\operatorname{max}} \\omega_{in}(X)] = 0\\). Then the family of similarity classifiers \\(\\{\\hat{\\mathfrak f}_n\\}_n\\) induced by these weights is consistent for \\(\\rho\\). Proof. Goal: to show that \\(E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\eta(X) - \\hat \\eta_n(X)|] \\to 0\\) as \\(n \\to \\infty\\). Introduce \\(\\bar \\eta_n(X) := \\sum_{i=1}^n \\eta(x_i) \\omega_{in}(X)\\), then \\[ \\begin{aligned} E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\eta(X) - \\hat \\eta_n(X)|] &amp;\\leq E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\eta(X) - \\bar \\eta_n(X)|] + \\\\ &amp;\\quad E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\bar \\eta_n(X) - \\hat \\eta_n(X)|]\\\\ &amp;\\underset{C.S. Ineq}{\\leq} E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\eta(X) - \\bar \\eta_n(X)|] \\text{ (&quot;bias term&quot;)}+ \\\\ &amp;\\quad \\Big(E_{(X,Y), (x_1,y_1), \\cdots, (x_n, y_n)} [|\\bar \\eta_n(X) - \\hat \\eta_n(X)|^2]\\Big)^2 \\text{ (&quot;variance term&quot;)} \\end{aligned} \\] Then show both terms \\(\\to 0\\) as \\(n \\to \\infty\\). Remark. \\[ \\begin{aligned} E[\\hat \\eta_n(X) | X, x_1 \\cdots, x_n] &amp;= \\sum_{i=1}^n E[y_i | X, x_1, \\cdots, x_n] ~\\omega_{in}(X)\\\\ &amp;= \\sum_{i=1}^n E[y_i | X, x_1, \\cdots, x_n] ~\\omega_{in}(X)\\\\ &amp;= \\sum_{i=1}^n E[y_i | x_i] ~\\omega_{in}(X)\\\\ &amp;= \\bar \\eta_n(X) \\end{aligned}; \\] To show that the variance term \\(\\to 0\\) as \\(n \\to \\infty\\), we only need to use condition 3 in the Theorem 3.1; To show that the bias term \\(\\to 0\\) as \\(n \\to \\infty\\), we only need to use conditions 1 and 2 in the Theorem 3.1. Example 3.7 (Continuation of Example 3.5) Let \\(h^d = h_n^d\\). If \\(n h_n \\to \\infty\\) as \\(n \\to \\infty\\), and if \\(h_n^d \\to 0\\) as \\(n \\to \\infty\\), then the family of histogram classifiers \\(\\{\\hat {\\mathfrak f}_{n, h_n}\\}_{n \\in N}\\) is universally consistent. Conditions of the Stones Theorem: Technical Assumption. Locality (\\(h = h_n \\to 0\\) as \\(n \\to \\infty\\)). No weight dominates the others. (For every \\(X\\), we should use a growing number of training data points to make a prediction, i.e. the expected number of points in a cell \\(nh_n^d \\to \\infty\\) as \\(n \\to \\infty\\)) For example, let \\(\\rho = Uniform\\), \\(N_X = \\#\\{x_i ~s.t. ~x_i \\in Cell(X)\\}\\), then \\(N_X \\sim Binomial(n, P_h)\\), where \\(P_h = P(x_i \\in Cell(X)) \\propto h^d\\). As a result, \\(E(N_X) \\propto nh^d\\). Example 3.8 (Continuation of Example 3.6) \\(k \\to \\infty\\) (locality) \\(\\frac{k}{n} \\to 0\\) (Related to condition 3 in Stone’s Theorem) 3.4 Strong Consistency 3.4.1 Theorems from probability theory: Theorem 3.2 (Strong Law of Large Numbers) Let \\(Z_1, Z_2, \\cdots\\) be \\(i.i.d\\) real valued random variables, and \\(E(Z_i) = m\\). Then \\(\\frac{1}{n} \\sum_{i=1}^n (Z_i - m) \\to 0\\) almost surely as \\(n \\to \\infty\\). Theorem 3.3 (Central Limit Theorem) (It quantifies in a sense how fast is the convergence in the LLN) Suppose \\(Var(Z_i) = \\sigma^2 &lt; \\infty\\) for \\(\\{Z_i\\}\\) in Theorem 3.2 , then \\(\\sqrt n (\\frac{1}{n} \\sum_{i=1}^n (Z_i - m)) \\underset{d}{\\to} N(0, \\sigma^2)\\). Recall, what this means is that \\(\\forall t\\in R\\), we have: \\[ \\underset{n\\to \\infty}{\\operatorname{lim}} P(\\frac{\\sqrt n}{\\sigma} [\\frac{1}{n}\\sum_{i=1}^n (Z_i - m)] &gt; t) = \\int_t^\\infty \\frac{e^{-s^2/2}}{\\sqrt {2\\pi}} ds = 1-F(t) \\] Remark. CLT is still asymptotic (渐近的). Theorem 3.4 (Berry-Essen Theorem) (It is a quantitative and non-asymptotic version of CLT) \\(\\forall t\\), we have: \\[ \\Big| P(\\frac{\\sqrt n}{\\sigma} [\\frac{1}{n}\\sum_{i=1}^n (Z_i - m)] \\geq t) - \\int_t^\\infty \\frac{e^{-s^2/2}}{\\sqrt {2\\pi}} ds \\Big| \\leq \\frac{c \\gamma}{\\sigma^3 \\sqrt{n}}, \\] where \\(\\gamma = E(|Z_1|^3) &lt; \\infty\\). Remark. Berry-Essen Theorem doesn’t provide very precise information about \\(P(\\frac{\\sqrt n}{\\sigma} [\\frac{1}{n}\\sum_{i=1}^n (Z_i - m)] \\geq t)\\) when \\(t\\) is very large. 3.4.2 Concentration Inequalities Goal: To quantify precisely what is the behavior of \\(P(\\frac{\\sqrt n}{\\sigma} \\sum_{i=1}^n (Z_i - m) \\geq t)\\) (tail information). What do we want more precisely: If \\(\\frac{\\sqrt n}{\\sigma} [\\frac{1}{n}\\sum_{i=1}^n (Z_i - m)]\\) was truly a standard Gaussian random variable \\(Z\\), in that case what can we say about \\(P(Z \\geq t)\\)? What we could say is that \\(P(Z \\geq t) \\leq e^{-\\frac{t^2}{2}}\\), \\(\\forall t &gt; 0\\). The concentration inequalities we are after should give us some information similar to the one above. To get there, let us start with the basics: For a given real valued random variable \\(W\\), what can we say about \\(P(W \\geq t)\\) for \\(t \\geq 0\\)? Theorem 3.5 (Markov Inequality) Let \\(t &gt; 0\\), then we have \\[ P(W \\geq t) \\leq E(\\frac{W}{t} ~1_{W \\geq t}) \\leq \\frac{E[|W|]}{t} \\] Proof. \\[ \\begin{aligned} P(W \\geq t) &amp;= E[1_{W \\geq t}]\\\\ &amp;\\leq E[\\frac{W}{t} ~1_{W \\geq t}]\\\\ &amp;\\leq E[\\frac{|W|}{t} ~1_{W \\geq t}]\\\\ &amp;\\leq E[\\frac{|W|}{t}]. \\end{aligned} \\] Remark. Let \\(\\phi: R \\to [0,\\infty)\\) be non-decreasing. Then \\(\\forall t&gt;0\\): \\[ \\begin{aligned} P(W \\geq t) \\leq P\\Big(\\phi(W) \\geq \\phi(t)\\Big) \\underset{M.I.}{\\leq} \\frac{E[\\phi(W)]}{\\phi(t)} \\end{aligned} \\] How about we select \\(\\phi\\) to be \\(\\phi(t) := exp(t \\cdot s)\\), where \\(s&gt;0\\). Then \\(\\forall s&gt;0\\): \\[ P(W \\geq t) \\leq \\frac{E[exp(sW)]}{exp(st)} = exp(-st)E[exp(sW)]. \\] As a result, we have: \\[ P(W \\geq t) \\leq \\underset{s&gt;0}{\\operatorname{inf}}\\{exp(-st) E[exp(sW)]\\} \\text{ (Chernoff&#39;s bound)}. \\] It is clear that we need to understand the behavior of \\(E[exp(sW)]\\). The \\(W\\) we want to apply this to, for example, \\(W:=\\frac{1}{n} \\sum_{i=1}^n (Z_i - m)\\), where the \\(Z_i\\)s are independent with mean \\(m\\). Thus, the \\(W\\) for us should be of the form \\(\\sum_{i=1}^n Z_i\\), where \\(Z_i\\)s are independent and \\(E(Z_i) = 0\\). Lemma 3.2 Suppose \\(Z\\) is a r.v. with \\(E(Z) = 0\\) and \\(\\exists \\text{ constant } a,b\\), \\(s.t. ~a\\leq Z \\leq b\\) (Consequently, \\(a&lt;0\\) and \\(b&gt;0\\)). Then \\(\\forall s&gt;0\\), we have \\[ E[exp(sZ)] \\leq exp(\\frac{s^2(b-a)^2}{8}) \\] Remark. This lemma actually is saying that a bounded random variable is a sub-Gaussian random variable. We say that a random variable \\(Z\\) (with \\(E(Z) = 0\\)) is sub-Gaussian with parameter \\(\\sigma^2\\) if \\[ E(e^{sZ}) \\leq exp(\\frac{\\sigma^2 s^2}{2}), ~s &gt; 0 \\] The right hand side is exactly the moment generating function of a \\(N(0, \\sigma^2)\\). In particular, the lemma is saying that \\(Z\\) satisfying \\(a\\leq b\\) and \\(E(Z) = 0\\) is sub-Gaussian with parameter \\(\\frac{(b-a)^2}{4}\\). Proof. Let \\(Z = \\frac{b-Z}{b-a} a + \\frac{Z-a}{b-a} b\\), then we have \\[ \\begin{aligned} E[exp(sZ)] &amp;= E \\Big[ exp[s(\\frac{b-Z}{b-a} a + \\frac{Z-a}{b-a} b)] \\Big] \\\\&amp;\\underset{convexity}{\\leq} E[\\frac{b-Z}{b-a} exp(sa) + \\frac{Z-a}{b-a} exp(sb)] \\\\&amp;\\underset{E(Z) = 0}{=} \\frac{b}{b-a} exp(sa) - \\frac{a}{b-a} exp(sb) \\end{aligned} \\] Let \\(q = -\\frac{a}{b-a}\\), then \\(\\frac{b}{b-a} = 1-q\\). Then, \\[ \\begin{aligned} E[exp(sZ)] &amp;\\leq (1-q) ~exp(-s(b-a)q) + q ~exp(s(b-a)(1-q))\\\\ &amp;= exp[-s(b-a)q + log\\Big((1-q) + q~exp(s(b-a))\\Big)] \\end{aligned} \\] Let \\(\\psi(h):= -qh + log\\Big( (1-q) + q ~exp(h)\\Big)\\), then \\(\\psi(0)=0\\), \\(\\psi&#39;(0) = 0\\), \\(\\psi&#39;&#39;(h) \\leq \\frac{1}{4} ~\\forall h &gt; 0\\). By Taylor’s Theorem, \\(\\psi(h) \\leq \\frac{1}{2} (\\frac{1}{4} h^2) = \\frac{h^2}{8}\\). As a result, \\[ E[exp(sZ)] \\underset{h = s(b-a)}{\\leq} exp(\\frac{s^2(b-a)^2}{8}) \\] Theorem 3.6 (Hoeffding's Inequality) Let \\(Z_1, \\cdots, Z_n\\) be independent random variable (not necessarily identically distributed) such that: \\(E(Z_i) = 0\\), \\(\\forall i=1,\\cdots, n\\); \\(\\forall i\\), \\(\\exists \\text{ constant } a_i, b_i\\), \\(s.t. ~a_i \\leq Z_i \\leq b_i\\). Then \\(\\forall t\\), we have: \\[ P(\\frac{1}{n} \\sum_{i=1}^n Z_i \\geq t) \\leq exp(\\frac{-2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\] Proof. Let \\(W = \\frac{1}{n} \\sum_{i=1}^n Z_i\\), then \\[ P(W \\geq t) \\underset{\\text{Chernoff&#39;s Inequality}}{\\leq} \\underset{s&gt;0}{\\operatorname{inf}} \\{\\frac{E[e^{\\frac{s}{n} \\sum_{i=1}^n Z_i}]}{e^{st}}\\}, \\] where \\[ E[e^{\\frac{s}{n} \\sum_{i=1}^n Z_i}] = E\\Big[\\prod_{i=1}^n e^{\\frac{s}{n} Z_i}\\Big] \\underset{independence}{=} \\prod_{i=1}^n E[e^{\\frac{s}{n} Z_i}] \\] Using Lemma 3.2, we have: \\[ \\prod_{i=1}^n E[e^{\\frac{s}{n} Z_i}] \\leq \\prod_{i=1}^n exp(\\frac{s^2(b_i-a_i)^2}{8n^2}) = exp(\\frac{s^2}{8n^2} \\sum_{i=1}^n (b_i-a_i)^2) \\] Therefore, \\[ P(W \\geq t) \\leq \\underset{s&gt;0}{\\operatorname{inf}} \\{\\frac{exp(\\frac{s^2}{8n^2} \\sum_{i=1}^n (b_i-a_i)^2)}{exp(st)}\\} = \\underset{s&gt;0}{\\operatorname{inf}} \\{exp(\\frac{s^2}{8n^2} \\sum_{i=1}^n (b_i-a_i)^2 - st)\\} \\] For the quadratic, \\(s^* = \\frac{4tn^2}{\\sum_{i=1}^n (b_i-a_i)^2}\\) minimizes it. As a result, \\[ P(W \\geq t) \\leq exp(\\frac{-2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\] Remark. The \\(Z_i\\)s can have different distributions with \\(E(Z_i) = 0\\) and \\(a_i \\leq Z_i \\leq b_i\\). Independence is essential though. What if \\(E(Z_i) \\neq 0\\)? Let \\(\\tilde Z_i := Z_i - E(Z_i)\\) and \\(a_i \\leq \\tilde Z_i \\leq b_i\\). Then \\(\\forall t&gt;0\\), we have: \\[ P(\\frac{1}{n} \\sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) = P(\\frac{1}{n} \\sum_{i=1}^n \\tilde Z_i &gt; t) \\leq exp(\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\] \\(\\forall t &gt; 0\\), we have: \\[ \\begin{aligned} P(\\frac{1}{n} \\sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) &amp;\\leq exp(\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\end{aligned} \\] and \\[ \\begin{aligned} P(\\frac{1}{n} \\sum_{i=1}^n (E(Z_i) - Z_i) \\geq t) &amp;\\leq exp(\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\end{aligned} \\] As a result, \\[ \\begin{aligned} P\\Big(|\\frac{1}{n} \\sum_{i=1}^n (Z_i - E(Z_i))| \\geq t \\Big) &amp;\\leq P(\\frac{1}{n} \\sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) + P(\\frac{1}{n} \\sum_{i=1}^n (E(Z_i) - Z_i) \\geq t) \\\\ &amp;\\leq 2~exp(\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2}) \\end{aligned} \\] Suppose all \\(Z_i\\)s come from the same distribution, say Uniform or Gaussian. According to Theorem 3.6, the right hand side (\\(exp(\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2})\\)) would be the same in both cases, which means that Hoeffding’s does not use variance information. Theorem 3.7 (Bernstein's Inequality) Let \\(Z_1, \\cdots, Z_n\\) be independent random variables with \\(E(Z_i) = 0\\) and such that \\(-M \\leq Z_i \\leq M\\), and let \\(\\sigma_i^2 = Var(Z_i)\\), \\(\\forall i = 1,\\cdots,n\\). Then \\(\\forall t&gt;0\\), we have: \\[ P(\\frac{1}{n} \\sum_{i=1}^n Z_i \\geq t) \\leq exp(\\frac{-\\frac{1}{2} t^2n^2}{\\frac{1}{3} Mnt + \\sum_{i=1}^n \\sigma_i^2}) \\] and \\[ P\\Big(| \\frac{1}{n} \\sum_{i=1}^n Z_i | \\geq t\\Big) \\leq 2~exp(\\frac{-\\frac{1}{2} t^2n^2}{\\frac{1}{3} Mnt + \\sum_{i=1}^n \\sigma_i^2}) \\] Remark. Suppose \\(\\forall i\\), \\(\\sigma_i^2 &lt; \\sigma^2\\), then we have \\[ P(\\frac{1}{n} \\sum_{i=1}^n Z_i \\geq t) \\leq exp(\\frac{-\\frac{1}{2} t^2n}{\\frac{1}{3} Mt + \\sigma^2}) \\] Hoeffding’s v.s. Bernstein’s: Let \\(a_i = -M\\), \\(b_i = M\\), \\(\\sigma^2 = Var(Z_i)\\). Then for Hoeffding’s, we have: \\[ \\begin{aligned} P(\\frac{1}{n} \\sum_{i=1}^n Z_i \\geq t) &amp;\\leq exp(\\frac{-2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2})\\\\ &amp;= exp(\\frac{-2t^2n^2}{4n M^2})\\\\ &amp;= exp(\\frac{-t^2n}{2M^2}) \\end{aligned} \\] For Bernstein’s, we have: \\[ \\begin{aligned} P(\\frac{1}{n} \\sum_{i=1}^n Z_i \\geq t) &amp;\\leq exp(\\frac{-\\frac{1}{2} t^2n^2}{\\frac{1}{3} Mnt + \\sum_{i=1}^n \\sigma^2})\\\\ &amp;= exp(\\frac{-\\frac{1}{2} t^2n}{\\frac{1}{3} Mt + \\sigma^2}) \\end{aligned} \\] As \\[ \\frac{-\\frac{1}{2} t^2n}{\\frac{1}{3} Mt + \\sigma^2} \\leq \\frac{-\\frac{1}{2}t^2n}{M^2} \\Leftrightarrow M^2 \\geq \\sigma^2 + \\frac{1}{3}Mt \\] which means that \\(M^2 \\geq \\sigma^2 + \\frac{1}{3} Mt, ~\\forall t \\in [0,M]\\), or \\(\\frac{2}{3}M^2 \\geq \\sigma^2\\). As a result, when \\(\\sigma^2\\) is considerably smaller than \\(M^2\\), we should use Bernstein’s Inequality rather than Hoeffding’s Inequality. In general, both inequalities have the form of \\[ P(| W_n - E(W_n) | \\geq t) \\leq 2 ~exp(\\cdots) \\] i.e. what is the likelihood that some variable is away from its mean more than distance \\(t\\). Summary: all of these are theorems about sums of independent random variable. 3.4.3 More about Con-Ineq Let \\(Z_1, \\cdots, Z_n\\) be real-valued random variables. Also that \\(U_1, \\cdots, U_n\\) be another sequence of random variables (not necessarily real-valued). We say \\(\\{Z_i\\}_{i=1,\\cdots,n}\\) is a martingale difference sequence w.r.t. \\(\\{U_i\\}_{i=1,\\cdots,n}\\) if \\(Z_k = h_k(U_1, \\cdots, U_k)\\), \\(\\forall k=1,\\cdots,n\\) \\(E(Z_{k+1} | U_1, \\cdots, U_k) = 0\\), \\(\\forall k = 1,\\cdots, n-1\\). Remark. Suppose that \\(Z_1, \\cdots, Z_n\\) are independent random variables with mean 0 and \\(U_i = Z_i\\), \\(\\forall i=1,\\cdots,n\\). Then \\(\\{Z_i\\}_{i=1,\\cdots,n}\\) is a MDS w.r.t. (relative to) \\(\\{U_i\\}_{i=1,\\cdots,n}\\) Theorem 3.8 (Hoeffding's Inequality for MDS) Let \\(\\{Z_i\\}_{i=1,\\cdots,n}\\) be a MDS relative to \\(\\{X_i\\}_{i=1,\\cdots,n}\\) (which generalizes “Let \\(\\{Z_i\\}_{i=1,\\cdots,n}\\) be independent r.v.s with mean 0”). Assume that \\(\\forall i=1,\\cdots, n\\), we have: \\[ V_i \\leq Z_i \\leq V_i + C_i, \\] where \\(C_i &gt;0\\) is a fixed number, and \\(V_i\\) can be a r.v. and \\(V_i = \\psi_i(X_1, \\cdots, X_{i-1})\\), \\(\\forall i\\) (which generalizes the assumption \\(a_i \\leq Z_i \\leq b_i\\)). Then, \\(\\forall t&gt;0\\), we have: \\[ P(\\frac{1}{n}\\sum_{i=1}^n Z_i \\geq t) \\leq exp(\\frac{-2t^2n^2}{\\sum_{i=1}^n C_i^2}) \\] Theorem 3.9 (McDiarmid's Inequality) Let \\(X_1, \\cdots, X_n\\) be independent r.v.s in \\(R^d\\). Suppose we have a function \\(g: (R^d)^n \\to R\\), i.e. \\(g(x_1, \\cdots, x_n) \\in R, ~x_1, \\cdots, x_n \\in R^d\\), satisfying: \\(\\forall i=1,\\cdots, n\\), \\[ \\underset{x_1, \\cdots, x_n, x_i&#39;}{\\operatorname{sup}} \\Big| g(x_1, \\cdots, x_n) - g(x_1, \\cdots, x_{i-1}, x_i&#39;, x_{i+1}, \\cdots, x_n) \\Big| \\leq C_i. \\] Then we have: \\[ P(W_n - E(W_n) \\geq t) \\leq exp(\\frac{-2t^2}{\\sum_{i=1}^n C_i^2}), \\] where \\(W_n := g(X_1, \\cdots, X_n)\\). Proof. Idea: To show that \\(W_n - E(W_n) = \\sum_{i=1}^n Z_i\\) for a suitable MDS satisfying the assumptions in Theorem 3.8. Let \\(Z_k:= E(W_n | X_1, \\cdots, X_k) - E(W_n | X_1, \\cdots, X_{k-1})\\), \\(k \\geq 2\\); \\(Z_1 := E(W_n | X_1) - E(W_n)\\). Then we have: we can write \\(W_n - E(W_n) = \\sum_{i=1}^n Z_i\\): \\[ \\begin{aligned} \\sum_{i=1}^n Z_i &amp;= E(W_n | X_1, \\cdots, X_n) - E(W_n)\\\\ &amp;= E(g(X_1, \\cdots, X_n) | X_1, \\cdots, X_n) - E(W_n)\\\\ &amp;= g(X_1, \\cdots, X_n)- E(W_n)\\\\ &amp;= W_n- E(W_n) \\end{aligned} \\] \\(\\{Z_i\\}\\) is a MDS relative to \\(\\{X_i\\}\\): \\[ \\begin{aligned} Z_k &amp;= E(W_n | X_1, \\cdots, X_k) - E(W_n | X_1, \\cdots, X_{k-1})\\\\ &amp;= \\tilde h_k(X_1, \\cdots, X_k) - \\tilde h_{k-1}(X_1, \\cdots, X_{k-1})\\\\ &amp;= h_k(X_1, \\cdots, X_k) \\end{aligned} \\] and \\[ \\begin{aligned} E(Z_{k+1} | X_1, \\cdots, X_k) &amp;= E\\Big(E(W_n |X_1, \\cdots, X_{k+1}) - E(W_n | X_1, \\cdots, X_k) \\Big| X_1, \\cdots, X_k \\Big)\\\\ &amp;= E\\Big( E(W_n |X_1, \\cdots, X_{k+1}) \\Big | X_1, \\cdots, X_k\\Big) - E\\Big( E(W_n |X_1, \\cdots, X_k) \\Big | X_1, \\cdots, X_k\\Big)\\\\ &amp;\\underset{\\text{Tower Property}}{=} E(W_n |X_1, \\cdots, X_k) - E(W_n |X_1, \\cdots, X_k)\\\\ &amp;= 0 \\end{aligned} \\] we can bound \\(V_k \\leq Z_k \\leq V_k + C_k\\) for some \\(V_k, C_k\\): (Show that in HW2) 3.4.4 Strong Consistency Statements that may encounter (\\(P(|W_n - E(W_n)| \\geq t) \\leq 2 exp(-cnt^2)\\)): With probability greater than \\(1-\\delta\\), we have: \\(|W_n - E(W_n)| \\leq f(\\delta, n)\\) (fixing threshold) With probability greater than \\(1-\\delta\\), we have: \\(|W_n - E(W_n)| \\leq \\sqrt{\\frac{1}{cn} log(\\frac{2}{\\delta})}\\) (let \\(\\delta = 2 exp(-cnt^2)\\), i.e. fixing probability) Theorem 3.10 (Borel-Cantelli Theorem) Let \\(E_1, E_2, \\cdots\\) be events in a probability space \\((\\Omega, \\mathcal F, P)\\), \\(\\sum_{i=1}^\\infty P(E_i) \\leq \\infty\\) (the events \\(E_i\\) become less and less likely), then for \\(E=\\{\\omega \\in \\Omega: ~\\exists N(\\omega), s.t. ~ \\forall n \\geq N(\\omega), \\omega \\in E_n^c\\}\\), we have \\(P(E) = 1\\). Example 3.9 \\(Z_1, Z_2, \\cdots\\) independent and \\(Z_i \\sim Bernoulli(p_i)\\), \\(\\sum_{i=1}^\\infty p_i &lt; \\infty\\). Notice \\(p_i = P(Z_i=1) \\to 0\\) as \\(i \\to \\infty\\). Example 3.10 \\(Z_1, Z_2, \\cdots\\) are i.i.d. r.v.s with mean \\(m\\) and \\(|Z_i| \\leq M\\), \\(\\forall i=1,2,\\cdots\\) By Hoeffding’s Inequality, \\(\\forall t&gt;0\\), we have: \\[ P(|\\frac{1}{n}\\sum_{i=1}^n Z_i - m| \\geq t) \\leq 2 ~exp(\\frac{-nt^2}{2M^2}) \\] The idea is to define a sequence \\(\\{t_n\\}_{n\\in N}\\) such that \\(t_n \\to 0\\) and \\(\\sum_{n=1}^\\infty 2 ~exp(\\frac{-nt_n^2}{2M^2}) &lt; \\infty\\). Then let \\(E_n := \\{|\\frac{1}{n}\\sum_{i=1}^n Z_i - m| \\geq t_n \\}\\). As a result, \\(\\sum_{n=1}^\\infty P(E_n) \\leq 2~exp(\\frac{-nt^2}{2M^2}) &lt; \\infty\\). By Borel-Cantelli: \\(P(E) = 1\\), where \\(E = \\{\\omega \\in \\Omega: \\forall n \\text{ large enough, } \\omega \\in E_n^c \\}\\). Therefore, what we are saying is that for almost every \\(\\omega \\in \\Omega\\), \\(\\forall\\) large enough \\(n\\), we have \\(|\\frac{1}{n}\\sum_{i=1}^n Z_i - m| \\leq t_n\\). As \\(t_n \\to 0\\), we then have for almost every \\(\\omega \\in \\Omega\\), \\(\\underset{n \\to \\infty}{\\operatorname{lim}} |\\frac{1}{n}\\sum_{i=1}^n Z_i - m| = 0\\). For example, \\(t_n := \\sqrt{4M^2 \\frac{log(n)}{n}}\\) satisfies the above assumptions. Theorem 3.11 Let \\(\\hat{\\mathfrak f}_n\\) be a histogram classifier built from data \\((x_1, y_1), \\cdots, (x_n, y_n)\\). Let \\(h_n\\) be the length-scale associated to \\(\\hat{\\mathfrak f}_n\\). If \\(\\begin{cases} h_n \\to 0 ~as~ n\\to \\infty\\\\ nh_n^d \\to \\infty ~as~ n \\to \\infty\\end{cases}\\), then \\(\\{\\hat{\\mathfrak f}_n\\}\\) is universally strong consistent. Proof. Recall histogram classifiers 3.5 \\(\\forall z\\in R^d\\), denote \\(A_n(z)\\) by the cell in the grid that contains \\(z\\). Then, \\(\\omega_{in}(X) = \\frac{1_{A_n(X)}(x_i)}{N_n(X)}\\), where \\(1_{A_n(X)}(x_i) := \\begin{cases}1&amp;x_i \\in A_n(X)\\\\0&amp;o.w. \\end{cases}\\), \\(N_n(X):= \\#\\{j ~s.t. 1_{A_n(X)}(x_j)=1 \\}\\). As \\(\\hat \\eta_n(X) = \\sum_{i=1}^n Y_i \\omega_{in}(X) = \\sum_{i=1}^n Y_i \\frac{1_{A_n(X)}(x_i)}{N_n(X)}\\), we have \\[ \\hat \\eta_n(X) \\geq \\frac{1}{2} \\Leftrightarrow \\sum_{i=1}^n Y_i1_{A_n(X)}(x_i) \\geq \\sum_{i=1}^n (1-Y_i)1_{A_n(X)}(x_i) \\] Let \\(\\rho_X(A_n(x)):= P_{(X,Y) \\sim \\rho} (X\\in A_n(x))\\), \\(\\hat \\alpha_n^0(x):= \\frac{\\sum_{i=1}^n (1-Y_i) 1_{A_n(X)}(x_i)}{n\\rho_X(A_n(x))}\\), \\(\\hat \\alpha_n^1(x):=\\frac{\\sum_{i=1}^n Y_i 1_{A_n(X)}(x_i)}{n\\rho_X(A_n(x))}\\), then \\[ \\hat \\eta_n(x) \\geq \\frac{1}{2} \\Leftrightarrow \\hat \\alpha_n^0(x) \\geq \\hat \\alpha_n^1(x) \\] It can be shown that \\(R(\\hat{\\mathfrak f}_n) - R^* \\leq E_{(X,Y)\\sim \\rho} \\Big[|1-\\eta(X) - \\hat \\alpha_n^0(X)|\\Big] + E_{(X,Y)\\sim \\rho} \\Big[|\\eta(X) - \\hat \\alpha_n^1(X)|\\Big]\\) (use \\(\\hat \\alpha_n^0(X)\\) to estimate \\(1-\\eta(X)\\) and use \\(\\hat \\alpha_n^1(X)\\) to estimate \\(\\eta(X)\\)). Let \\[ \\begin{aligned} g_n((x_1, y_1), \\cdots, (x_n, y_n)) &amp;= E_{(X,Y)\\sim \\rho} \\Big[|\\eta(X) - \\hat \\alpha_n^1(X)|\\Big]\\\\ &amp;= g_n((x_1, y_1), \\cdots, (x_n, y_n)) - E_{(x_1, y_1), \\cdots, (x_n, y_n)[g_n((x_1, y_1), \\cdots, (x_n, y_n))]} \\text{ (variance)}\\\\ &amp;+E_{(x_1, y_1), \\cdots, (x_n, y_n)[g_n((x_1, y_1), \\cdots, (x_n, y_n))]} \\text{ (bias)} \\end{aligned} \\] then for the bias term, it goes to 0 as \\(n \\to \\infty\\) following a slight generalization of Stone’s Theorem since here weights don’t exactly add to 1. For the variance term, we have: \\[ \\begin{aligned} g_n((x_1, y_1), \\cdots, (x_i,y_i),\\cdots, (x_n, y_n)) &amp;= E_{(X,Y) \\sim \\rho}\\Big[ |\\eta(X) - \\frac{\\sum_{k=1}^n y_k 1_{A_n(X)}(x_k)}{n\\rho_X(A_n(x))}| \\Big]\\\\ &amp;= E_{(X,Y) \\sim \\rho}\\Big[ |\\eta(X) - \\frac{\\sum_{k\\neq i} y_k 1_{A_n(X)}(x_k)}{n\\rho_X(A_n(x))} + \\frac{y_i 1_{A_n(X)}(x_i)}{n\\rho_X(A_n(x))}| \\Big] \\end{aligned} \\] and \\[ \\begin{aligned} g_n((x_1, y_1), \\cdots, (x_i&#39;,y_i&#39;),\\cdots, (x_n, y_n)) &amp;= E_{(X,Y) \\sim \\rho}\\Big[ |\\eta(X) - \\frac{\\sum_{k\\neq i} y_k 1_{A_n(X)}(x_k)}{n\\rho_X(A_n(x))} + \\frac{y_i&#39; 1_{A_n(X)}(x_i&#39;)}{n\\rho_X(A_n(x))}| \\Big] \\end{aligned} \\] As a reuslt, \\[ \\begin{aligned} | g_n((x_1, y_1), \\cdots, (x_i,y_i),\\cdots, (x_n, y_n)) -\\\\ g_n((x_1, y_1), \\cdots, (x_i&#39;,y_i&#39;),\\cdots, (x_n, y_n))| &amp;\\leq E_{(X,Y)\\sim \\rho}[\\frac{y_i 1_{A_n(X)}(x_i)}{n\\rho_X(A_n(x))} + \\frac{y_i&#39; 1_{A_n(X)}(x_i&#39;)}{n\\rho_X(A_n(x))}]\\\\ &amp;\\underset{x_i \\in A_n(X) \\Leftrightarrow X \\in A_n(x_i)}{\\leq} E[\\frac{1_{A_n(x_i)}(X)}{n\\rho_X(A_n(x_i))}] + E[\\frac{1_{A_n(x_i&#39;)}(X)}{n\\rho_X(A_n(x_i&#39;))}]\\\\ &amp;= \\frac{1}{n \\rho_X(A_n(x_i))}P(X\\in A_n(x_i)) + \\frac{1}{n \\rho_X(A_n(x_i&#39;))}P(X\\in A_n(x_i&#39;))\\\\ &amp;= \\frac{2}{n} \\end{aligned} \\] Let \\(C_i = \\frac{2}{n}\\), by McDiarmid’s Inequality, we have: \\[ \\begin{aligned} P(|g_n - E[g_n]| \\geq t) &amp;\\leq 2~exp(\\frac{-2t^2}{n\\sum C_i^2})\\\\ &amp;= 2~exp(-\\frac{1}{2}nt^2) \\end{aligned} \\] Let \\(t_n:= \\sqrt{c \\frac{log(n)}{n}}\\), where we can choose \\(C\\) large enough so that by Borel-Cantelli Theorem, we have: with probability 1, \\(\\forall\\) large enough \\(n\\), we have \\(|g_n - E[g_n]| \\leq t_n\\). As a result, we have \\(|g_n - E[g_n]| \\to 0\\) as \\(n \\to \\infty\\) a.s. "],["empirical-risk-minimization.html", "Chapter 4 Empirical Risk Minimization 4.1 Questions 4.2 Concentration Bounds for \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |\\)", " Chapter 4 Empirical Risk Minimization Definition 4.1 (Empirical Risk) Given \\((x_1, y_1), \\cdots, (x_n, y_n)\\), \\(\\mathfrak f: \\mathcal X \\to \\{0,1\\}\\), define the empirical risk: \\(R_n(\\mathcal f) = \\frac{1}{n}\\sum_{i=1}^n 1_{\\mathfrak f(x_i) \\neq y_i}\\). Goal: To understand how does \\(R(\\mathfrak f_n^*)\\) behave as a function of \\(n\\) and the training data, where \\(\\mathfrak f_n^* \\in \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{argmin}} R_n(\\mathfrak f)\\). 4.1 Questions Suppose for a moment that we have proved that with probability greater than \\(1-\\delta\\): \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\leq \\epsilon(n, \\mathcal F, \\delta)\\), we have: \\(\\mathfrak f_n^* \\in \\mathcal F\\) \\(\\mathfrak f^* \\in \\mathcal F\\) \\(R_n(\\mathfrak f_n^*) \\leq R_n(\\mathfrak f^*)\\) \\(R(\\mathfrak f^*) \\leq R(\\mathfrak f_n^*)\\) Comparison between empirical risk and true risk: With probability greater than \\(1-\\delta\\), \\(R(\\mathfrak f_n^*) \\leq R_n(\\mathfrak f_n^*) + \\epsilon(n, \\mathcal F, \\delta)\\). \\[ \\begin{aligned} | R(\\mathfrak f_n^*) - R_n(\\mathfrak f_n^*) | &amp;\\leq \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |\\\\ &amp;\\leq \\epsilon(n, \\mathcal F, \\delta) \\end{aligned} \\] Let \\(\\mathfrak f^* \\in \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{argmin}} R(\\mathfrak f)\\), with probability greater than \\(1-\\delta\\): \\(R(\\mathfrak f_n^*) \\leq R(\\mathfrak f^*) + \\epsilon(n, \\mathcal F, \\delta)\\). \\[ \\begin{aligned} 0 \\leq R(\\mathfrak f_n^*) - R(\\mathfrak f^*) &amp;= R(\\mathfrak f_n^*) - R_n(\\mathfrak f_n^*) \\\\ &amp;\\quad + R_n(\\mathfrak f_n^*) - R_n(\\mathfrak f^*) \\\\ &amp;\\quad+ R_n(\\mathfrak f^*) - R(\\mathfrak f^*) \\quad(\\leq 0)\\\\ &amp;\\leq R(\\mathfrak f_n^*) - R_n(\\mathfrak f_n^*)\\\\ &amp;\\quad+ R(\\mathfrak f_n^*) - R_n(\\mathfrak f_n^*)\\\\ &amp;\\leq 2| R(\\mathfrak f_n^*) - R_n(\\mathfrak f_n^*) |\\\\ &amp;\\leq 2 \\epsilon(n, \\mathcal F, \\delta) \\end{aligned} \\] Comparison between true risk of \\(\\mathfrak f_n^*\\) and Bayes risk: With probability greater than \\(1-\\delta\\): \\(R(\\mathfrak f_n^*) \\leq R(\\mathfrak f_B) + \\epsilon(n, \\mathcal F, \\delta)\\). \\[ \\begin{aligned} R(\\mathfrak f_n^*) &amp;\\leq R(\\mathfrak f^*) + \\epsilon(n, \\mathcal F, \\delta)\\\\ &amp;= R(\\mathfrak f_B) \\\\ &amp;\\quad+ R(\\mathfrak f^*) - R(\\mathfrak f_B) \\quad (\\leq 0)\\\\ &amp;\\quad+ \\epsilon(n, \\mathcal F, \\delta)\\\\ &amp;\\leq R(\\mathfrak f_B) + \\epsilon(n, \\mathcal F, \\delta) \\end{aligned} \\] 4.2 Concentration Bounds for \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |\\) 4.2.1 In terms of Shattering Number Theorem 4.1 (Vapnick-Chervonenkis) \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\to 0\\) in probability iff \\(| R(\\mathfrak f_n^*) - R(\\mathfrak f^*) | \\to 0\\) in probability. Suppose \\(| \\mathcal F | &lt; \\infty\\), where we can use a union bound: \\[ \\begin{aligned} P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t) &amp;= P(\\exists \\mathfrak f \\in \\mathcal F \\quad s.t. \\quad | R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t)\\\\ &amp;= P(\\bigcup_{\\mathfrak f \\in \\mathcal F} \\{| R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t\\})\\\\ &amp;\\leq \\sum_{\\mathfrak f \\in \\mathcal F} P(| R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t)\\\\ &amp;\\underset{Hoeffdings}{\\leq} | \\mathcal F| \\cdot 2\\operatorname{exp}(-\\frac{nt^2}{8}) \\end{aligned} \\] To extend to a setting where \\(| \\mathcal F | = \\infty\\), we have: Definition 4.2 (Shattering Number) \\(S(n, \\mathcal F) = \\underset{x_1, \\cdots, x_n}{\\operatorname{sup}} | \\mathcal F_{x_1, \\cdots, x_n}|\\) Definition 4.3 (VC Dimension) \\(\\forall n &lt; VC(\\mathcal F)\\), \\(S(n, \\mathcal F) = 2^n\\), and \\(S(VC(\\mathcal F), \\mathcal F) &lt; 2^{VC(\\mathcal F)}\\). Symmetrization: Theorem 4.2 Let \\(t&gt;0\\) be such that \\(t \\geq \\sqrt{\\frac{2}{n}}\\). Let \\(R_n\\) be the empirical risk associated to \\((x_1, y_1), \\cdots, (x_n, y_n) \\underset{i.i.d}{\\sim} \\rho\\). Let \\(R_n&#39;\\) be the empirical risk associated to \\((x_1&#39;, y_1&#39;), \\cdots, (x_n&#39;, y_n&#39;) \\underset{i.i.d}{\\sim} \\rho\\) independent from \\((x_1, y_1), \\cdots, (x_n, y_n)\\): \\[ \\begin{aligned} &amp;R_n(\\mathfrak f) = \\frac{1}{n} \\sum_{i=1}^n 1_{\\mathfrak f(x_i) \\neq y_i}\\\\ &amp;R_n&#39;(\\mathfrak f) = \\frac{1}{n} \\sum_{i=1}^n 1_{\\mathfrak f(x&#39;_i) \\neq y&#39;_i}\\\\ \\end{aligned} \\] Then we have: \\[ \\begin{aligned} P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t) \\leq 2P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R&#39;_n(\\mathfrak f) | \\geq \\frac{t}{2}) \\end{aligned} \\] Proof. Suppose \\(\\tilde{\\mathfrak f}_n \\in \\mathcal F\\) is a maximizer for \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |\\). Claim: \\[ \\begin{aligned} 1_{| R(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; t} \\cdot 1_{| R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) | &lt; \\frac{t}{2}} \\leq 1_{| R&#39;_n(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; \\frac{t}{2}} \\end{aligned} \\] Case 1: LHS = 0, then there is nothing to prove. Case 2: LHS = 1, then we have \\(| R(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; t\\) and \\(| R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) | &lt; \\frac{t}{2}\\). Thus, \\[ \\begin{aligned} | R&#39;_n(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &amp;\\geq | R(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | - | R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) |\\\\ &amp;&gt; t - \\frac{t}{2} = \\frac{t}{2} \\end{aligned} \\] which means RHS = 1. Then, for \\(R&#39;_n(\\tilde{\\mathfrak f}_n) = \\frac{1}{n} \\sum_{i=1}^n 1_{\\tilde{\\mathfrak f}_n(x&#39;_i) \\neq y_i}\\), we have \\(E[R&#39;_n(\\tilde{\\mathfrak f}_n) | (x_1, y_1), \\cdots, (x_n, y_n)] = R(\\tilde{\\mathfrak f}_n)\\). Thus, \\[ \\begin{aligned} Var(R&#39;_n(\\tilde{\\mathfrak f}_n) - R(\\tilde{\\mathfrak f}_n) | (x_1, y_1), \\cdots, (x_n, y_n)) &amp;= \\frac{1}{n^2} \\sum_{i=1}^n Var(\\frac{1}{n} \\sum_{i=1}^n 1_{\\tilde{\\mathfrak f}_n(x&#39;_i) \\neq y_i} | (x_1, y_1), \\cdots, (x_n, y_n))\\\\ &amp;\\leq \\frac{1}{n^2} \\cdot n \\cdot \\frac{1}{4}\\\\ &amp;= \\frac{1}{4n} \\end{aligned} \\] Then, \\[ \\begin{aligned} 1_{| R(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; t} \\cdot E[1_{| R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) | &lt; \\frac{t}{2}} | (x_1, y_1), \\cdots, (x_n, y_n)] \\leq E[1_{| R&#39;_n(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; \\frac{t}{2}} | (x_1, y_1), \\cdots, (x_n, y_n)] \\end{aligned} \\] As \\[ \\begin{aligned} E[1_{| R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) | &lt; \\frac{t}{2}} | (x_1, y_1), \\cdots, (x_n, y_n)] &amp;= 1 - E[1_{| R(\\tilde{\\mathfrak f}_n) - R&#39;_n(\\tilde{\\mathfrak f}_n) | \\geq \\frac{t}{2}} | (x_1, y_1), \\cdots, (x_n, y_n)]\\\\ &amp;\\geq 1 - (\\frac{2}{t})^2 Var(R&#39;_n(\\tilde{\\mathfrak f}_n) - R(\\tilde{\\mathfrak f}_n) | (x_1, y_1), \\cdots, (x_n, y_n))\\\\ &amp;\\geq 1 - \\frac{1}{t^2n}\\\\ &amp;\\underset{t \\geq \\sqrt{\\frac{2}{n}}}{\\geq} \\frac{1}{2} \\end{aligned} \\] we have: \\[ \\begin{aligned} 1_{| R(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; t} \\leq 2 E[1_{| R&#39;_n(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | &gt; \\frac{t}{2}} | (x_1, y_1), \\cdots, (x_n, y_n)] \\end{aligned} \\] Take expectation of both sides, we then have: \\[ \\begin{aligned} P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t) &amp;\\leq 2P(| R&#39;_n(\\tilde{\\mathfrak f}_n) - R_n(\\tilde{\\mathfrak f}_n) | \\geq \\frac{t}{2})\\\\ &amp;\\leq 2 P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R&#39;_n(\\mathfrak f) - R_n(\\mathfrak f) | \\geq \\frac{t}{2}) \\end{aligned} \\] How do we bound \\(P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R&#39;_n(\\mathfrak f) - R_n(\\mathfrak f) | \\geq \\frac{t}{2})\\)? At most the number of different values that \\(R&#39;_n(\\mathfrak f) - R_n(\\mathfrak f)\\) can take when we change \\(\\mathfrak f \\in \\mathcal F\\) is the number of assignment that the family \\(\\mathcal F\\) induces on \\((x_1, \\cdots, x_n, x&#39;_1, \\cdots, x&#39;_n)\\). Therefore, \\[ \\begin{aligned} P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R&#39;_n(\\mathfrak f) - R_n(\\mathfrak f) | \\geq \\frac{t}{2}) &amp;= P(\\underset{\\mathfrak f \\in \\mathcal F_{x_1, \\cdots, x_n, x&#39;_1, \\cdots, x&#39;_n}}{\\operatorname{max}} | R&#39;_n(\\mathfrak f) - R_n(\\mathfrak f) | \\geq \\frac{t}{2})\\\\ &amp;\\underset{S(2n, \\mathcal F) = \\operatorname{sup} |\\mathcal F_{x_1, \\cdots, x_n, x&#39;_1, \\cdots, x&#39;_n}|}{\\leq} S(2n, \\mathcal F) \\cdot 2\\operatorname{exp}(-\\frac{nt^2}{8}) \\end{aligned} \\] Thus, combining the Symmetrization argument and the above we have: Theorem 4.3 (Vapnick-Chervonenkisi Therorem) Let \\(t&gt;0\\) be such that \\(t \\geq \\sqrt{\\frac{2}{n}}\\). Then, \\[ \\begin{aligned} P(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | \\geq t) \\leq 4 S(2n, \\mathcal F) \\cdot 2\\operatorname{exp}(-\\frac{nt^2}{8}) \\end{aligned} \\] Let \\(\\delta = 4 S(2n, \\mathcal F) \\cdot 2\\operatorname{exp}(-\\frac{nt^2}{8})\\), we have \\(t = \\sqrt{\\frac{8 \\operatorname{log} (\\frac{4S(2n, \\mathcal F)}{\\delta})}{n}}\\). Then, with probability of at least \\(1-\\delta\\), we have: \\[ \\begin{aligned} \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | &amp;\\leq t\\\\ &amp;= \\sqrt{\\frac{8 \\operatorname{log} (\\frac{4S(2n, \\mathcal F)}{\\delta})}{n}}\\\\ &amp;= \\sqrt{\\frac{8 \\operatorname{log} (4S(2n, \\mathcal F)) + 8\\operatorname{log (\\frac{1}{\\delta})}}{n}} \\end{aligned} \\] We call \\(0 \\leq R(f_n^* - R^*) \\to 0\\) as \\(n \\to \\infty\\) a restricted (\\(\\mathfrak f \\in \\mathcal F\\)) consistency statement. 4.2.2 In terms of VC Dimension Theorem 4.4 (Vapnick,Chervonenkisi,Sauer,Shelah) If \\(VC(\\mathcal F) &lt; \\infty\\), then: \\[ S(n,\\mathcal F) \\leq (\\frac{e n}{VC(\\mathcal F)})^{VC(\\mathcal F)} \\] The RHS is a polynomial in n. Corollary 4.1 With probability of at least \\(1-\\delta\\), we have: \\[ \\begin{aligned} \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | &amp;\\leq \\sqrt{\\frac{8 \\operatorname{log} (\\frac{4S(2n, \\mathcal F)}{\\delta})}{n}}\\\\ &amp;\\leq \\sqrt{\\frac{8 \\operatorname{log} (4(\\frac{e n}{VC(\\mathcal F)})^{VC(\\mathcal F)}) + 8\\operatorname{log (\\frac{1}{\\delta})}}{n}}\\\\ &amp;\\approx \\sqrt{\\frac{c \\cdot VC(\\mathcal F) \\operatorname{log} (n) + c\\operatorname{log (\\frac{1}{\\delta})}}{n}} \\end{aligned} \\] If \\(VC(\\mathcal F) = \\infty\\), \\(\\sqrt{\\frac{c \\operatorname{log} (2^n) + c\\operatorname{log (\\frac{1}{\\delta})}}{n}} = \\sqrt{c \\operatorname{log} (2) + \\frac{c\\operatorname{log} (\\frac{1}{\\delta})}{n}}\\), which doesn’t go to 0 as \\(n \\to \\infty\\). Thus, keep VC dimension finite is critical for restricted universal strong consistency of ERM. 4.2.3 In terms of Rademacher Complexity A new convention: let us assume our classifier: \\(\\mathfrak f: \\mathcal X \\to \\{-1,1\\}\\), \\(y_i \\in \\{-1,1\\}\\). Definition 4.4 Given a family of classifier \\(\\mathcal F\\) (\\(\\mathfrak f: \\mathcal X \\to \\{-1,1\\}\\)), we define \\(Rad_n(\\mathcal F) = E[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} R_n^{\\sigma}(\\mathfrak f)]\\) as Rademacher average. \\(\\tilde{Rad}_n(\\mathcal F) = E_{\\sigma}[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} R_n^{\\sigma}(\\mathfrak f)]\\) as conditional Rademacher average Here, \\(R_n^{\\sigma}(\\mathfrak f) = \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\mathfrak f(x_i)\\), where \\(x_1, \\cdots, x_n\\) are \\(i.i.d\\) samples and \\(\\sigma_1, \\cdots, \\sigma_n\\) are \\(i.i.d\\) Rademacher variables (\\(\\sigma_i \\in \\{-1,1\\}\\) and \\(P(\\sigma_i=-1)=P(\\sigma_i=1)=\\frac{1}{2}\\)) that are independent from the data. \\(E\\): expectation over \\(x_1, \\cdots, x_n\\) and over \\(\\sigma_1, \\cdots, \\sigma_n\\). \\(E_\\sigma\\): expectation over \\(\\sigma_1, \\cdots, \\sigma_n\\). In particular, \\(\\tilde{Rad}_n(\\mathcal F)\\) is a random variable that depends on \\(x_1, \\cdots, x_n\\). Theorem 4.5 For all \\(\\delta \\in (0,1)\\), with probability at least \\(1-\\delta\\), we have: \\[ \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} |R(\\mathfrak f) - R_n(\\mathfrak f)| \\leq 2Rad_n(\\mathcal F) + \\sqrt{\\frac{c \\operatorname{log}(\\frac{1}{\\delta})}{n}} \\] Corollary 4.2 If \\(Rad_n(\\mathcal F) \\to 0\\) as \\(n \\to \\infty\\), then ERM with \\(\\mathcal F\\) is (restricted) universally strongly consistent. Unfortunately, \\(Rad_n(\\mathcal F)\\) is still distribution dependent: \\(x_1, \\cdots, x_n \\sim \\rho_X\\), \\(\\sigma_1, \\cdots, \\sigma_n \\sim Rademacher\\). Theorem 4.6 For all \\(\\delta \\in (0,1)\\), with probability at least \\(1-\\delta\\), we have: \\[ \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} |R(\\mathfrak f) - R_n(\\mathfrak f)| \\leq 2 \\tilde{Rad}_n(\\mathcal F) + \\sqrt{\\frac{c \\operatorname{log}(\\frac{1}{\\delta})}{n}} \\] Proof. (Sketch of Proofs): For Theorem 4.5: We are interested in bounding \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |\\). Given \\((x_1,y_1) \\cdots, (x_n,y_n)\\), \\(g((x_1,y_1) \\cdots, (x_n,y_n)):= \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | \\frac{1}{n} \\sum_{i=1}^n 1_{\\mathfrak f(x_i) \\neq y_i} - R(\\mathfrak f) |\\). We can check that the function \\(g\\) satisfies the condition for using McDiarmid’s Inequality: \\[ | g((x_1,y_1) \\cdots,(x_i,y_i),\\cdots, (x_n,y_n)) - g((x_1,y_1) \\cdots,(x&#39;_i,y&#39;_i),\\cdots, (x_n,y_n)) | \\leq \\frac{2}{n} \\] Because of this, we can conclude that \\(\\Big | \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) | - E[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |] \\Big | \\to 0\\) as \\(n \\to \\infty\\) a.s. To conclude, we would need to show that \\(E[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | R_n(\\mathfrak f) - R(\\mathfrak f) |] \\leq 2 Rad_n(\\mathcal F)\\). \\[ R_n(\\mathfrak f) = \\frac{1}{n} \\sum_{i=1}^n 1_{\\mathfrak f(x_i) \\neq y_i} = \\frac{1}{2n} \\sum_{i=1}^n (1 - \\mathfrak f(x_i)y_i) \\] \\[ R(\\mathfrak f) = \\frac{1}{2} E[1-\\mathfrak f(X)Y] \\] \\[ |R_n(\\mathfrak f) - R(\\mathfrak f)| = \\frac{1}{2} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\mathfrak f(x_i)y_i - E[\\mathfrak f(X)Y]\\Big| \\] For simplicity, assume \\(y_i = 1, i=1,\\cdots,n\\) and \\(Y=1\\), then we need to show that \\[ E\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\mathfrak f(x_i) - E[\\mathfrak f(X)]\\Big|\\Big] \\leq 2 Rad_n(\\mathcal F) \\] \\[ \\begin{aligned} E_X\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\mathfrak f(x_i) - E_{X&#39;}[\\mathfrak f(x_i&#39;)]\\Big|\\Big] &amp;= E_X\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} E_{X&#39;}\\Big|\\frac{1}{n} \\sum_{i=1}^n \\Big(\\mathfrak f(x_i) - \\mathfrak f(x_i&#39;)\\Big)\\Big|\\Big]\\\\ &amp;\\leq E_X E_{X&#39;}\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\Big(\\mathfrak f(x_i) - \\mathfrak f(x_i&#39;)\\Big)\\Big|\\Big]\\\\ &amp;= E_{X,X&#39;}\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\Big(\\mathfrak f(x_i) - \\mathfrak f(x_i&#39;)\\Big)\\Big|\\Big]\\\\ &amp;\\underset{\\text{show below}}{=} E_{X,X&#39;,\\sigma}\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\Big(\\mathfrak f(x_i) - \\mathfrak f(x_i&#39;)\\Big)\\Big|\\Big]\\\\ &amp;\\leq 2E_{X,\\sigma}\\Big[\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} \\Big|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\mathfrak f(x_i)\\Big|\\Big]\\\\ &amp;= 2 Rad_n(\\mathcal F) \\end{aligned} \\] For the equality mentioned above, whichever \\(\\sigma_i\\) is (\\(-1,1\\)), we have \\(\\sigma_i \\Big(\\mathfrak f(x_i) - \\mathfrak f(x_i&#39;)\\Big)\\) always have the same distribution. For Theorem 4.6: With high probability we have \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} |R(\\mathfrak f) - R_n(\\mathfrak f)| \\leq 2Rad_n(\\mathcal F) + O(\\sqrt{\\frac{1}{n}})\\). However, notice that we can find concentration bounds for \\(\\tilde{Rad}_n(\\mathcal F) - Rad_n(\\mathcal F)\\), as \\(E_X[\\tilde{Rad}_n(\\mathcal F)] = Rad_n(\\mathcal F)\\). Thus, \\(\\tilde{Rad}_n(\\mathcal F) - Rad_n(\\mathcal F) = \\tilde{Rad}_n(\\mathcal F) - E[\\tilde{Rad}_n(\\mathcal F)] := h(x_1, \\cdots, x_n)\\). Then, we can use McDiarmid’s Inequality if \\(|h(x_1, \\cdots, x_i, \\cdots, x_n) - h(x_1, \\cdots, x&#39;_i, \\cdots, x_n)| \\leq \\frac{2}{n}\\). How to Compute the Conditional Rademacher Average for a Given Family? How to estimate \\(\\tilde{Rad}_n(\\mathcal F)\\)? A: Use a Mote Carlo estimate. For \\(k = 1, \\cdots, K\\), we do the following: \\(\\sigma_1^k, \\cdots, \\sigma_n^k \\sim Rademacher\\) (\\(\\sigma_i^k \\in \\{-1, 1\\}\\)). Now consider: \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | \\frac{1}{n} \\sum_{i=1}^n \\sigma_i^k \\mathfrak f(x_i) |\\), which is not so different from solving the problem: \\(\\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{max}} \\frac{1}{n} \\sum_{i=1}^n y_i \\mathfrak f(x_i)\\), but is as difficult as ERM. \\(\\tilde{Rad}_n(\\mathcal F)_k := \\underset{\\mathfrak f \\in \\mathcal F}{\\operatorname{sup}} | \\frac{1}{n} \\sum_{i=1}^n \\sigma_i^k \\mathfrak f(x_i) |\\). By doing this, we produce: \\(\\tilde{Rad}_n(\\mathcal F)_1, \\cdots, \\tilde{Rad}_n(\\mathcal F)_K\\). Therefore, by LLN, CLT, Concentration Inequalities, we can study how big \\(\\frac{1}{K} \\sum_{k=1}^K \\tilde{Rad}_n(\\mathcal F)_k - \\tilde{Rad}_n(\\mathcal F)\\) is (Concentration Ineqs as a function of \\(K\\)). "],["summary.html", "Chapter 5 Summary 5.1 Classification Problem 5.2 No Free Lunch Theorem", " Chapter 5 Summary 5.1 Classification Problem What are some statistical properties of two families of classifiers built from data? 5.1.1 Similarity Classifiers 5.1.2 Emprirical Risk Minimization 5.2 No Free Lunch Theorem “Pointwise Statement”: “Uniform Statement”: Example of Pointwise Statement vs Uniform Statement: No Free Lunch Theorem 1: Practical Corollary: No Free Lunch Theorem 2: Practical Corollary: Observation: we can not quantify the universal convergence rate. Conclusion: No magic classifier that suits every situation. "],["linear-classifiers.html", "Chapter 6 Linear Classifiers 6.1 LDA or QDA 6.2 Logistic Regression 6.3 Perceptrons and SVMs", " Chapter 6 Linear Classifiers \\(\\mathcal F = \\{\\text{all linear classifiers}\\}\\), \\(\\mathcal X = R^d\\), \\(\\mathcal Y = \\{-1,1\\}\\) or \\(\\{1,\\cdots,K\\}\\). Consider mainly on binary case. Let \\(\\beta \\in R^d\\) and \\(\\beta_0 \\in R\\), \\(\\mathcal H_{\\beta, \\beta_0} = \\{x\\in R^d: &lt;\\beta, x&gt; + \\beta_0 = 0\\}\\) (hyperplane), \\(\\mathcal H^+_{\\beta, \\beta_0} = \\{x\\in R^d: &lt;\\beta, x&gt; + \\beta_0 \\geq 0\\}\\) and \\(\\mathcal H^-_{\\beta, \\beta_0} = \\{x\\in R^d: &lt;\\beta, x&gt; + \\beta_0 &lt; 0\\}\\) (half plane). \\[ \\mathfrak f_{\\beta, \\beta_0}(x):=\\begin{cases}1 &amp; &lt;\\beta,x&gt; + \\beta_0 \\geq 0 (\\Leftrightarrow x \\in H^+_{\\beta,\\beta_0})\\\\- 1&amp; &lt;\\beta,x&gt; + \\beta_0 &lt; 0(\\Leftrightarrow x \\in H^-_{\\beta,\\beta_0})\\end{cases} \\] Question: How to find a linear classifier based on \\((x_i, y_i)\\), \\(i=1,\\cdots,n\\). A: Three different ways to tune \\(\\beta, \\beta_0\\) from data: LDA (linear discriminant analysis) or QDA (quadratic discriminant analysis), Logistic Regression, Perceptrons and SVMs. 6.1 LDA or QDA Let \\(\\mathcal Y = \\{1, \\cdots, K\\}\\), \\((X,Y) \\sim \\rho\\), where \\(P(Y=k) = \\omega_k, \\rho_{X|Y}(X | Y=k) = N(\\mu_k, \\Sigma_k)\\), we have: \\[ \\begin{aligned} \\mathfrak f_{Bayes}(x) &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} P(Y=k | X=x)\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} \\frac{\\rho_{X|Y}(x | Y=k) \\cdot \\omega_k}{\\rho_X(x)}\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} [\\rho_{X|Y}(x | Y=k) \\cdot \\omega_k]\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} [log\\Big (\\rho_{X|Y}(x | Y=k)\\Big ) + log(\\omega_k)]\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmin}} [-log\\Big (\\frac{1}{(2\\pi)^{d/2} (det(\\Sigma_k))^{1/2}} \\Big ) + \\frac{1}{2} &lt;\\Sigma_k^{-1} (x-\\mu_k), (x-\\mu_k)&gt; - log(\\omega_k)]\\\\ &amp;:= \\underset{k=1,\\cdots, K}{\\operatorname{argmin}} \\delta_k(x) \\end{aligned} \\] Observation: \\(\\delta_k(x)\\) is quadratic and convex in \\(x\\). QDA: What if we only have \\((x_i, y_i)\\), \\(i=1,\\cdots,n\\)? We use the observations to estimate \\(\\mu_k, \\omega_k, \\Sigma_k\\), \\(k=1,\\cdots, K\\). Example 6.1 \\[ \\hat \\mu_k := \\frac{\\sum_{i ~s.t. ~y_i=k} x_i}{\\#\\{x_i ~s.t. ~y_i=k\\} ~(:= N_k)} \\] \\[ (\\hat \\Sigma_k)_{lm} := (\\frac{1}{N_k} \\sum_{i ~s.t. ~y_i=k} x_{il}x_{im} - \\hat \\mu_{kl} \\hat \\mu_{km}), \\text{ where } l=1,\\cdots,d, ~m=1,\\cdots,d. \\] \\[ \\hat \\omega_k = \\frac{N_k}{n} \\] \\[ \\hat \\delta_k(x) \\text{ same as } \\delta_k \\text{ but with } \\hat{} \\text{ everywhere} \\] LDA: What if we had assumed \\(\\Sigma_1 = \\Sigma_2 = \\cdots = \\Sigma_K = \\Sigma\\)? \\[ \\begin{aligned} \\mathfrak f(x) &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmin}} [\\frac{1}{2} &lt;\\Sigma^{-1}x, x&gt; + &lt;\\Sigma^{-1}x, \\mu_k&gt; + \\frac{1}{2} &lt;\\Sigma^{-1}\\mu_k, \\mu_k&gt; - log(\\omega_k)]\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmin}} [&lt;\\Sigma^{-1}x, \\mu_k&gt; + \\frac{1}{2} &lt;\\Sigma^{-1}\\mu_k, \\mu_k&gt; - log(\\omega_k)]\\\\ &amp;:= \\underset{k=1,\\cdots, K}{\\operatorname{argmin}} l_k(x), ~(l_k(x) \\text{ is linear}) \\end{aligned} \\] We can estimate \\(\\mu_k ,\\omega_k\\) by \\(\\hat \\mu_k ,\\hat \\omega_k\\), and \\(\\Sigma\\) with the full data set. 6.2 Logistic Regression Let \\(\\mathcal Y = \\{1, \\cdots, K\\}\\), \\(\\vec \\beta_k \\in R^d\\), \\(\\beta_{0k} \\in R\\), and \\((X,Y)\\) satisfies: \\(P(Y=k | X=x) = \\frac{exp(&lt;x,\\vec \\beta_k&gt; + \\beta_{0k})}{1 + \\sum_{l=1}^{K-1} exp(&lt;x,\\vec \\beta_l&gt; + \\beta_{0l})}\\), \\(P(Y=K | X=x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} exp(&lt;x,\\vec \\beta_l&gt; + \\beta_{0l})}\\), where \\(k=1,\\cdots,K-1\\). Let \\(\\varphi_k(x) := exp(&lt;x,\\vec \\beta_k&gt; + \\beta_{0k})\\) and \\(\\varphi_K(x):=1\\), where \\(k=1,\\cdots,K-1\\). Then we have: \\[ \\begin{aligned} \\mathfrak f_{Bayes} (x) &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} P(Y=k | X=x)\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} \\varphi_k(x)\\\\ &amp;= \\underset{k=1,\\cdots, K}{\\operatorname{argmax}} log(\\varphi_k(x)) \\end{aligned} \\] What if we only have observed \\((x_i, y_i)\\), \\(i=1,\\cdots,n\\)? We use the observations to estimate the parameters. Example 6.2 (MLE) Given the data, find the best parameters (the ones maximizing the likelihood of the observations), i.e. \\[ \\{(\\vec \\beta_k^*, \\beta_{0k}^*)\\} = \\underset{\\{(\\vec \\beta_k, \\beta_{0k})\\}_{k=1,\\cdots,K-1}}{\\operatorname{max}} \\prod_{i=1}^n P(Y = y_i | X = x_i) \\] 6.3 Perceptrons and SVMs Let \\(\\mathcal Y = \\{-1, 1\\}\\), \\((x_i, y_i)_{i=1,\\cdots,n}\\), \\((\\vec \\beta, \\beta_0)\\), \\(\\mathfrak f_{\\beta, \\beta_0}(x):=\\begin{cases}1 &amp; &lt;\\beta,x&gt; + \\beta_0 \\geq 0 (\\Leftrightarrow x \\in H^+_{\\beta,\\beta_0})\\\\- 1&amp; &lt;\\beta,x&gt; + \\beta_0 &lt; 0(\\Leftrightarrow x \\in H^-_{\\beta,\\beta_0})\\end{cases}\\), \\(\\sigma(\\vec \\beta, \\beta_0) := \\sum_{i \\in \\mathcal M_{\\vec \\beta, \\beta_0}} dist(x_i, \\mathcal H_{\\vec \\beta, \\beta_0})\\), where \\(\\mathcal M_{\\vec \\beta, \\beta_0} = \\{i \\text{ s.t. } \\mathfrak f_{\\vec \\beta, \\beta_0} (x_i) \\neq y_i\\}\\) 6.3.1 Perceptrons Let \\((\\vec \\beta^*, \\beta_0^*) := \\underset{(\\vec \\beta, \\beta_{0})}{\\operatorname{min}} \\sigma(\\vec \\beta, \\beta_0)\\), then the perceptron classifier is \\(\\mathfrak f_{\\vec \\beta^*, \\beta_0^*} (x)\\). There exist many solutions of perceptron problems but some hyperplanes seem to be more robust: Example 6.3 Hyperplane 2 seems to be more robust. 6.3.2 SVMs See next chapter. "],["support-vector-machine.html", "Chapter 7 Support Vector Machine 7.1 Hard Margin SVM 7.2 Soft Margin SVM", " Chapter 7 Support Vector Machine 7.1 Hard Margin SVM Let’s suppose that \\((x_i, y_i)_{i=1,\\cdots,n}\\) is linearly separable (for motivation for now). Then there exists at least one \\(\\vec \\beta, \\beta_0\\) s.t. \\(\\mathcal M_{\\vec \\beta, \\beta_0} = \\emptyset\\). What we want is to find \\[ \\begin{aligned} \\underset{(\\vec \\beta, \\beta_{0})}{\\operatorname{max}} &amp;~margin(\\vec \\beta, \\beta_0)\\\\ s.t. &amp;~\\mathcal M_{\\vec \\beta, \\beta_0} = \\emptyset \\end{aligned} \\] where \\(margin(\\vec \\beta, \\beta_0):= min\\{C^+_{\\vec \\beta, \\beta_0}, C^-_{\\vec \\beta, \\beta_0}\\}\\), \\(C^+_{\\vec \\beta, \\beta_0} = \\underset{x_i \\text{ s.t. } y_i = 1}{\\operatorname{min}} dist(x_i, \\mathcal H_{\\vec \\beta, \\beta_0})\\) and \\(C^-_{\\vec \\beta, \\beta_0} = \\underset{x_i \\text{ s.t. } y_i = -1}{\\operatorname{min}} dist(x_i, \\mathcal H_{\\vec \\beta, \\beta_0})\\). Let \\(\\tilde \\beta = \\frac{\\vec \\beta}{|| \\vec \\beta ||}\\), \\(\\tilde \\beta_0 = \\frac{\\beta_0}{|| \\vec \\beta ||}\\), where \\(||\\tilde \\beta || = 1\\). Then we have \\(\\mathcal H_{\\tilde \\beta, \\tilde \\beta_0} = \\mathcal H_{\\vec \\beta, \\beta_0}\\), \\(\\mathcal H^+_{\\tilde \\beta, \\tilde \\beta_0} = \\mathcal H^+_{\\vec \\beta, \\beta_0}\\), and \\(\\mathcal H^-_{\\tilde \\beta, \\tilde \\beta_0} = \\mathcal H^-_{\\vec \\beta, \\beta_0}\\). Thus, the Geometric Formulation of SVM is: \\[ \\begin{aligned} \\underset{(\\vec \\beta, \\beta_{0})}{\\operatorname{max}} &amp;~margin(\\tilde \\beta, \\tilde \\beta_0)\\\\ s.t. &amp;~\\mathcal M_{\\tilde \\beta, \\tilde \\beta_0} = \\emptyset\\\\ &amp;~||\\tilde \\beta|| = 1 \\end{aligned} \\] As \\(dist(x, \\mathcal H_{\\tilde \\beta, \\tilde \\beta_0}) = | &lt;x, \\tilde \\beta&gt; + \\tilde \\beta_0|\\), we have \\(margin(\\tilde \\beta, \\tilde \\beta_0) = \\underset{i=1.\\cdots,n}{\\operatorname{min}} | &lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0|\\). As \\(\\mathcal M_{\\tilde \\beta, \\tilde \\beta_0} = \\emptyset\\), we have \\(sign(&lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0) = y_i\\), \\(\\forall i=1, \\cdots, n\\). Then, \\(y_i (&lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0) \\geq 0\\), \\(\\forall i=1,\\cdots,n\\). Thus, \\(| &lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0| = y_i (&lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0)\\), \\(\\forall i=1, \\cdots, n\\). As a result, the margin becomes: \\[ \\begin{aligned} m &amp;= \\underset{i=1.\\cdots,n}{\\operatorname{min}} y_i(&lt;x_i, \\tilde \\beta&gt; + \\tilde \\beta_0)\\\\ &amp;\\qquad \\qquad \\Updownarrow\\\\ 1 &amp;= \\underset{i=1.\\cdots,n}{\\operatorname{min}} y_i(&lt;x_i, \\frac{\\tilde \\beta}{m}&gt; + \\frac{\\tilde \\beta_0}{m}) \\end{aligned} \\] Let \\(\\beta = \\frac{\\tilde \\beta}{m}\\), \\(\\beta_0 = \\frac{\\tilde \\beta_0}{m}\\), then we also have \\(1 \\leq y_i(&lt;x_i, \\beta&gt; + \\beta_0)\\), \\(\\forall i=1, \\cdots, n\\). As \\(|| \\beta || = || \\frac{\\tilde \\beta}{m} || = \\frac{1}{m}\\), we have \\(m = \\frac{1}{|| \\beta ||}\\). Thus, the geometric formulation of SVM becomes: \\[ \\begin{aligned} \\underset{(\\beta, \\beta_{0})}{\\operatorname{max}} &amp;~\\frac{1}{|| \\beta ||}\\\\ s.t. &amp;~y_i(&lt;x_i, \\beta&gt; + \\beta_0) \\geq 1 ~\\forall i=1,\\cdots,n\\\\ &amp;\\qquad \\qquad \\Updownarrow\\\\ \\underset{(\\beta, \\beta_{0})}{\\operatorname{min}} &amp;~|| \\beta ||\\\\ s.t. &amp;~y_i(&lt;x_i, \\beta&gt; + \\beta_0) \\geq 1 ~\\forall i=1,\\cdots,n\\\\ &amp;\\qquad \\qquad \\Updownarrow\\\\ \\underset{(\\beta, \\beta_{0})}{\\operatorname{min}} &amp;~|| \\beta ||^2\\\\ s.t. &amp;~y_i(&lt;x_i, \\beta&gt; + \\beta_0) \\geq 1 ~\\forall i=1,\\cdots,n \\end{aligned} \\] which is the Convex Optimization Formulation of SVM. Remark. SVM problem is fessible only when data set is linearly separable. SVM problem is a convex optimization problem. Duality for Convex Optimization: Primal problem (P): \\[ \\begin{aligned} \\underset{z\\in R^s}{\\operatorname{min}} &amp;~f(z)\\\\ s.t. &amp;~h_i(z) \\leq 0 \\text{ (constraints) }~\\forall i=1,\\cdots,n \\end{aligned} \\] We first introduce the notion of Lagrangian: \\[ \\mathcal L(z, \\lambda) = f(z) + \\sum_{i=1}^n \\lambda_i h_i(z) \\] where \\(\\lambda \\in R^n\\), \\(n\\) is the number of constraints and \\(\\lambda = (\\lambda_1, \\cdots, \\lambda_n)\\) is the vector of Lagrangian multipliers. Let \\(g(h) = \\underset{z\\in R^s}{\\operatorname{min}} ~\\mathcal L(z, \\lambda)\\), where \\(\\lambda\\) is fixed, then we have the Dual Problem of (P) ((D)): \\[ \\begin{aligned} \\underset{\\lambda}{\\operatorname{max}} &amp;~g(\\lambda)\\\\ s.t. &amp;~\\lambda_i \\geq 0 ~\\forall i=1,\\cdots,n \\end{aligned} \\] Let’s suppose that \\(f:R^s \\to R\\) and \\(h_i:R^s \\to R\\) are all differentiable and convex functions. Then, \\[ \\begin{aligned} \\underset{\\lambda \\geq 0}{\\operatorname{max}} g(\\lambda) &amp;= \\underset{\\lambda \\geq 0}{\\operatorname{max}} \\underset{z\\in R^s}{\\operatorname{min}} \\mathcal L(z,\\lambda)\\\\ &amp;= \\underset{z\\in R^s}{\\operatorname{min}} \\underset{\\lambda \\geq 0}{\\operatorname{max}} \\mathcal L(z,\\lambda)\\\\ &amp;= \\underset{z\\in R^s}{\\operatorname{min}} f(z) ~(\\text{suppose min and max can be swapped})\\\\ &amp;\\qquad s.t. ~h_i(z) \\leq 0 ~\\forall i=1,\\cdots,n \\end{aligned} \\] For the equation mentioned above: \\[ \\underset{\\lambda \\geq 0}{\\operatorname{max}} \\mathcal L(z,\\lambda) = \\underset{\\lambda \\geq 0}{\\operatorname{max}} f(z) + \\sum_{i=1}^n \\lambda_i h_i(z) = \\begin{cases}\\infty&amp; \\exists i, \\text{ s.t. } ~h_i(z) &gt; 0\\\\f(z) &amp; \\forall i, ~h_i(z) \\leq 0\\end{cases} \\] Theorem 7.1 (Karush-Kuhn-Tucker) Suppose \\(\\exists \\tilde z\\), s.t. \\(h_i(\\tilde z) \\leq 0\\), \\(\\forall i=1,\\cdots, n\\) (Slatter’s condition) Suppose \\(f, h_i\\) are differentiable and convex functions. Then, \\(\\forall z^*\\) solution to (P), \\(\\exists \\lambda^*\\) solution to (D), s.t. \\(\\vec 0 ~(\\in R^s) = \\nabla f(z^*) + \\sum_{i=1}^n \\lambda_i^* \\nabla h_i(z^*)\\) (Stationarity) \\(h_i(z^*) \\geq 0\\), \\(\\forall i=1,\\cdots,n\\) (Primal Feasibility) \\(\\lambda_i^* \\geq 0\\), \\(\\forall i=1,\\cdots,n\\) (Dual Feasibility) \\(\\lambda_i^* h_i(z^*) = 0\\), \\(\\forall i=1,\\cdots,n\\) (Complementary Slackness) Conversely, if \\((z^*, \\lambda^*)\\) satisfy 1-4, then \\(z^*\\) is a solution to (P) and \\(\\lambda^*\\) is a solution to (D). Back to SVM problem: \\[ \\begin{aligned} \\underset{(\\beta, \\beta_{0})}{\\operatorname{min}} &amp;~\\frac{|| \\beta ||^2}{2}\\\\ s.t. &amp;~y_i(&lt;x_i, \\beta&gt; + \\beta_0) \\geq 1 ~\\forall i=1,\\cdots,n \\end{aligned} \\] Let \\(z = (\\beta, \\beta_0)\\), \\(f(z) = \\frac{|| \\beta ||^2}{2}\\), \\(h_i(z) = 1 - y_i(&lt;x_i, \\beta&gt; + \\beta_0)\\). Then we have \\(\\mathcal L(\\beta, \\beta_0, \\lambda) = \\frac{|| \\beta ||^2}{2} + \\sum_{i=1}^n \\lambda_i \\Big(1 - y_i(&lt;x_i, \\beta&gt; + \\beta_0)\\Big)\\) and \\(g(\\lambda) = \\underset{(\\beta, \\beta_{0})}{\\operatorname{min}} \\{\\frac{|| \\beta ||^2}{2} + \\sum_{i=1}^n \\lambda_i \\Big(1 - y_i(&lt;x_i, \\beta&gt; + \\beta_0)\\Big)\\}\\) Case 1: If \\(\\sum_{i=1}^n \\lambda_iy_i \\neq 0\\), \\(g(\\lambda) = -\\infty\\) Case 2: If \\(\\sum_{i=1}^n \\lambda_iy_i = 0\\), \\(g(\\lambda) = \\underset{\\beta}{\\operatorname{min}} \\{\\frac{|| \\beta ||^2}{2} + \\sum_{i=1}^n \\lambda_i \\Big(1 - y_i(&lt;x_i, \\beta&gt;)\\Big)\\}\\). To find this, we can find the critical point for the above problem: \\(\\vec 0 = \\beta - \\sum_{i=1}^n \\lambda_i y_i x_i\\). Thus, plug the \\(\\beta\\) back to the above expression, we have: \\(g(\\lambda) = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j &lt;x_i, x_j&gt; + \\sum_{i=1}^n \\lambda_i\\). As a result, we have the Dual of SVM (D): \\[ \\begin{aligned} \\underset{\\lambda \\geq 0}{\\operatorname{max}} g(\\lambda) &amp;= \\underset{\\lambda \\in R^n}{\\operatorname{max}} \\{-\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j &lt;x_i, x_j&gt; + \\sum_{i=1}^n \\lambda_i \\}\\\\ &amp;\\qquad s.t. ~ \\begin{cases}\\sum_{i=1}^n \\lambda_iy_i = 0\\\\ \\lambda_i \\geq 0 \\end{cases}~\\forall i=1,\\cdots,n \\end{aligned} \\] Now, using KKT conditions: let \\(\\lambda^*\\) be a solution to (D). From the (Stationarity), we have \\(\\beta^* = \\sum_{i=1}^n \\lambda_i^* y_i x_i\\). From the (Complementary Slackness), we have \\[ \\begin{aligned} &amp;\\lambda_i^*\\Big(1-y_i(&lt;x_i, \\beta^*&gt; + \\beta_0^*)\\Big) = 0, ~\\forall i\\\\ \\underset{\\text{choose } \\hat{i} \\text{ s.t. } \\lambda^*_{\\hat{i}} &gt; 0}{\\Rightarrow} &amp;~ 1 - y_{\\hat i} (&lt;x_{\\hat i}, \\beta^*&gt; + \\beta_0^*) = 0\\\\ \\Rightarrow &amp;~ 1 = y_{\\hat i} (&lt;x_{\\hat i}, \\beta^*&gt; + \\beta_0^*)\\\\ \\underset{y_i^2=1}{\\Rightarrow} &amp;~ y_{\\hat i} = (&lt;x_{\\hat i}, \\beta^*&gt; + \\beta_0^*)\\\\ \\Rightarrow &amp;~ (y_{\\hat i} - &lt;x_{\\hat i}, \\beta^*&gt;) = \\beta_0^* \\end{aligned} \\] Then, we go from \\(\\lambda^*\\) to \\((\\beta^*, \\beta_0^*)\\) or (D) to (P): \\[ \\begin{aligned} \\beta^* &amp;= \\sum_{i=1}^n \\lambda_i^* y_i x_i\\\\ \\beta_0^* &amp;= y_{\\hat i} - &lt;x_{\\hat i}, \\beta^*&gt;\\\\ &amp;= y_{\\hat i} - \\sum_{j=1}^n \\lambda_j^* y_j &lt;x_j, x_{\\hat i}&gt;, \\text{ where } \\hat{i} \\text{ s.t. } \\lambda^*_{\\hat{i}} &gt; 0 \\end{aligned} \\] One can show that the primal problem (P) has a unique solution as shown above (Use strictly convex to show uniqueness for \\(\\beta^*\\)). Thus the classifier is: \\[ \\begin{aligned} l(x)&amp;= \\begin{cases} 1 &amp; &lt;\\beta^*, x&gt; + \\beta_0^* &gt; 0\\\\ -1 &amp; &lt;\\beta^*, x&gt; + \\beta_0^* &lt; 0 \\end{cases}\\\\ &amp;= \\begin{cases} 1 &amp; \\sum_{j=1}^n \\lambda_j^* y_j (&lt;x_j, x&gt; - &lt;x_j, x_i&gt;) + y_i &gt; 0\\\\ -1 &amp; o.w. \\end{cases}, \\text{ where }i \\text{ s.t. } \\lambda_i^*&gt;0\\\\ \\end{aligned} \\] which depends on \\(x\\) and \\(x_i\\) only through inner products. This is a crucial property that we will exploit to define a larger class of SVM classifiers. Remark. Hard Margin SVM problem has a solution (primal problem is feasible) if data is linearly separable . In KKT, primal problem is feasible iff dual problem is bounded (in which case both primal and dual have solution and it makes sense to talk about \\(z^*\\) and \\(\\lambda^*\\)). Definition 7.1 (Support Vectors) Under the assumptions, let \\(\\beta^*, \\beta_0^*, \\lambda^*\\) be the solution to (P). We say \\(x_i\\) is a support vector if \\(y_i (&lt;x_i, \\beta^*&gt; + \\beta_0^*) = 1\\). In particular, the minimum distance between \\(\\{x_1, \\cdots, x_n\\}\\) and the optimal hyperplane \\(\\mathcal H_{\\beta^*, \\beta_0^*}\\) is achieved at the support vectors. Theorem 7.2 (Stability Property of SVMs) Let \\(x_i\\) be a training data point and not a support vector. Suppose \\(x_i\\) is changed for a point \\(\\tilde x_i\\) s.t. \\(y_i (&lt;x_i, \\beta^*&gt; + \\beta_0^*) \\geq 1\\). Then the solution for the new data set is the same for the original data set. Proof. \\((\\lambda^*, \\beta_0^*, \\beta^*)\\) from the original problem still satisfy KKT condition for new data set: (Stationarity) As \\(x_i\\) is not a support vector, we have \\(\\lambda^*_i = 0\\). Thus, \\(\\beta^* = \\sum_{j=1}^n \\lambda_j^* y_j x_j = \\sum_{j \\neq i} \\lambda_j^* y_j x_j + \\lambda_i^* y_i x_i = \\sum_{j \\neq i} \\lambda_j^* y_j x_j + \\lambda_i^* y_i \\tilde x_i\\), which means that \\(\\beta^*\\) doesn’t change. (Primal Feasibility) \\(y_j (&lt;\\beta^*, x_j&gt; + \\beta_0^*) \\geq 1\\), \\(\\forall j \\neq i\\) and \\(y_i (&lt;\\beta^*, x_j&gt; + \\beta_0^*) \\geq 1\\) by the assumption above. (Dual Feasibility) \\(\\lambda_j \\geq 0\\), \\(\\forall j\\). (Complementary Slackness) As \\(\\lambda_i^* = 0\\), \\(\\lambda_i^* (1-y_i(&lt;\\tilde x_i, \\beta^*&gt; + \\beta_0^*)) = 0\\). As we didn’t change any \\(y_i\\), we still have \\(\\sum_{j=1}^n \\lambda_i^* y_i = 0\\). As a result, \\((\\beta_0^*, \\beta^*)\\) is still a solution for the primal problem for the new data set. Remark. The robustness associated to SVMs is markedly different to what happens with LDA or Logistic Regression, where outliers can change completely the decision boundaries. It depends only on support vectors. 7.2 Soft Margin SVM What if the data set is not linearly separable? 7.2.1 Case 1 The data set is not linearly separable mildly. This case will motivate the soft margin SVM (generalizes Hard Margin SVM). \\[ \\begin{aligned} \\underset{\\beta_0, \\beta, \\xi_1, \\cdots, \\xi_n}{\\operatorname{min}} &amp;~\\frac{|| \\beta ||^2}{2} + C \\sum_{i=1}^n \\xi_i\\\\ s.t. &amp;~ \\begin{cases} y_i(&lt;x_i, \\beta&gt;) \\geq 1 - \\xi_i\\\\ \\xi_i \\geq 0 \\end{cases}, ~\\forall i=1,\\cdots,n \\end{aligned} \\] where \\(C\\) is a parameter of the problem: as \\(C \\nearrow \\infty\\) we recover the hard margin problem (in case the data is linearly separable). However, for \\(C \\in (0, \\infty)\\), the problem is always feasible. 7.2.2 Case 2 The data set is not linearly separable but by a lot. After a transformation, we will be able to transform the data into a linearly separable data set. Let \\(\\begin{aligned}\\psi: R^2 &amp;\\to \\mathcal H( = R^3)\\\\(x,y) &amp;\\to (x,y,x^2+y^2)\\end{aligned}\\). Then the data set \\((\\psi(x_1), y_1), \\cdots, (\\psi(x_n), y_n)\\) is now linearly separable, and we will work on the embedded space: (P): \\[ \\begin{aligned} \\underset{\\beta_0 \\in R, \\beta \\in \\mathcal H}{\\operatorname{min}} &amp;~\\frac{|| \\beta ||_{\\mathcal H}^2}{2}\\\\ s.t. &amp;~y_i(&lt;\\beta, \\psi(x_i)&gt;_{\\mathcal H}) \\geq 1, ~\\forall i=1,\\cdots,n \\end{aligned} \\] (D): \\[ \\begin{aligned} \\underset{\\lambda \\in R^n}{\\operatorname{max}} &amp;~ -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\lambda_j y_i y_j &lt;\\psi(x_i), \\psi(x_j)&gt;_{\\mathcal H} + \\sum_{i=1}^n \\lambda_i\\\\ &amp;s.t. ~ \\begin{cases}\\sum_{i=1}^n \\lambda_iy_i = 0\\\\ \\lambda_i \\geq 0 \\end{cases},~\\forall i=1,\\cdots,n \\end{aligned} \\] From \\(\\lambda^*\\), we can obtain \\(\\beta^* = \\sum_{j=1}^n \\lambda_j^* y_j \\psi(x_j)\\) Given \\(i\\) s.t. \\(\\lambda_i^* &gt; 0\\), \\(\\beta_0^*\\) can then be computed: \\(\\beta_0^* = y_i - &lt;\\beta^*, \\psi(x_i)&gt;_{\\mathcal H} = y_i - \\sum_{j=1}^n \\lambda_j^* y_j &lt;\\psi(x_j), \\psi(x_i)&gt;_{\\mathcal H}\\). The classifier is (\\(x \\in R^d\\)): \\[ \\begin{aligned} l_{\\psi}(x):&amp;= \\begin{cases} 1 &amp; &lt;\\beta^*, \\psi(x)&gt;_{\\mathcal H} + \\beta_0^* &gt; 0\\\\ -1 &amp; &lt;\\beta^*, \\psi(x)&gt;_{\\mathcal H} + \\beta_0^* &lt; 0 \\end{cases}\\\\ &amp;= \\begin{cases} 1 &amp; \\sum_{j=1}^n \\lambda_j^* y_j (&lt;\\psi(x_j), \\psi(x)&gt;_{\\mathcal H} - &lt;\\psi(x_j), \\psi(x_i)&gt;_{\\mathcal H}) + y_i &gt; 0\\\\ -1 &amp; o.w. \\end{cases}\\\\ \\end{aligned} \\] where \\(i\\) s.t. \\(\\lambda_i^* &gt; 0\\), and \\(\\lambda_j^*\\) only depends on \\(&lt;\\psi(x_j), \\psi(x_l)&gt;_{\\mathcal H}\\) Let \\(K(x, \\tilde x) = &lt;\\psi(x), \\psi(\\tilde x)&gt;_{\\mathcal H}\\) for arbitrary \\(x, \\tilde x \\in R^d\\). Then the only thing we need to build \\(l_\\psi\\) is the function \\(K\\). In particular, we don’t need \\(\\psi\\) explicitly. Thus, it is fair to write \\(l_\\psi = l_K\\). In particular, to produce more general SVM classifiers, we can either: Pick a “Hilbert Space” \\(\\mathcal H\\) and define a map \\(\\psi: R^d \\to \\mathcal H\\), then we can build the function \\(K\\), which is all we need to compute \\(l_\\psi\\). Pick a “Kernel” \\(K\\) and produce the classifier \\(l_K\\). Question: If we start with a “Kernel” \\(K\\), can we implicitly picking \\(\\mathcal H\\) and \\(\\psi\\)? Answer: Under fairly general condition, yes. Definition 7.2 (Kernels) A kernel over a set \\(\\mathcal X\\) is a function \\(K: \\mathcal X \\times \\mathcal X \\to R\\) with the following property: \\(\\forall n \\in N\\) and \\(\\forall\\) sequence of points \\(x_1, \\cdots, x_n \\in \\mathcal X\\), the matrix \\[ A:= \\left( \\begin{matrix} K(x_1,x_1) &amp; \\cdots &amp; K(x_1, x_n)\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ K(x_n ,x_1) &amp; \\cdots &amp; K(x_n, x_n) \\end{matrix} \\right) \\] is symmetric (\\(A = A^T\\)) and positive semi-definite \\(&lt;Av, v&gt;_{R^n} \\geq 0\\), \\(\\forall b \\in R^n\\). Remark. Kernels must be symmetric functions: \\(K(x,y) = K(y,x)\\), \\(\\forall x,y \\in \\mathcal X\\). But there are symmetric functions that are not kernels. In particular, we need to check the positive semi-definiteness condition. Proposition 7.1 Inner products are kernels: \\(K(x, y) = &lt;x,y&gt;_{R^d}\\). If \\(K_1: \\mathcal X \\times \\mathcal X \\to R\\) and \\(K_2: \\mathcal X \\times \\mathcal X \\to R\\) are two kernels, then \\(K(x,y):= a_1 K_1(x,y) + a_2 K_2(x,y)\\) is also a kernel, provided \\(a_1, a_2\\) are two non-negative numbers. If \\(K: \\mathcal X \\times \\mathcal X \\to R\\) is a kernel and \\(f:\\mathcal X \\to R\\) is a function, then \\(\\tilde K(x,y) := f(x) \\cdot f(y) \\cdot K(x,y)\\) is also a kernel. If \\(K_1: \\mathcal X \\times \\mathcal X \\to R\\) and \\(K_1: \\mathcal X \\times \\mathcal X \\to R\\) are two kernels, then \\(K(x,y) := K_1(x,y) \\cdot K_2(x,y)\\) is also a kernel. Proof. (of 1) Let \\(n \\in N\\) and \\(x_1, \\cdots, x_n \\in R^d\\) be arbitrary. Consider the matrix \\(A \\in R^{n \\times n}\\) with entries \\(A_{ij} = &lt;x_i, x_j&gt;_{R^d}\\), then It is symmetric: \\(A_{ij} = &lt;x_i, x_j&gt;_{R^d} = &lt;x_j, x_i&gt;_{R^d} = A_{ji}\\) It is positive semi-definite: Let \\(v = (v_1, \\cdots, v_n) \\in R^n\\). Then \\[ \\begin{aligned} &lt;Av, v&gt;_{R^d} &amp;= \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} v_i v_j\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n &lt;x_i, x_j&gt;_{R^d} v_i v_j\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n &lt;v_i x_i, v_j x_j&gt;_{R^d}\\\\ &amp;= &lt;\\sum_{i=1}^n v_i x_i, \\sum_{j=1}^n v_j x_j&gt;_{R^d}\\\\ &amp;= || \\sum_{i=1}^n v_i x_i ||^2_{R_d}\\\\ &amp;\\geq 0 \\end{aligned} \\] By the above proof, we can build the kernel \\(K(x,y) = &lt;\\psi(x), \\psi(y)&gt;_{\\mathcal H}\\) given \\(\\mathcal H\\) and \\(\\psi :R^d \\to \\mathcal H\\). Corollary 7.1 The following are kernels in \\(R^d\\): Inner product: \\(K(x,y) = &lt;x,y&gt;_{R^d}\\). Polynomial: \\(K(x,y) = a_0 + a_1 &lt;x,y&gt;_{R^d} + \\cdots + a_l (&lt;x,y&gt;_{R^d})^l\\) as long as \\(a_0, \\cdots, a_l \\geq 0\\). (\\((&lt;x,y&gt;_{R^d})^l\\) is a kernel) Radial Basis Kernel or Gaussian Kernel: \\(K(x,y) = \\operatorname{exp} (-\\frac{|| x-y ||^2}{\\sigma})\\) for \\(\\sigma &gt; 0\\). Remark. For every kernel we will have a different classification rule. Try the simplest classifiers first (linear classifiers or soft margin SVMs). Any classifier that can be purely defined in terms of inner products can be kernelized. For a given kernel, how do we know if the embedded data set \\((\\psi(x_1), y_1), \\cdots, (\\psi(x_n), y_n)\\) is linearly separable? In some cases it is always the case. But even if the data set is not linearly separable, we may be in the Case 3 mentioned below, where we should use the kernelized version of SVMs. Definition 7.3 (Hilbert Space) A Hilbert Space is a vector space (1) \\(\\mathcal H\\) together with an inner product (2) \\(&lt;\\cdot, \\cdot&gt;_{\\mathcal H}\\), under which \\(\\mathcal H\\) is a complete (3) metric space. Vector Space: We can add together objects in \\(\\mathcal H\\) and multiply by scales (\\(v, \\tilde v \\in \\mathcal H\\), then \\(av + \\tilde v \\in \\mathcal H\\)). Inner Product: \\(&lt;f,g&gt;_{\\mathcal H} = &lt;g,f&gt;_{\\mathcal H}\\) (symmetry) \\(&lt;af + b\\tilde f, g&gt;_{\\mathcal H} = a&lt;f,g&gt;_{\\mathcal H} + b&lt;\\tilde f, g&gt;_{\\mathcal H}\\) (linear in each entry) \\(|| f ||^2_{\\mathcal H} := &lt;f, f&gt;_{\\mathcal H} \\geq 0\\), \\(\\forall f\\in \\mathcal H\\). And \\(&lt;f, f&gt;_{\\mathcal H} = 0 \\Leftrightarrow f = 0\\). Completeness: An inner product induces a norm and thus a metric: \\(||f-g||^2_{\\mathcal H} := &lt;f-g, f-g&gt;_{\\mathcal H}\\). Under this metric, \\(\\mathcal H\\) must be a complete metric space. Properties of Hilbert Space (Cauthy-Schwartz Ineq): \\(|&lt;f,g&gt;_{\\mathcal H}| \\leq ||f||_{\\mathcal H} ||g||_{\\mathcal H}\\), \\(||f||_{\\mathcal H}:= \\sqrt{&lt;f, f&gt;_{\\mathcal H}}\\). Example 7.1 Basic example: \\((R^m, &lt;\\cdot, \\cdot&gt;_{R^m})\\) The space of “square integrable measurable function” defined on R: \\(\\mathcal L^2(R) := \\{f \\text{ measurbale s.t. } \\int_R f^2dx &lt; \\infty\\}\\). Note: \\(\\mathcal L^2(R)\\) is not correctly defined as above, and in fact one has to work with “equivalent classes”. So for the example, a measurable function \\(f\\) is identified with another function \\(\\tilde f\\) if \\(f = \\tilde f\\) a.e. (almost everywhere). In any cases, with this technical detail in mind: \\(&lt;f,g&gt;_{\\mathcal L^2(R)} := \\int_R fg ~dx\\). RKHS (Reproducing Kernel Hilbert Spaces): Definition 7.4 (Continuous) Let \\(\\mathcal H\\) be Hilbert Space, then a linear function \\(L: \\mathcal H \\to R\\) is continuous if there is a constant \\(C\\) s.t \\(|L(f)| \\leq C ||f||_{\\mathcal H}\\), \\(\\forall f \\in \\mathcal H\\). Example 7.2 Fix \\(g \\in \\mathcal H\\) and define \\(L_g(f) := &lt;f,g&gt;_\\mathcal H\\). Then \\(L_g\\) is linear and continuous: \\(L_g(f + a\\tilde f) = &lt;f+a\\tilde f, g&gt;_\\mathcal H = &lt;f,g&gt;_\\mathcal H + a&lt;\\tilde f, g&gt;_\\mathcal H = L_g(f) + aL_g(\\tilde f)\\). And by Cauthy-Schwartz Ineq, we have \\(|L_g(f)| = |&lt;f,g&gt;_\\mathcal H| \\leq ||g||_\\mathcal H \\cdot ||f||_\\mathcal H\\), \\(\\forall f \\in \\mathcal H\\). Theorem 7.3 (Riesz Representation Theorem) If \\(\\mathcal H\\) is a Hilbert Space and \\(L:\\mathcal \\to R\\) is linear and continuous, then \\(\\exists\\) a unique \\(g \\in \\mathcal H\\) s.t \\(L(f) = L_g(f)\\), \\(\\forall f \\in \\mathcal H\\). Definition 7.5 (Reproducing Kernel Hilbert Space) Let \\(\\mathcal X\\) be a set, \\(\\mathcal H\\) be a Hilbert Space of real valued functions \\(f: \\mathcal X \\to R\\) (i.e. elements in \\(\\mathcal H\\) are functions from \\(\\mathcal X\\) into \\(R\\)). \\(\\mathcal H\\) is said to be a RKHS if \\(\\forall x \\in \\mathcal X\\), the “evaluation map” \\(L_x: f \\in \\mathcal H \\to f(x) \\in R\\) is a linear and continuous map. In particular, \\(|f(x) - \\tilde f(x)| \\leq C_x || f - \\tilde f ||_\\mathcal H\\), \\(\\forall f, \\tilde f\\). Why “Reproducing Kernel”?: \\(L_x\\) is clearly linear, and if in addition it is continuous, then by Theorem 7.3, \\(\\exists\\) a unique \\(K_x \\in \\mathcal H\\) s.t \\(f(x) = L_x(f) = &lt;f, K_x&gt;_\\mathcal H\\), \\(\\forall f \\in \\mathcal H\\). That is, to evaluate \\(f\\) at \\(x\\), it is enough to take the inner product of \\(f\\) with \\(K_x\\). Now, let \\(\\tilde x \\in \\mathcal X\\), then \\(K_{\\tilde x} (x) = &lt;K_{\\tilde x}, K_x&gt;_\\mathcal H\\). Let’s call them \\(K(\\tilde x, x)\\). Then the function \\(K\\) defined above is a kernel: take \\(\\psi: \\begin{matrix} \\mathcal X \\to \\mathcal H\\\\x \\to K_x\\end{matrix}\\), then \\(K(\\tilde x, x) = &lt;K_{\\tilde x}, K_x&gt;_\\mathcal H = &lt;\\psi(\\tilde x), \\psi(x)&gt;_\\mathcal H\\). So far, we have gone from RKHS to Kernel (\\(K\\)). And the Kernel \\(K\\) has the reproducing property: \\(&lt;f, K(\\cdot, x)&gt;_\\mathcal H \\ f(x)\\), \\(\\forall f\\). How about we go from Kernel (\\(K\\)) to RKHS? Theorem 7.4 (Moore-Aronszjan Theorem) Let \\(K\\) be a kernel defined on \\(\\mathcal X\\). Define \\[ \\begin{aligned} \\mathcal H = \\Big \\{\\sum_{i=1}^\\infty a_i K(\\cdot, x_i): &amp;\\text{for some colletcion }a_i \\in R, x_i \\in \\mathcal X \\\\ &amp;\\text{with } \\sum_{i,j}a_i a_j K(x_i, x_j) &lt; \\infty\\Big \\} \\end{aligned} \\] and \\[ &lt;\\sum_{i=1}^\\infty a_i K(\\cdot, x_i), \\sum_{j=1}^\\infty b_j K(\\cdot, y_j)&gt;_\\mathcal H := \\sum_{i=1}^\\infty\\sum_{j=1}^\\infty a_i b_j K(x_i, y_j). \\] Think of this as \\(a^T K b\\) where \\(a = (a_1, \\cdots)^T\\), \\(b = (b_1, \\cdots)^T\\), \\(K = \\left( \\begin{matrix}K(x_1,y_1) &amp; \\cdots\\\\\\vdots &amp; \\ddots \\end{matrix}\\right)\\). It can be shown that \\(\\mathcal H\\) with this inner product is a Hilbert Space and \\(K\\) is a reproducing kernel. Proof. Let \\(f \\in \\mathcal H\\), \\(f = \\sum_{i=1}^\\infty a_i K(\\cdot, x_i)\\). Then, \\(&lt;f, K(\\cdot, x)&gt;_\\mathcal H = &lt;\\sum_{i=1}^\\infty a_i K(\\cdot, x_i), K(\\cdot, x)&gt; = \\sum_{i=1}^\\infty a_i K(x, x_i) = f(x)\\). 7.2.3 Case 3 Use the kernelized version of Soft Margin SVM. "],["regularized-empirical-risk-minimization.html", "Chapter 8 Regularized Empirical Risk Minimization 8.1 ill Posed Problems 8.2 ERM with Kernels 8.3 Regularity", " Chapter 8 Regularized Empirical Risk Minimization 8.1 ill Posed Problems Let’s start with a simple example: in linear regression, we try to find \\(\\beta \\in \\mathcal R^d\\) s.t \\(\\sum_{i=1}^n (y_i - &lt;\\beta, x_i&gt;_{\\mathcal R^d})^2\\) is minimized. The solution \\(\\beta^* = (X^T X)^{-1} X^T Y\\) only make sense when \\((X^TX)^{-1}\\) is invertible, or the optimization problem has a unique solution. The ill posed problems are the situation where there is no unique solution or \\((X^TX)^{-1}\\) is not invertible (\\(d &gt;&gt; n\\), not full rank, …). Ridge regression aims at tackling this problem by adding a “regularizer” to the optimization problem: \\(\\underset{\\beta}{\\operatorname{min}} \\lambda || \\beta ||^2 + \\sum_{i=1}^n (y_i - &lt;x_i, \\beta&gt;)^2\\) with \\(\\beta^* = (X^T X + \\lambda I)^{-1} X^T Y\\), where higher \\(\\lambda\\) implies more “regular” and lower \\(\\lambda\\) implies lesser “regular”. Naturally, one needs to choose \\(\\lambda \\in (0, \\infty)\\) to balance between variance and bias. In more generality, suppose we wanted to fit a function \\(f\\) that explains some observed data (\\(f: [0,1] \\to \\mathcal R\\)): We are trying to solve \\(\\underset{f \\in Z}{\\operatorname{min}} \\sum_{i=1}^n (f(x_i) - y_i)^2\\). How many functions are close to the observations is the ill posed problem. The idea in empirical risk minimization is to enforce “regularity” on the functions by adding a regularizer to the empirical risk object function: \\(\\underset{f \\in Z}{\\operatorname{min}} \\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda R(f)\\), where \\(Z\\) is some family of functions for which the objective function makes sense and \\(R\\) is some regularizer. Remark. It is important to make sure that the objective function makes sense for elements \\(f \\in Z\\). For example, \\(Z\\) can not be \\(\\mathcal L^2(\\mathcal R)\\), since for an arbitrary \\(f \\in \\mathcal L^2(\\mathcal R)\\), we can not make sense of \\(f(x_i)\\). In any case, after choosing \\(Z\\) and \\(R\\), we consider the solution to the regularized empirical risk minimization problem as a regression function for the data. Notice that this approach is a non-parametric approach (\\(f \\in Z\\)). Q: What types of regularizers we could work with? A: Later we will consider \\(Z = \\mathcal H\\) (RKHS) and \\(R(f) = ||f||^2_\\mathcal H\\), but for now let us consider an unrelated setting: Example 8.1 Let \\(\\mathcal X = [0,1]\\) and the data \\(0&lt;x_1&lt;\\cdots&lt;x_n&lt;1\\) with corresponding values \\(y_1,\\cdots,y_n \\in \\mathcal R\\). Let \\(Z\\) be the set of functions \\(f:[0,1] \\to \\mathcal R\\) s.t \\(f(x) = \\int_0^x f&#39;(t)dt\\), \\(\\forall x \\in (0,1)\\) and s.t \\(\\int_0^1 (f&#39;(x))^2 dx &lt; \\infty\\). Define \\(C_1([0,1])\\) as Absolutely Continuous Functions defined on [0,1]. Then, \\(C_1([0,1]) \\subset Z\\) i.e. \\(f \\in C_1([0,1]) \\text{ and } f(0) = 0\\Rightarrow f\\in Z\\). Now let \\(J(f):= \\lambda \\int_0^1 (f&#39;(x))^2 dx (\\text{ Regularization Term}) + \\sum_{i=1}^n (f(x_i) - y_i)^2 (\\text{Fidality Term})\\). The goal is to: Minimize \\(J\\): \\(\\underset{f \\in Z}{\\operatorname{min}} J(f)\\). Obtain a “first order” condition for optimality. In search of inspiration, let’s go back to the Euclidean case: \\(\\underset{x \\in \\mathcal R^m}{\\operatorname{min}} F(x)\\). First order condition: \\(\\nabla F(x^*) = \\vec 0\\), that is \\(\\frac{\\partial F}{\\partial x_i}(x^*) = 0\\), \\(\\forall i=1,\\cdots,m\\). Moreover, if \\(x^*\\) minimizes \\(F\\) over the whole \\(\\mathcal R^m\\), it certainly minimizes \\(F\\) over the line \\(\\{x^* + tv: t\\in \\mathcal R\\}\\). Thus, fix \\(v\\) and define the function \\(\\phi_v(t):= F(x^* + tv)\\). Then this function has a minimum at \\(t=0\\), which means that \\[ 0 = \\phi&#39;_v(0) = \\frac{d}{dt} F(x^* + tv) |_{t=0} = &lt;\\nabla F(x^*), v&gt; = \\partial_v F(x^*) \\] The optimality condition in \\(\\mathcal R^m\\) is equivalent to the optimality condition fro a collection of functions in \\(\\mathcal R\\). So, going back to Example 8.1, suppose \\(f^*\\) solves the problem (it is a minimizer). Pick an arbitrary \\(g \\in Z\\) and consider the function: \\(\\phi_g(t):= J(f^* + tg)\\), \\(t \\in \\mathcal R\\). Then \\(\\phi_g\\) has a minimum at \\(t=0\\) (\\(\\phi&#39;_g(0) = 0\\)). Thus, we have: \\[ \\begin{aligned} J(f^* + tg) &amp;= \\lambda \\int_0^1 (f^{*\\prime} + tg&#39;)^2 dx + \\sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2\\\\ &amp;= \\lambda \\int_0^1 (f^{*\\prime})^2 dx + 2 \\lambda t \\int_0^1 f^{*\\prime} g&#39; dx + \\lambda t^2 \\int_0^1 (g&#39;)^2 dx + \\sum_{i=1}^n (f^*(x_i) + tg(x_i) - y_i)^2 \\end{aligned} \\] Then, \\(0 = \\phi&#39;_g(0) = 2 \\lambda \\int_0^1 f^{*\\prime} g&#39; dx + 2\\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i)\\), which means that \\[ \\lambda \\int_0^1 f^{*\\prime} g&#39; dx = -\\sum_{i=1}^n (f^*(x_i) - y_i)g(x_i) \\] This has to be true \\(\\forall g \\in Z\\). These equations are called Euler Lagrange Equations or First Order Optimality Conditions. Above goes from \\(f^*\\) to First Order Optimality Conditions. Now consider the opposite situation: Take \\(g=g_1\\) where \\(g_1(x):= \\operatorname{min} \\{x,x_1\\}\\). It follows that \\(\\lambda \\int_0^{x_1} f^{*\\prime} dx = -\\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \\wedge x_1)\\). As \\(\\int_0^{x_1} f^{*\\prime} dx = f^*(x_1) - f^*(0) = f^*(x_1)\\), we have: \\[ \\lambda f^*(x_1) = -\\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \\wedge x_1) \\] Doing the same for \\(j = 2, \\cdots, n\\), we obtain the system of equations: \\[ \\lambda f^*(x_j) = -\\sum_{i=1}^n (f^*(x_i) - y_i)(x_i \\wedge x_j), ~j = 1, \\cdots, n \\] We can solve this system to get the values of \\(f^*\\) at the points \\(x_1, \\cdots, x_n\\). For the rest of \\(f^*\\), use linear interpolation for each interval and use a constant after \\(x_n\\): To see \\(f^*\\) is linear in \\([0,x_1]\\), it is enough to show \\((f^*)&#39;&#39; = 0\\) in the interval: Take \\(g: [0,1] \\to \\mathcal R\\) s.t \\(g(x)=0\\), \\(\\forall x \\geq x_1\\) and \\(g(0) = 0\\), \\(g \\in Z\\). Then, we must have \\(\\int_0^{x_1} (f^*)&#39; g&#39; dx = 0\\). As \\(\\int_0^{x_1}(f^*)&#39; g&#39; dx = -\\int_0^{x_1}(f^*)&#39;&#39; g dx + (f^*)&#39; g |_0^{x_1} = -\\int_0^{x_1}(f^*)&#39;&#39; g dx\\), \\(\\forall g\\). Thus, we have \\((f^*)&#39;&#39; \\equiv 0\\). To see \\(f^*\\) is constant in \\([x_n, 1]\\): \\[ \\begin{aligned} J(f)&amp;= \\lambda \\int_0^1 (f&#39;(x))^2 dx + \\sum_{i=1}^n (f(x_i) - y_i)^2\\\\ &amp;= \\lambda \\int_0^{x_n} (f&#39;(x))^2 dx + \\lambda \\int_{x_n}^1 (f&#39;(x))^2 dx + \\sum_{i=1}^n (f(x_i) - y_i)^2 \\end{aligned} \\] As \\(J(f)\\) has a minimum when \\(f&#39;(x) = 0\\) at \\([x_n, 1]\\) and \\(f^*\\) minimizes \\(J(f)\\), we have \\(f^*\\) is constant in \\([x_n, 1]\\). 8.2 ERM with Kernels Let \\(K\\) be a kernel over \\(\\mathcal X\\) and consider \\(\\mathcal H\\) the RKHS associated to \\(K\\). Consider the problem: \\[ \\underset{f \\in \\mathcal H}{\\operatorname{min}}\\underset{\\text{Regularizer}}{\\lambda || f ||_\\mathcal H^2} + \\underset{\\text{Fidality}}{\\sum_{i=1}^n (y_i - f(x_i))^2} \\] This is actually Ridge Regression in disguise: let \\(\\psi: \\begin{matrix}\\mathcal X \\to \\mathcal H\\\\x \\to K(\\cdot, x) \\end{matrix}\\), \\(f \\in \\mathcal H\\), then \\(f(x_i) = &lt;f, K(\\cdot, x_i)&gt;_\\mathcal H = &lt;f, \\psi(x_i)&gt;_\\mathcal H\\), which means we want to solve: \\[ \\underset{f \\in \\mathcal H}{\\operatorname{min}}\\underset{\\text{Regularizer}}{\\lambda || f ||_\\mathcal H^2} + \\underset{\\text{Fidality}}{\\sum_{i=1}^n (y_i - &lt;f, \\psi(x_i)&gt;_\\mathcal H)^2} \\] Here, \\(\\phi_g(t) = \\lambda || f^* + tg||^2_\\mathcal H + \\sum_{i=1}^n (f^*(x_i) - y_i)^2\\), where \\(|| f^* + tg||^2_\\mathcal H = &lt;f^* + tg, f^* + tg&gt;_\\mathcal H = &lt;f^*, f^*&gt;_\\mathcal H + 2t&lt;f^*, g&gt;_\\mathcal H + t^2 &lt;g,g&gt;_\\mathcal H\\). Theorem 8.1 (Representer Theorem) After using the Euler-Lagrange equations with the reproducing property of the kernel, we will deduce: \\[ f^* = \\sum_{i=1}^n a_i^* K(\\cdot, x_i) \\] Moreover, we can characterize the coefficient completely from \\(y_1, \\cdots, y_n\\) and the matrix \\(K = \\{K(x_i, x_j)\\}\\): \\[ \\begin{aligned} \\underset{a_1, \\cdots, a_n}{\\operatorname{min}} \\lambda || \\sum_{j=1}^n a_j K(\\cdot, x_j)||^2_\\mathcal H &amp;+ \\sum_{i=1}^n(\\sum_{j=1}^n a_j K(x_i, x_j) - y_i)^2\\\\ &amp;\\Updownarrow\\\\ \\underset{\\vec a \\in \\mathcal R^n}{\\operatorname{min}} \\lambda &lt;K \\vec a, \\vec a&gt;_{\\mathcal R^n} &amp;+ ||K \\vec a - \\vec y||^2\\\\ \\end{aligned} \\] 8.2.1 Comparison Example 8.2 Go back to Example 8.1, \\(f^*\\) coincides with the solution to a problem of the form \\[ \\underset{f \\in \\mathcal H}{\\operatorname{min}} \\lambda ||f||^2_\\mathcal H + \\sum_{i=1}^n (f(x_i) - y_i)^2 \\] where \\(\\mathcal H\\) is the RKHS for a well chosen kernel \\(K\\). Let \\(K: [0,1] \\times [0,1] \\to \\mathcal R\\), \\(K(x, \\tilde x) = x \\wedge \\tilde x\\), by the representer theorem, the solution to the above problem takes the form: \\[ \\tilde f := \\sum_{i=1}^n a_i^* ~min\\{\\cdot, x_i\\} \\] which coincides with the solution \\(f^*\\) to \\(\\underset{f \\in Z}{\\operatorname{min}} J(f)\\) (linear within intervals). In general, we can consider the optimization problem: \\[ \\underset{f \\in \\mathcal H}{\\operatorname{min}} \\lambda ||f||^2_\\mathcal H + \\sum_{i=1}^n G_i(f(x_i)) \\] where \\(G: \\mathcal R \\to \\mathcal R\\) is general depends on \\(y_i\\). For Regression: \\(G_i(t) = (t - y_i)^2\\) (Usual Square Loss) But there are reasons to use things like \\(G_i(t) = G(t - y_i)\\), where \\(G\\) is the Hubber Loss: \\(G(t) = \\begin{cases} t^2 &amp; t \\in [-1,1]\\\\ |t| &amp; o.w.\\end{cases}\\). For example, to take care of outliers in the labels \\(y_i\\). For Classification: \\(y_i \\in \\{-1,1\\}\\) \\(G_i(t) = \\log(\\frac{exp(y_i-t)}{1 + exp(y_i-t)})\\), problem above is then a regularized (non-parametric) logistic regression. \\(G_i(t) = max\\{0, 1 - ty_i\\}\\), problem above is then a kernelized soft margin SVM. Notice that when we solve the above problem for a classification problem, we get \\(f^* :\\mathcal X \\to \\mathcal R\\), we still need to do \\[ l(x):= \\begin{cases} 1 &amp; f^*(x) &gt; 0\\\\ -1 &amp; f^*(x) &lt; 0 \\end{cases} \\] Theorem 8.2 (Generalized Representer Theorem) Consider the problem \\[ \\underset{f \\in \\mathcal H}{\\operatorname{min}} \\lambda ||f||^2_\\mathcal H + \\sum_{i=1}^n G_i(f(x_i)), \\] the solution is \\(f^* = \\sum_{i=1}^n a_i^* K(\\cdot, x_i)\\), where \\(a_i^*\\) are \\[ \\begin{aligned} \\underset{\\vec a \\in \\mathcal R^n}{\\operatorname{min}} \\lambda &lt;K \\vec a, \\vec a&gt;_{\\mathcal R^n} + \\sum_{i=1}^n G_i\\Big (\\sum_{j=1}^n a_j K(x_i, x_j)\\Big) \\end{aligned} \\] 8.3 Regularity For a given kernel \\(K\\) over \\(\\mathcal R^d\\), its associated RKHS \\(\\mathcal H\\) is built using the function \\(K(\\cdot, x)\\), \\(x \\in \\mathcal R^d\\). Thus, it is to be expected that the regularity of the kernel influences the regularity of the element in \\(\\mathcal H\\). For example, if \\(K(\\cdot, \\cdot)\\) has derivatives of all orders, it should not come as a surprise that the element in \\(\\mathcal H\\) have derivatives of all orders (This is the case for the Gaussian Kernel \\(K(x, \\tilde x) = exp(-\\frac{|x - \\tilde x|^2}{\\sigma^2})\\)). Conversely, if the kernel \\(K(x, \\tilde x)\\) is not differentiable (\\(K(x, \\tilde x) = x \\wedge \\tilde x\\)), the function in \\(\\mathcal H\\) will not be very smooth either. Also, notice that for the Gaussian Kernel , all functions \\(K(\\cdot, x)\\) have the same amount of regularity, this is because \\(K\\) is homogeneous, i.e. \\(K(x, \\tilde x) = h(x - \\tilde x)\\). In some applications however, it may be useful to have a space of functions \\(\\mathcal H\\) with inhomogeneous regularity. To change the set of examples of homogeneous kernels, we introduce the following theorem. Theorem 8.3 (Bochner's Theorem) (A simplified version that doesn’t involve complex random vectors) Suppose that \\(Z\\) is a d-dimensional random vector and that its characteristic function: \\(h(\\vec u):= E[e^{\\textbf{i} &lt;Z, \\vec u&gt;_{\\mathcal R^d}}]\\), \\(\\vec u \\in \\mathcal R^d\\) is real valued. Then, the function \\(K: \\mathcal R^d \\times \\mathcal R^d \\to \\mathcal R\\) defined by \\(K(x, \\tilde x) = h(x - \\tilde x)\\) is a kernel. Remark. Bochner’s Theorem includes a converse statement (If \\(K\\) is a homogeneous kernel, then \\(\\exists r.v. Z)\\), s.t. the above condition holds) and a more general version for complex valued random vectors. One way to produce kernel: \\(K(x, \\tilde x) = E[e^{\\textbf{i} &lt;Z, x - \\tilde x&gt;}] \\underset{M.C.S}{\\approx} \\frac{1}{N} \\sum_{j=1}^n e^{\\textbf{i} &lt;z_j, x - \\tilde x&gt;}\\). Proof. (Sketch) Symmetry: \\(K(x, \\tilde x) = K(\\tilde x, x)\\). We would need to show that \\(h(\\vec u) = h(-\\vec u)\\). However, if a characteristic function is real values, necessarily it has to be symmetric: \\[ \\begin{aligned} E[e^{\\textbf{i} &lt;\\vec u, Z&gt;}] &amp;= E[cos(&lt;\\vec u, Z&gt;) + \\textbf{i} ~sin(&lt;\\vec u, Z&gt;)]\\\\ &amp;= E[cos(&lt;\\vec u, Z&gt;)]\\\\ &amp;= E[cos(&lt;-\\vec u, Z&gt;)]\\\\ &amp;= E[e^{\\textbf{i} &lt;-\\vec u, Z&gt;}] \\end{aligned} \\] Positive Definiteness: \\[ \\begin{aligned} &lt;Kv ,v&gt;_{\\mathcal R^d} &amp;= \\sum_{i=1}^n \\sum_{j=1}^n K(x_i, x_j) v_i v_j\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n E[e^{\\textbf{i} &lt;x_i - x_j, Z&gt;}] v_i v_j\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n E[e^{\\textbf{i} &lt;x_i, Z&gt;} e^{-\\textbf{i} &lt;x_i, Z&gt;}] v_i v_j\\\\ &amp;= E\\Big[(\\sum_{i=1}^n v_ie^{\\textbf{i} &lt;x_i, Z&gt;}) \\cdot (\\sum_{j=1}^n v_j e^{-\\textbf{i} &lt;x_i, Z&gt;}) \\Big]\\\\ &amp;:= E[T \\cdot \\bar T]\\\\ &amp;= E[|T|^2]\\\\ &amp;\\underset{\\text{conjugate of complex number}}{=}E\\Big[|\\sum_{j=1}^n v_j e^{\\textbf{i} &lt;x_i, Z&gt;}|^2\\Big]\\\\ &amp;\\geq 0 \\end{aligned} \\] Thanks to this theorem, we can show that the following \\(K(x, \\tilde x) = h(x - \\tilde x)\\), \\(x, \\tilde x \\in \\mathcal R^d\\) are kernels, where Example 8.3 (Rational Quadratic Family) \\[ h(\\vec u) = (1 + \\frac{|\\vec u|^2}{\\alpha l^2})^{-\\alpha}, ~\\alpha, l &gt; 0 \\] When \\(\\alpha \\to \\infty\\), we recover Gaussian Kernel. Example 8.4 (Exponential Family) \\[ h(\\vec u) = exp(-\\frac{|\\vec u|^\\gamma}{l^\\gamma}), ~0 &lt; \\gamma \\leq 2, ~l &gt; 0 \\] When \\(\\gamma = 2\\), we get Gaussian Kernel. Change \\(\\gamma\\), we change the regularity. Example 8.5 (Matern Family of Kernels) \\(\\cdots\\) These are all examples of homogeneous kernels. Now let’s make inhomogeneous kernels from homogeneous ones. Let \\(\\beta: \\mathcal R^d \\to \\mathcal R^m\\) be a non-linear function and let \\(\\tilde K: \\mathcal R^m \\times \\mathcal R^m \\to \\mathcal R\\) be as kernel. Then, \\(K(x, \\tilde x):= \\tilde K(\\beta(x), \\beta(\\tilde x))\\) is a kernel over \\(\\mathcal R^d\\). In particular, if \\(\\tilde K(z, \\tilde z) = h(z - \\tilde z)\\) is one of the homogeneous kernels we considered before, \\(K(x, \\tilde x) = h(\\beta(x) - \\beta(\\tilde x))\\) is a kernel which in general will not be homogeneous (\\(h(\\beta(x) - \\beta(\\tilde x)) \\neq h(\\beta(\\tilde x) - \\beta(x))\\)). Example 8.6 Let \\(\\beta(x): x \\in \\mathcal R \\to (cos(x), sin(x))\\). Take \\(h(\\vec u) = exp(-\\frac{|\\vec u|^2}{\\sigma^2})\\), then \\[ \\begin{aligned} K(x, \\tilde x) &amp;= exp(- \\frac{[cos(x) - cos(\\tilde x)]^2 + [sin(x) - sin(\\tilde x)]^2}{\\sigma^2})\\\\ &amp;= exp(-\\frac{4sin^2(\\frac{x - \\tilde x}{2})}{\\sigma^2}) \\end{aligned} \\] "],["uncertainty-quantification.html", "Chapter 9 Uncertainty Quantification 9.1 Bayes Perspective 9.2 Gaussians in \\(\\mathcal R^d\\) 9.3 Diagonalization of \\(\\Sigma\\) and Sampling for \\(N(0, \\Sigma)\\) 9.4 Stochastic Processes and Gaussian Processes", " Chapter 9 Uncertainty Quantification Imagine there is an unknown quantity \\(u\\) that we can’t observe directly. However, we may obtain a collection of observations \\(Y = (y_i, \\cdots, y_p) \\in \\mathcal R^p\\). Then the Forward Problem is \\(\\underset{\\text{unknown}}{u \\in Z} \\to \\underset{\\text{observed}}{Y}\\). In a sense, when we talk about science, we are talking about use \\(Y\\) to learn about \\(u\\). In practice, there are at least two “sources” of uncertainty associated to the picture: \\(u \\to Y\\): \\(u\\) is unknown. Observations are usually contaminated by some sort of noise so that in general \\(Y\\) is not simply a function of the unknown \\(u\\). In statistics, we may model the “likelihood” or “probability” of observing \\(Y\\) given a certain value of \\(u\\). This quantifies our uncertainty of the possible values \\(Y\\) given the unknown \\(u\\) through the probability mass function: \\(P(Y = y, | u)\\) or through a density function: \\(P(Y \\in C | u) = \\int_C p(y | u) dy\\). Example 9.1 Let \\(u \\in (0,1)\\) be the probability that the outcome of a certain biased coin flip is H. Suppose that this coin is flipped \\(p\\) times and we get \\(y_i = \\begin{cases}1 &amp;i\\text{-th is H}\\\\0 &amp; i\\text{-th is T}\\end{cases}\\). In this case, the likelihood is \\[ P(Y = y | u) = u^{\\sum_{i=1}^p y_i} (1-u)^{p - \\sum_{i=1}^p y_i}, ~y_i \\in \\{0,1\\} \\] Example 9.2 Regression: \\(u: \\mathcal X \\to \\mathcal R\\) (Hidden regression function). \\(y_i = u(x_i) + \\varepsilon_i\\), \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). \\[ p(y|u) = \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^p} exp(-\\frac{||y - (u(x_1), \\cdots, u(x_n)||^2}{2\\sigma^2}) \\] Classification with logistic model: Given \\(u: \\mathcal X \\to \\mathcal R\\), define: \\(q_i = \\frac{exp(u(x_i))}{1+exp(u(x_i))}\\) for \\(x_1, \\cdots, x_p\\). Then, \\(y_i = \\begin{cases}1 &amp;\\text{with prob } q_i\\\\0 &amp; \\text{with prob } 1-q_i\\end{cases}\\). Thus, \\[ P(y | u) = \\prod_{i=1}^p q_i^{y_i} (1- q_i)^{1-y_i} = \\prod_{i=1}^p \\frac{exp(y_iu(x_i))}{1+exp(u(x_i))} \\] Example 9.3 (Density Estimation) Let \\(\\mathcal X = \\mathcal R^d\\) and suppose that \\(u: \\mathcal X \\to \\mathcal R\\) is an unknown density function (\\(u(x) \\geq 0\\), \\(\\forall x\\), and \\(\\int_{\\mathcal R^d} u(x) dx = 1\\)), \\(y_1, \\cdots, y_p \\underset{i.i.d}{\\sim} u\\). Thus, the likelihood of \\(Y=(y_1, \\cdots, y_p)\\) is: \\[ p(y|u) = u(y_1) \\cdots u(y_p) \\] So far we have talked about the distribution of \\(Y\\) given \\(u\\) (Forward Problem). This models the uncertainty of our “measurement” if we had access to \\(u\\). However, how do we learn \\(u\\) from \\(Y\\) (Inverse Problem)? Moreover, how do we quantify the uncertainty of the unknown \\(u\\) before observing \\(Y\\), and after observing \\(Y\\)? 9.1 Bayes Perspective Bayes Rule: \\(p(u | y) \\propto p(y | u) \\pi(u)\\). For the moment, we consider the case where \\(u \\in \\mathcal R^d\\). Suppose \\(\\pi(u)\\) is then interpreted as a density in \\(\\mathcal R^d\\). The posterior is usually also a density in \\(\\mathcal R^d\\). Example 9.4 (Example 9.1 Continued) Let \\(u \\in (0,1)\\), the likelihood is \\(Bernoulli(u):\\) \\[ P(Y = y | u) = u^{\\sum_{i=1}^p y_i} (1-u)^{p - \\sum_{i=1}^p y_i}, ~y_i \\in \\{0,1\\}, \\] the prior is \\(Beta(\\alpha, \\beta)\\): \\[ \\pi(u) = \\frac{u^{\\alpha-1}(1-u)^{\\beta-1}}{\\Gamma(\\alpha, \\beta)}, ~\\alpha, \\beta &gt;0. \\] Then the posterior is \\[ p(u | y) \\propto p(y |u) \\pi(u) \\propto u^{\\sum_{i=1}^p y_i + \\alpha - 1} (1-u)^{p - \\sum_{i=1}^p y_i + \\beta - 1}, \\] that is, given \\(Y=y\\), \\(u\\) is \\(Beta(\\sum_{i=1}^p y_i + \\alpha, p - \\sum_{i=1}^p y_i + \\beta)\\) Example 9.5 (Example 9.2 Continued) Let \\(u \\in \\mathcal R^d\\) and \\(x_1, \\cdots, x_p \\in \\mathcal R^d\\), the likelihood is: \\[ p(y|u) = \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^p} exp(-\\frac{||y - (u(x_1), \\cdots, u(x_n)||^2}{2\\sigma^2}), \\] the prior is \\[ \\pi(u) = \\frac{1}{(\\sqrt{2 \\pi \\lambda^{-1}})^p} exp(-\\frac{\\lambda}{2} ||u||^2_{\\mathcal R^d}), ~i.e. ~ u \\sim N(\\vec 0, \\frac{1}{\\lambda} I). \\] Let \\(X = (x_1, \\cdots, x_p)^T_{p \\times d}\\). Then the posterior is \\[ \\begin{aligned} p(u | y) &amp;\\propto p(y |u) \\pi(u) \\\\ &amp;\\propto exp \\Big(&lt;\\frac{X^Ty}{\\sigma^2}, u&gt; - \\frac{1}{2} &lt;(\\lambda I + \\frac{X^TX}{\\sigma^2})u, u&gt;\\Big)\\\\ &amp;= N(\\mu, \\Sigma), \\end{aligned} \\] where \\(\\mu = \\frac{1}{\\sigma^2} (\\lambda I + \\frac{1}{\\sigma^2} (X^TX))^{-1} X^Ty\\) and \\(\\Sigma= (\\lambda I + \\frac{1}{\\sigma^2} (X^TX))^{-1}\\). With the posterior, we can then compute the following: Posterior Mean: \\[ \\begin{aligned} \\hat u_{PM} = E[u_i | y] &amp;= \\frac{\\int_{\\mathcal R^d} u_i p(y | u) \\pi(u) du_1 \\cdots du_d}{\\int_{\\mathcal R^d} p(y | u) \\pi(u) du_1 \\cdots du_d}\\\\ &amp;= \\frac{\\int_{\\mathcal R^d} u_i p(y | u) \\pi(u) du_1 \\cdots du_d}{Z(y)} \\end{aligned} \\] where \\(Z(y)\\) is the normalization constant. Then, based on the observations (and model), we estimate the i-th coordinate of the unknown as \\(E[u_i | y]\\). We could also estimate \\(u_i^2 cos(u_j)\\) with: \\(E[u_i^2 cos(u_j) | y] = \\frac{\\int_{\\mathcal R^d} u_i^2cos(u_j) p(y | u) \\pi(u) du_1 \\cdots du_d}{Z(y)}\\) Posterior Covariance: \\(Cov(u_i, u_j | y)\\) or \\(Var(u_i|y) = E[u_i^2 | y] - (E[u|y])^2\\). A measure of how certain we are about the estimation of \\(u_i\\). Map (Maximum a posterior): \\[ \\begin{aligned} u^* &amp;= \\underset{u}{\\operatorname{arg max}} \\pi(u) p(y | u)\\\\ &amp;= \\underset{u}{\\operatorname{arg max}} log(\\pi(u)) + log(p(y | u))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} -log(\\pi(u)) - log(p(y | u)) \\end{aligned} \\] Both 1 and 2 are based on being able to take expectation w.r.t. posterior. 3 on the other hand, is not related to expectation but rather to optimization. Example 9.6 (Example 9.5 Continued) We have: \\(E[u_i |y] = \\mu_i\\) \\(Cov(u_iu_j | y) = \\Sigma_{ij}\\) The Map is determined by: \\[ \\begin{aligned} u^* &amp;= \\underset{u}{\\operatorname{arg min}} -log(p(u | y))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} -log(\\pi(u)) - log(p(y | u))\\\\ &amp;= \\underset{u}{\\operatorname{arg min}} \\frac{\\lambda}{2} ||u||^2 + \\frac{1}{\\sigma^2} \\sum_{j=1}^p (y_i - &lt;u, x_i&gt;)^2, \\end{aligned} \\] which is just the ridge regression. Remark. \\[ u^* = \\mu \\] This can be seen by direct optimization or by noticing that the Map of a Gaussian random vector coincides with its mean vector. The posterior can be interpreted as a different kind of regularization: \\[ p(u | y) \\propto \\underset{\\text{Fidality}}{p(y | u)} ~\\underset{\\text{Regularizer}}{\\pi(u)} \\] Back to Example 9.6, when \\(u | y\\) is \\(N(\\mu, \\Sigma)\\), we have formulas for \\(E[u_i | y]\\) and \\(Cov(u_i, u_j)\\). However, something like \\(E[u_1cos(u_2) exp(u_3) | y]\\) would be quite impossible to compute. Nevertheless, we can use Monte-Carlo to approximate this. The idea is that we know what the posterior is. Thus, we can try to sample \\(u^1, \\cdots, u^K \\sim N_d(\\mu, \\Sigma)\\). Then consider the empirical average: \\(\\frac{1}{K} \\sum{j=1}^K u_1^j cos(u_2^j) exp(u_3^j)\\). That is, if we can sample from the posterior, we can numerically approximate \\(E[G(u) | y] \\approx \\frac{1}{K} \\sum_{j=1}^K G(u^j)\\), where \\(u^1, \\cdots, u^K \\underset{i.i.d}{\\sim} Posterior\\). If it is difficult to sample from the posterior, we can use MCMC (Monte Carlo Markov Chain). 9.2 Gaussians in \\(\\mathcal R^d\\) Definition 9.1 A Gaussian random vector \\(Z\\) with mean zero and covariance \\(\\Sigma \\in \\mathcal R_{d\\times d}\\) (positive definite matrix) is a random vector with density (in \\(\\mathcal R^d\\)): \\[ p(z) = \\frac{1}{\\sqrt{(2 \\pi)^d ~det(\\Sigma)}} \\cdot exp(-\\frac{1}{2} &lt; \\Sigma^{-1} z, z&gt;), \\] and we write \\(Z \\sim N(0, \\Sigma)\\). We can characterize this distribution via its characteristic function: \\(\\phi_Z(v):= E[e^{\\textbf{i} &lt;Z, v&gt;}]_{V \\in \\mathcal R^d} = e^{-\\frac{1}{2} &lt;\\Sigma v, v&gt;}\\). Notice that we can see \\(Z\\) as a collection of real valued random variables: \\(Z = (Z_1, \\cdots, Z_d)\\), namely the coordinates of \\(Z\\). For convenience let’s write \\(Z\\) in the following form: \\(\\{Z_x\\}_{x \\in \\mathcal X}\\), where \\(\\mathcal X\\) is the set \\(\\mathcal X = \\{1, \\cdots, d\\}\\). Proposition 9.1 Let \\(\\mathcal X = \\{1, \\cdots, d\\}\\), \\(\\{Z_x\\}_{x \\in \\mathcal X}\\) a collection of real valued random variables is a Gaussian iff \\(\\forall\\) (finite) subset \\(\\mathcal X&#39; \\subseteq \\mathcal X\\), the collection \\(\\{Z_x\\}_{x \\in \\mathcal X&#39;}\\) is a Gaussian. Proof. “\\(\\Leftarrow\\)”: It is obvious since we can pick \\(\\mathcal X&#39; = \\mathcal X\\). “\\(\\Rightarrow\\)”: For simplicity, suppose \\(\\mathcal X&#39; = \\{1, \\cdots, m\\}\\), \\(m \\leq d\\). Let \\(\\tilde Z = \\{Z_x\\}_{x \\in \\mathcal X&#39;}\\), \\(w \\in \\mathcal R^m\\). Then we have: \\[ \\begin{aligned} E[e^{\\textbf{i} &lt;\\tilde Z, w&gt;_{\\mathcal R^m}}] &amp;= E[e^{\\textbf{i} &lt;\\tilde Z,uw&gt;_{\\mathcal R^m}}]\\\\ &amp;(u = (w_1, \\cdots, w_m, 0, \\cdots, 0))\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\Sigma u, u &gt;_{\\mathcal R^d}}\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\tilde \\Sigma w, w &gt;_{\\mathcal R^m}}, \\end{aligned} \\] where \\(\\tilde \\Sigma \\in \\mathcal R^{m \\times m}\\) and \\(\\tilde \\Sigma_{ij} = \\Sigma_{ij}\\), \\(i,j = 1,\\cdots,m\\). Since the characteristic function of \\(\\tilde Z\\) is that of a Gaussian in \\(\\mathcal R^m\\), we have \\(\\tilde Z \\sim N_m(0, \\tilde \\Sigma)\\). Proposition 9.2 Let \\(Z\\) be a Gaussian random vector in \\(\\mathcal R^d: Z \\sim N_d(0, \\Sigma)\\). If \\(A \\in \\mathcal R^{m \\times d}\\) is a matrix, then \\(U = AZ\\) is a Gaussian in \\(\\mathcal R^m\\) and \\(U \\sim N_m(0, A\\Sigma A^T)\\). If \\(Z \\sim N_d(0, \\Sigma)\\), \\(\\tilde Z \\sim N_m(0, \\tilde \\Sigma)\\), and \\(Z, \\tilde Z\\) are independent, then \\(Z, \\tilde Z \\sim N_{d+m}(0, \\left(\\begin{matrix}\\Sigma&amp;0\\\\0&amp;\\tilde \\Sigma\\end{matrix}\\right))\\). \\(Z\\) is a Gaussian in \\(\\mathcal R^d\\) iff every linear combination of its coordinates is univariate (单元的) Gaussian: \\[ Z = (Z_1, \\cdots, Z_d) \\sim N_d \\Leftrightarrow \\forall a_1, \\cdots, a_d \\in \\mathcal R, ~\\sum_{j=1}^d a_j Z_j \\sim N_1. \\] Suppose that \\(Z \\sim N_d(0, \\Sigma)\\), and let \\(v, \\tilde v \\in \\mathcal R^d\\) be two vectors, then \\[ Cov(&lt;Z, v&gt;, &lt;Z, \\tilde v&gt;) = &lt;\\Sigma v, \\tilde v&gt; = &lt;\\Sigma \\tilde v, v&gt; \\] Suppose \\(Z = (Z_1, \\cdots, Z_d)\\) is Gaussian with cov. matrix \\(\\Sigma\\), then the coordinates of \\(Z\\) are independent random variables iff \\(\\Sigma\\) is a diagonal matrix. Proof. \\[ \\begin{aligned} E[e^{\\textbf{i}&lt;w,U&gt;_{\\mathcal R^m}}] &amp;= E[e^{\\textbf{i}&lt;w,AZ&gt;_{\\mathcal R^m}}]\\\\ &amp;\\underset{&lt;A,B&gt; = tr(A^TB)}{=} E[e^{\\textbf{i}&lt;A^Tw,Z&gt;_{\\mathcal R^m}}]\\\\ &amp;= e^{-\\frac{1}{2} &lt; \\Sigma (A^Tw), A^Tw&gt;_{\\mathcal R^d}}\\\\ &amp;= e^{-\\frac{1}{2} &lt; A \\Sigma A^Tw, w&gt;_{\\mathcal R^d}} \\end{aligned} \\] “\\(\\Rightarrow\\)”: Let \\(U = \\sum_{j=1}^d a_j Z_j\\) be a linear combination of the coordinates of \\(Z\\). By 1, we have \\(U=AZ\\), where \\(A = (a_1, \\cdots, a_d)\\), is a Gaussian. “\\(\\Leftarrow\\)”: Let \\(a \\in \\mathcal R^d\\), with \\(a = (a_1, \\cdots, a_d)\\). Then, \\(E[e^{\\textbf{i}&lt;a,Z&gt;}] = E[e^{\\textbf{i} \\sum_{j=1}^d a_j Z_j}] \\underset{\\sum_{j=1}^d a_j Z_j \\sim N(0, \\sigma_a^2)}{=} e^{-\\frac{1}{2} \\sigma_a^2}\\), where \\(\\sigma_a^2 = Var(\\sum_{j=1}^d a_j Z_j) = &lt;\\Sigma a, a&gt;_{\\mathcal R^d}\\), \\(\\Sigma\\) is the covariance matrix of \\(Z = (Z_1, \\cdots, Z_d)\\). Thus, \\(E[e^{\\textbf{i}&lt;a,Z&gt;}] = e^{-\\frac{1}{2} &lt;\\Sigma a, a&gt;_{\\mathcal R^d}}\\), \\(\\forall a \\in \\mathcal R^d\\), which means that \\(Z \\sim N_d\\). \\[ \\begin{aligned} Cov(&lt;Z, v&gt;, &lt;Z, \\tilde v&gt;) &amp;= Cov(\\sum_{i=1}^n Z_i v_i, \\sum_{j=1}^n Z_j, \\tilde v_j)\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n v_i \\tilde v_j Cov(Z_i, Z_j)\\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1}^n (\\Sigma_{ij}v_i \\tilde v_j)\\\\ &amp;= &lt;\\Sigma v, \\tilde v&gt; = &lt;\\Sigma \\tilde v, v&gt;. \\end{aligned} \\] 9.3 Diagonalization of \\(\\Sigma\\) and Sampling for \\(N(0, \\Sigma)\\) Theorem 9.1 If \\(\\Sigma \\in \\mathcal R^{d \\times d}\\) is symmetric positive semi definite, then there are vectors \\(v_1, \\cdots, v_d \\in \\mathcal R^d\\) and non-negative \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_d \\geq 0\\) s.t \\(\\Sigma v_i = \\lambda_i vi_i\\) and \\(&lt;v_i, v_j&gt;_{\\mathcal R^d} = \\begin{cases}1 &amp; i=j\\\\0 &amp; i \\neq j\\end{cases}\\). In short: \\(\\Sigma\\) induces an orthonormal basis for \\(\\mathcal R^d\\) formed of eigen vectors of \\(\\Sigma\\). Moreover, all of \\(\\Sigma\\)’s eigen values are non-negative. How to sample from \\(Z \\sim N_d(0, \\Sigma)\\): Since \\(Z \\in \\mathcal R^d\\), we notice that \\(Z = \\sum_{i=1}^d &lt;Z, v_i&gt;_{\\mathcal R^d} v_i\\), where \\(\\{v_i\\}\\) is an orthonormal basis for \\(\\mathcal R^d\\) and are fixed. We then have \\(&lt;Z, v_i&gt; \\sim N_1(0, &lt;\\Sigma v_i, v_i&gt;) = N_1(0, \\lambda_i &lt;v_i, v_i&gt;) = N_1(0, \\lambda_i)\\). Moreover, for \\(i \\neq j\\): \\[ \\begin{aligned} Cov(&lt;Z, v_i&gt;, &lt;Z, v_j&gt;) &amp;= &lt;\\Sigma v_i, v_j&gt;\\\\ &amp;= &lt;\\lambda_i v_i, v_j&gt;\\\\ &amp;= \\lambda &lt;v_i, v_j&gt;\\\\ &amp;= 0 \\end{aligned} \\] which means that the random variables \\(&lt;Z, v_1&gt;,\\cdots, &lt;Z,v_d&gt;\\) are independent. So, we can write: \\(Z = \\sum_{i=1}^d \\sqrt{\\lambda_i}(\\frac{&lt;Z, v_i&gt;}{\\sqrt{\\lambda_i}}) v_i\\ = \\sum_{i=1}^d \\sqrt{\\lambda_i}\\xi_i v_i\\), where \\(\\xi_1, \\cdots, \\xi_d\\) are i.i.d \\(N(0,1)\\) random variables. Thus, to sample from \\(N_d(0, \\Sigma)\\), we can do the following: Find the eigen pairs \\(\\{(\\lambda_i, v_i)\\}_{i=1,\\cdots,d}\\) for \\(\\Sigma\\). Sample \\(\\xi_1, \\cdots, \\xi_d \\sim N(0,1)\\). Set \\(Z = \\sum_{i=1}^d \\sqrt{\\lambda_i}\\xi_i v_i\\). Remark. The above is preciselt what we need to sample form Gaussian in more general settings. In particular, for Gaussians in certain RKHSs. 9.4 Stochastic Processes and Gaussian Processes Definition 9.2 (Stochastic Processes) Let \\(\\mathcal X\\) be a set. A stochastic process over \\(\\mathcal X\\) is a collection of random variables \\(\\{Z_x\\}_{x \\in \\mathcal X}\\). Definition 9.3 (Gaussian Processes) A Gaussian Process over \\(\\mathcal X\\) is a stochastic process \\(\\{Z_x\\}_{x \\in \\mathcal X}\\) with the property that \\(\\forall\\) finite subset \\(\\mathcal X&#39;\\) of \\(\\mathcal X\\), the vector \\(\\{Z_x\\}_{x \\in \\mathcal X&#39;}\\) is a Gaussian. Definition 9.4 (Covariance Funciton) Suppose \\(Z = \\{Z_x\\}_{x \\in \\mathcal X}\\) is a Gaussian Process over \\(\\mathcal X\\). We define \\(Z\\)’s covariance function as: \\[ K: \\mathcal X \\times \\mathcal X \\to \\mathcal R\\\\ K(x, \\tilde x):= Cov(Z_x, Z_{\\tilde x}) \\] We will write \\(Z \\sim N(0, K)\\) for simplicity. Proposition 9.3 The covariance function of a Gaussian process over \\(\\mathcal X\\) is a kernel over \\(\\mathcal X\\): \\(x_1, \\cdots, x_n \\in \\mathcal X\\), \\(\\{Z_{x_1}, \\cdots, Z_{x_n}\\}\\) is a n-dimensional Gaussian. Thus \\(\\Sigma\\) is positive semi definite and \\(\\Sigma_{ij} = K(x_i, x_j)\\). Let \\(K: \\mathcal R^d \\times \\mathcal R^d \\to \\mathcal R\\) be a continuous kernel, for which \\(|K(x, \\tilde x)| \\leq M\\), where \\(M\\) is a constant. Let \\(p: \\mathcal R^d \\to \\mathcal R\\) be a density function. Consider the following operator: \\[ T_K: f \\in \\mathcal L^2(p) \\to \\int_{\\mathcal R^d} K(\\cdot, x) f(x) dx, \\] where \\(\\mathcal L^2(p) = \\{f: \\int_{\\mathcal R^d} f^2 p(x)dx &lt; \\infty\\}\\). In particular, for \\(f \\in \\mathcal L^2(p)\\), \\(T_K f\\) is a function on \\(\\mathcal R^d\\), defined by \\(T_K f(\\tilde x):= \\int_{\\mathcal R^d} K(\\tilde x, x) f(x) p(x) dx\\), \\(\\tilde x \\in \\mathcal R^d\\). Definition 9.5 (Eigen Values and Eigen Functions) \\(\\lambda \\in \\mathcal R\\) is said to be an eigen value of \\(T_K\\) if \\(\\exists f \\in \\mathcal L^2(p)\\), s.t \\(T_Kf = \\lambda f\\). In other words, if \\(\\lambda f(\\cdot) = \\int_{\\mathcal R^d} K(\\cdot, \\tilde x) f(\\tilde x) p(\\tilde x) d \\tilde x\\), we call \\((\\lambda, f)\\) and eigen pair. Theorem 9.2 (Mercer's Theorem) There exists a sequence of eigen pairs \\(\\{(\\lambda_k, \\phi_k)\\}_{k \\in \\mathcal N}\\) of \\(T_K\\) s.t \\(&lt;\\phi_k, \\phi_l)&gt;_{\\mathcal L^2(p)} = \\int_{\\mathcal R^d} \\phi_k(x) \\phi_l(x) p(x) dx = \\begin{cases}1 &amp;k=l\\\\0&amp;o.w.\\end{cases}\\). \\(\\mathcal L^2(p) = \\{\\sum_{i=1}^\\infty a_i \\phi_i: \\sum_{i=1}^\\infty a_i^2 &lt; \\infty\\}\\) and \\(f \\in \\mathcal L^2(p)\\) can be represented as \\(\\sum_{i=1}^\\infty &lt;f, \\phi_i&gt;_{\\mathcal L^2(p)} \\phi_i\\) and the inner product can be written as \\(&lt;f,g&gt;_{\\mathcal L^2(p)} = \\sum_{i=1}^\\infty &lt;f, \\phi_i&gt;_{\\mathcal L^2(p)} &lt; g, \\phi_i&gt;_{\\mathcal L^2(p)}\\). The numbers \\(\\lambda_1, \\lambda_2, \\cdots\\) satisfy \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq 0\\) and \\(\\lambda_n \\to 0\\) as \\(n \\to \\infty\\). The kernel \\(K\\) can be written as \\(K(x, \\tilde x) = \\sum_{i=1}^\\infty \\lambda_i \\phi_i(x) \\phi_i(\\tilde x)\\). Proof. First, note that \\(K(\\cdot, x) \\in \\mathcal L^2(p)\\): \\[ \\begin{aligned} \\int_{\\mathcal R^d} (K(\\tilde x, x))^2 p(\\tilde x) d\\tilde x &amp;\\leq \\int_{\\mathcal R^d} [\\sqrt{K(\\tilde x, \\tilde x)} \\sqrt{K(x,x)}]^2 p(\\tilde x) d\\tilde x\\\\ &amp;= K(x,x) \\int_{\\mathcal R^d} K(\\tilde x, \\tilde x) p(\\tilde x) d\\tilde x\\\\ &amp;&lt; \\infty \\end{aligned} \\] In particular, \\(K(\\cdot, x) = \\sum_{i=1}^\\infty &lt;K(\\cdot,x), \\phi_i&gt;_{\\mathcal L^2(p)} \\phi_i(\\cdot)\\). Thus, \\(K(\\tilde x, x) = \\sum_{i=1}^\\infty &lt;K(\\cdot,x), \\phi_i&gt;_{\\mathcal L^2(p)} \\phi_i(\\tilde x)\\). We just need to show that \\(&lt;K(\\cdot,x), \\phi_i&gt;_{\\mathcal L^2(p)} = \\lambda_i \\phi_i(x)\\): \\[ \\begin{aligned} &lt;K(\\cdot,x), \\phi_i&gt;_{\\mathcal L^2(p)} &amp;= \\int_{\\mathcal R^d} K(\\tilde x, x) \\phi_i(\\tilde x) p(\\tilde x) d \\tilde x\\\\ &amp;= T_K \\phi_i(x)\\\\ &amp;= \\lambda_i \\phi_i(x) \\end{aligned} \\] How to define a Gaussian Process over \\(\\mathcal X\\) with covariance function \\(K\\)? How to do sampling? Assuming \\(K\\) satisfies the condition for Mercer’s Theorem for some density \\(p\\), we consider the eigen pairs \\(\\{(\\lambda_i, \\phi_i)\\}\\) and do the following: Let \\(\\xi_1, \\cdots\\) be i.i.d \\(N(0,1)\\) random variables. We define a random function: \\(Z = \\sum_{i=1}^\\infty \\sqrt{\\lambda_i} \\xi_i \\phi_i\\). In particular, \\(Z(x) = \\sum_{i=1}^\\infty \\sqrt{\\lambda_i} \\xi_i \\phi_i(x_i)\\). It follows that, \\[ \\begin{aligned} Cov(Z(x), Z(\\tilde x)) &amp;= E[\\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\sqrt{\\lambda_i} \\sqrt{\\lambda_j} \\xi_1 \\xi_j \\phi_i(\\tilde x)\\phi_i(x)]\\\\ &amp;= \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\sqrt{\\lambda_i} \\sqrt{\\lambda_j} \\phi_i(\\tilde x)\\phi_i(x)E[\\xi_1 \\xi_j]\\\\ &amp;\\underset{E[\\xi_i \\xi_j] = 1_{i=j}}{=} \\sum_{i=1}^\\infty\\lambda_i \\phi_i(x) \\phi_i(\\tilde x)\\\\ &amp;= K(x, \\tilde x) \\end{aligned} \\] Theorem 9.3 (Karhunen-Loewe Expansion) The representation: \\(Z = \\sum_{i=1}^\\infty \\sqrt{\\lambda_i} \\xi_i \\phi_i\\) for \\(Z \\sim N(0, K)\\) is known as Karhunen-Loewe Expansion. Example 9.7 Let \\(\\mathcal X = [0,1]\\), \\(K(s,t) = min\\{s,t\\} - st\\) is a kernel. Consider \\(p(t) = 1\\), \\(t \\in [0,1]\\). Consider the operator: \\(T_K: f \\in \\mathcal L^2(p) \\to T_Kf \\in \\mathcal L^2(p)\\), where \\(T_K f(\\cdot) = \\int_0^1 K(\\cdot, s) f(s) ds\\). Let us try to find the eigen pairs of \\(T_K\\). That is, \\[ \\begin{aligned} \\lambda \\phi(t) &amp;= \\int_0^1 K(t,s) \\phi(s) ds\\\\ &amp;= \\int_0^1 (min\\{s,t\\} - st) \\phi(s) ds\\\\ &amp;= \\int_0^1 min\\{s,t\\} \\phi(s) ds - t\\int_0^1 s \\phi(s) ds\\\\ &amp;= \\int_0^t s\\phi(s) ds + t\\int_t^1 \\phi(s) ds - t\\int_0^1 s \\phi(s) ds\\\\ &amp;:= \\int_0^t s\\phi(s) ds + t\\int_t^1 \\phi(s) ds - tC \\end{aligned} \\] Take derivative w.r.t. \\(t\\) to get \\[ \\begin{aligned} \\lambda \\phi&#39;(t) &amp;= t\\phi(t) + [\\int_t^1 \\phi(s) ds - t\\phi(t)] - C\\\\ &amp; = \\int_t^1 \\phi(s) ds - C \\end{aligned} \\] Take derivative w.t.t. \\(t\\) again to derive: \\[ \\lambda \\phi&#39;&#39;(t) = - \\phi(t) \\] For reasons that will soon become apparent (\\(\\phi \\in \\mathcal H\\)), we must have: \\[ \\phi(0) = 0 = \\phi(1) \\] Thus, we have the equation: \\[ \\begin{cases} \\lambda \\phi&#39;&#39;(t) + \\phi(t) = 0, ~\\forall t \\in (0,1)\\\\ \\phi(0) = 0\\\\ \\phi(1) = 0 \\end{cases} \\] Thus, \\(\\phi\\) has to take the form: \\[\\phi(t) = A cos(\\frac{t}{\\sqrt{\\lambda}}) + B sin(\\frac{t}{\\sqrt \\lambda})\\] However, we need to make sure \\(\\phi(0) = 0 = \\phi(1)\\), which means that \\(0 = \\phi(0) = A\\), and \\(0 = \\phi(1) = B sin(\\frac{1}{\\sqrt \\lambda})\\), which forces \\(\\frac{1}{\\sqrt \\lambda}\\) to be a multiple of \\(\\pi\\). In other words: \\(\\frac{1}{\\sqrt \\lambda} = k\\pi\\), i.e. \\(\\lambda = \\frac{1}{k^2 \\pi^2}\\), \\(k \\in \\mathcal N\\). Thus, we have \\[ \\begin{cases} \\lambda_k = \\frac{1}{k^2 \\pi^2}\\\\ \\phi_k = C_k sin (k\\pi t) \\end{cases} \\] where \\(C_k\\) satisfies \\(\\int_0^1 \\phi^2(x) dx = 1\\). Remark. Let’s go back to the setting of Mercer’s Theorem: \\(K\\) a continuous, bounded kernel. \\(p\\) a density. \\(\\{(\\lambda_k, \\phi_k)\\}_{k \\in \\mathcal N}\\) are eigen pairs of \\(T_K\\). \\(\\mathcal L^2(p)\\) can be alternatively written as: \\(\\mathcal L^2(p) = \\{\\sum_{i=1}^\\infty : \\sum_{i=1}^\\infty a_i^2 &lt; \\infty\\}\\). The RKHS \\(\\mathcal H\\) of \\(K\\) is defined as \\[ \\mathcal H := \\{\\sum_{i=1}^\\infty b_i K(\\cdot, x_i): \\{x_i\\} \\subseteq \\mathcal X, \\{b_i\\} \\subseteq \\mathcal R, \\text{ s.t } \\sum_{j=1}^\\infty \\sum_{i=1}^\\infty K(x_i, x_j) b_i b_j &lt; \\infty\\} \\] As \\[ \\begin{aligned} \\sum_{i=1}^\\infty b_i K(\\cdot, x_i) &amp;= \\sum_{i=1}^\\infty b_i \\sum_{k=1}^\\infty \\lambda_k \\phi_k(\\cdot) \\phi_k(x_i)\\\\ &amp;= \\sum_{k=1}^\\infty \\lambda_k (\\sum_{i=1}^\\infty b_i\\phi_k(x_i)) \\phi_k(\\cdot)\\\\ &amp;= \\sum_{k=1}^\\infty &lt;\\sum_{i=1}^\\infty b_i K(\\cdot, x_i), \\phi_k&gt;_{\\mathcal L^2(p)} \\phi_k(\\cdot)\\\\ &amp;:= \\sum_{k=1}^\\infty a_k \\phi_k(\\cdot), \\end{aligned} \\] we can write it as: \\[\\mathcal H := \\{\\sum_{k=1}^\\infty a_k \\phi_k(\\cdot): \\{x_i\\} \\subseteq \\mathcal X, \\{b_i\\} \\subseteq \\mathcal R, \\text{ s.t } \\sum_{i=1}^\\infty \\frac{a_i^2}{\\lambda_i} &lt; \\infty.\\] In particular, \\(\\mathcal H \\subseteq \\mathcal L^2(p)\\) \\(\\phi_i \\in \\mathcal H\\), \\(\\forall i\\). \\(Z = \\sum_{i=1}^\\infty \\sqrt{\\lambda_i} \\xi_i \\phi_i \\notin \\mathcal H\\), although it does belong to \\(\\mathcal L^2(p)\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
