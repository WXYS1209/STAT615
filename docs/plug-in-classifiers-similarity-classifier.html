<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Plug-in Classifiers (Similarity Classifier) | STAT 615 Note Book</title>
  <meta name="description" content="This is a notebook for STAT 615." />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Plug-in Classifiers (Similarity Classifier) | STAT 615 Note Book" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a notebook for STAT 615." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Plug-in Classifiers (Similarity Classifier) | STAT 615 Note Book" />
  
  <meta name="twitter:description" content="This is a notebook for STAT 615." />
  

<meta name="author" content="Peter Wang" />


<meta name="date" content="2023-05-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-problem-bayes-classifier.html"/>
<link rel="next" href="empirical-risk-minimization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT615 Notebook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.1</b> Usage</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>1.2</b> Render book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>1.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html"><i class="fa fa-check"></i><b>2</b> Classification Problem &amp; Bayes Classifier</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation</a></li>
<li class="chapter" data-level="2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#classification-problem"><i class="fa fa-check"></i><b>2.2</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#goal"><i class="fa fa-check"></i><b>2.2.1</b> Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#questions"><i class="fa fa-check"></i><b>2.2.2</b> Questions</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#the-0-1-loss"><i class="fa fa-check"></i><b>2.2.3</b> The 0-1 Loss</a></li>
<li class="chapter" data-level="2.2.4" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#observations"><i class="fa fa-check"></i><b>2.2.4</b> Observations</a></li>
<li class="chapter" data-level="2.2.5" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#examples"><i class="fa fa-check"></i><b>2.2.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-problem-bayes-classifier.html"><a href="classification-problem-bayes-classifier.html#other-topics"><i class="fa fa-check"></i><b>2.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html"><i class="fa fa-check"></i><b>3</b> Plug-in Classifiers (Similarity Classifier)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier"><i class="fa fa-check"></i><b>3.1</b> Notions of Consistency for Families of Binary Classifier</a></li>
<li class="chapter" data-level="3.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hat-eta_n"><i class="fa fa-check"></i><b>3.2</b> Functions of <span class="math inline">\(\hat \eta_n\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#consistency"><i class="fa fa-check"></i><b>3.3</b> Consistency</a></li>
<li class="chapter" data-level="3.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency"><i class="fa fa-check"></i><b>3.4</b> Strong Consistency</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory"><i class="fa fa-check"></i><b>3.4.1</b> Theorems from probability theory:</a></li>
<li class="chapter" data-level="3.4.2" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities"><i class="fa fa-check"></i><b>3.4.2</b> Concentration Inequalities</a></li>
<li class="chapter" data-level="3.4.3" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq"><i class="fa fa-check"></i><b>3.4.3</b> More about Con-Ineq</a></li>
<li class="chapter" data-level="3.4.4" data-path="plug-in-classifiers-similarity-classifier.html"><a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1"><i class="fa fa-check"></i><b>3.4.4</b> Strong Consistency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html"><i class="fa fa-check"></i><b>4</b> Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#questions-1"><i class="fa fa-check"></i><b>4.1</b> Questions</a></li>
<li class="chapter" data-level="4.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#concentration-bounds-for-undersetmathfrak-f-in-mathcal-foperatornamesup-r_nmathfrak-f---rmathfrak-f"><i class="fa fa-check"></i><b>4.2</b> Concentration Bounds for <span class="math inline">\(\underset{\mathfrak f \in \mathcal F}{\operatorname{sup}} | R_n(\mathfrak f) - R(\mathfrak f) |\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-shattering-number"><i class="fa fa-check"></i><b>4.2.1</b> In terms of Shattering Number</a></li>
<li class="chapter" data-level="4.2.2" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-vc-dimension"><i class="fa fa-check"></i><b>4.2.2</b> In terms of VC Dimension</a></li>
<li class="chapter" data-level="4.2.3" data-path="empirical-risk-minimization.html"><a href="empirical-risk-minimization.html#in-terms-of-rademacher-complexity"><i class="fa fa-check"></i><b>4.2.3</b> In terms of Rademacher Complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a>
<ul>
<li class="chapter" data-level="5.1" data-path="summary.html"><a href="summary.html#classification-problem-1"><i class="fa fa-check"></i><b>5.1</b> Classification Problem</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="summary.html"><a href="summary.html#similarity-classifiers"><i class="fa fa-check"></i><b>5.1.1</b> Similarity Classifiers</a></li>
<li class="chapter" data-level="5.1.2" data-path="summary.html"><a href="summary.html#emprirical-risk-minimization"><i class="fa fa-check"></i><b>5.1.2</b> Emprirical Risk Minimization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary.html"><a href="summary.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>5.2</b> No Free Lunch Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-classifiers.html"><a href="linear-classifiers.html"><i class="fa fa-check"></i><b>6</b> Linear Classifiers</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#lda-or-qda"><i class="fa fa-check"></i><b>6.1</b> LDA or QDA</a></li>
<li class="chapter" data-level="6.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons-and-svms"><i class="fa fa-check"></i><b>6.3</b> Perceptrons and SVMs</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-classifiers.html"><a href="linear-classifiers.html#perceptrons"><i class="fa fa-check"></i><b>6.3.1</b> Perceptrons</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-classifiers.html"><a href="linear-classifiers.html#svms"><i class="fa fa-check"></i><b>6.3.2</b> SVMs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#hard-margin-svm"><i class="fa fa-check"></i><b>7.1</b> Hard Margin SVM</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#soft-margin-svm"><i class="fa fa-check"></i><b>7.2</b> Soft Margin SVM</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-1"><i class="fa fa-check"></i><b>7.2.1</b> Case 1</a></li>
<li class="chapter" data-level="7.2.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-2"><i class="fa fa-check"></i><b>7.2.2</b> Case 2</a></li>
<li class="chapter" data-level="7.2.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#case-3"><i class="fa fa-check"></i><b>7.2.3</b> Case 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html"><i class="fa fa-check"></i><b>8</b> Regularized Empirical Risk Minimization</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#ill-posed-problems"><i class="fa fa-check"></i><b>8.1</b> ill Posed Problems</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#erm-with-kernels"><i class="fa fa-check"></i><b>8.2</b> ERM with Kernels</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#comparison"><i class="fa fa-check"></i><b>8.2.1</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-empirical-risk-minimization.html"><a href="regularized-empirical-risk-minimization.html#regularity"><i class="fa fa-check"></i><b>8.3</b> Regularity</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html"><i class="fa fa-check"></i><b>9</b> Uncertainty Quantification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#bayes-perspective"><i class="fa fa-check"></i><b>9.1</b> Bayes Perspective</a></li>
<li class="chapter" data-level="9.2" data-path="uncertainty-quantification.html"><a href="uncertainty-quantification.html#gaussians-in-mathcal-rd"><i class="fa fa-check"></i><b>9.2</b> Gaussians in <span class="math inline">\(\mathcal R^d\)</span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 615 Note Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="plug-in-classifiers-similarity-classifier" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Plug-in Classifiers (Similarity Classifier)<a href="plug-in-classifiers-similarity-classifier.html#plug-in-classifiers-similarity-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>For binary classification with 0-1 loss:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathfrak f_B(x)\)</span> = <span class="math inline">\(\begin{cases}1 &amp; if ~P_{(X,Y) \in \rho}(Y=1 \mid X=x) \geq P_{(X,Y) \in \rho}(Y=0 \mid X=x)\\0 &amp; o.w.\end{cases}\)</span> is <em>Motivate “plug-in” or similarity classifier</em>;</p></li>
<li><p><span class="math inline">\(\mathfrak f_B \in \underset{\mathfrak f: \mathcal X \to \{0,1\}}{\operatorname{argmin}} ~ P(\mathfrak f(X) \neq Y)\)</span> is <em>Motivate ERM (Empirical Risk Minimization)</em>.</p></li>
</ol>
<p>For this chapter, we will talk about the first one.</p>
<p>Define <span class="math inline">\(\eta(x) := P(Y=1 \mid X=x)\)</span>, then we can rewrite <span class="math inline">\(\mathfrak f_B(x)\)</span> as: <span class="math inline">\(\mathfrak f_B(x) = \begin{cases}1 &amp; \eta(x) \geq \frac{1}{2}\\0 &amp; o.w.\end{cases}.\)</span></p>
<p>Idea: Replace <span class="math inline">\(\eta(x)\)</span> with <span class="math inline">\(\hat \eta_n(x)\)</span> that is built from our training data <span class="math inline">\((x_1,y_1), \cdots, (x_n,y_n)\)</span>: <span class="math inline">\(\hat{\mathfrak f}_n(x) = \begin{cases}1 &amp; \hat \eta_n(x) \geq \frac{1}{2}\\0 &amp; o.w.\end{cases}.\)</span></p>
<div class="lemma">
<p><span id="lem:base" class="lemma"><strong>Lemma 3.1  </strong></span>Let <span class="math inline">\(\mathfrak f\)</span> be an arbitrary binary classifier. Then</p>
<p><span class="math display">\[
0 \leq R(\mathfrak f) - R_B^* = E\Big[\mid 2 \eta(X) - 1 \mid \cdot 1_{\mathfrak f(X) \neq \mathfrak f_B(X)}\Big] \leq 2 E\Big[ \mid \eta(X) - \alpha(X) \mid \Big],
\]</span></p>
<p>where we suppose <span class="math inline">\(\mathfrak f(X) = \begin{cases}1 &amp; \alpha(X) \geq \frac{1}{2}\\0 &amp; o.w.\end{cases}.\)</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>As</p>
<p><span class="math display">\[
\begin{aligned}
E[1_{\mathfrak f(X) \neq Y} - 1_{\mathfrak f_B(X) \neq Y} \mid X] &amp;= (1_{\mathfrak f(X) \neq 1} - 1_{\mathfrak f
_B(X) \neq 1}) \cdot P(Y=1 \mid X) + (1_{\mathfrak f(X) \neq 0} - 1_{\mathfrak f
_B(X) \neq 0}) \cdot P(Y=0 \mid X)\\
&amp;= (1_{\mathfrak f(X) \neq 1} - 1_{\mathfrak f
_B(X) \neq 1}) \cdot \eta(X) + (1_{\mathfrak f(X) \neq 0} - 1_{\mathfrak f
_B(X) \neq 0}) \cdot (1-\eta(X))\\
&amp;= \begin{cases}
0 &amp; if ~ \mathfrak f(X) = \mathfrak f_B(X) = 1\\
0 &amp; if ~ \mathfrak f(X) = \mathfrak f_B(X) = 0\\
\mid 1-2\eta(X) \mid &amp; if ~ \mathfrak f(X) =1,~ \mathfrak f_B(X) = 0\\
\mid 2\eta(X)-1 \mid &amp; if ~ \mathfrak f(X) =0,~ \mathfrak f_B(X) = 1\\
\end{cases}\\
&amp;= \mid 2\eta(X)-1 \mid \cdot 1_{\mathfrak f(X) \neq \mathfrak f_B(X)},
\end{aligned}
\]</span></p>
<p>we have:</p>
<p><span class="math display">\[
\begin{aligned}
R(\mathfrak f) - R_B^* &amp;= E\Big[E[1_{\mathfrak f(X) \neq Y} - 1_{\mathfrak f_B(X) \neq Y} \mid X] \Big]\\
&amp;= E\Big[\mid 2 \eta(X) - 1 \mid \cdot 1_{\mathfrak f(X) \neq \mathfrak f_B(X)}\Big].
\end{aligned}
\]</span></p>
<p>Moreover,</p>
<ul>
<li><p>Case1: <span class="math inline">\(\mathfrak f(X) \neq \mathfrak f_B(X)\)</span>, <span class="math inline">\(\eta(X) &lt; \frac{1}{2}\)</span><br />
In this case, <span class="math inline">\(\mathfrak f_B(X) = 0\)</span> and <span class="math inline">\(\mathfrak f(X) = 1\)</span>, which means that <span class="math inline">\(\alpha(X) \geq \frac{1}{2}\)</span>. So, <span class="math inline">\(2 \mid \eta(X) - \frac{1}{2} \mid \leq 2 \mid \eta(X) - \alpha(X) \mid\)</span>.</p></li>
<li><p>Case 2: <span class="math inline">\(\mathfrak f(X) \neq \mathfrak f_B(X)\)</span>, <span class="math inline">\(\eta(X) \geq \frac{1}{2}\)</span><br />
Similarly, we have <span class="math inline">\(2 \mid \frac{1}{2} - \eta(X) \mid \leq 2 \mid \eta(X) - \alpha(X) \mid\)</span>.</p></li>
</ul>
<p>Thus, <span class="math inline">\(E\Big[\mid 2 \eta(X) - 1 \mid \cdot 1_{\mathfrak f(X) \neq \mathfrak f_B(X)}\Big] \leq 2E\Big[\mid \eta(X) -\alpha(X) \mid \cdot 1_{\mathfrak f(X) \neq \mathfrak f_B(X)}\Big].\)</span></p>
</div>
<div class="corollary">
<p><span id="cor:base" class="corollary"><strong>Corollary 3.1  </strong></span>If <span class="math inline">\(\hat{\mathfrak f}_n(X) = \begin{cases}1&amp; if~ \hat \eta_n(X) \geq 1/2 \\0 &amp; o.w.\end{cases}\)</span>, <span class="math inline">\(\hat \eta_n(X) = \hat \eta_n (X; (x_1, y_1), \cdots, (x_n, y_n))\)</span>, then</p>
<p><span class="math display">\[
E[R(\hat{\mathfrak f}_n)] - R^*_B \leq 2 E\Big[\mid \eta(X) - \hat \eta_n(X) \mid\Big].
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span>For <span class="math inline">\(\hat{\mathfrak f}_n\)</span>, <span class="math inline">\(\eta(X)\)</span>, and <span class="math inline">\(\hat \eta_n(X)\)</span>, we have <span class="math inline">\((X, Y) \sim \rho\)</span> and <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n) \sim \rho\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>Given <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n)\)</span>, according to <strong>Lemma <a href="plug-in-classifiers-similarity-classifier.html#lem:base">3.1</a></strong>, we have: <span class="math inline">\(R(\hat{\mathfrak f}_n) - R^*_B \leq 2 E\Big[\mid \eta(X) - \hat \eta_n(X) \mid\Big]\)</span>.</p>
</div>
<p>In general, we are interested in building classifier from observation <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n)\)</span>: <span class="math inline">\(\hat {\mathfrak f}_n: ~\mathcal X \to \{0,1\}\)</span>, <span class="math inline">\(\hat {\mathfrak f}_n (X) \in \{0,1\}\)</span>.</p>
<hr />
<div class="example">
<p><span id="exm:data" class="example"><strong>Example 3.1  (Dependency on Data) </strong></span>Given <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n)\)</span>,</p>
<p><span class="math display">\[
\hat \eta_n(X) := \frac{\sum_{i=1}^n Y_i exp(-\mid X_i - X \mid ^2 / r_n^2)}{\sum_{i=1}^n exp(-\mid X_i - X \mid ^2 / r_n^2)}.
\]</span></p>
<p>Then we have:</p>
<p><span class="math display">\[
\hat {\mathfrak f}_n(X; (x_1, y_1), \cdots, (x_n, y_n)) =
\begin{cases}
1 &amp; if ~\hat \eta_n(X) \geq \frac{1}{2}\\
0 &amp; o.w.
\end{cases}
\]</span></p>
</div>
<hr />
<div class="remark">
<p><span id="unlabeled-div-5" class="remark"><em>Remark</em>. </span><span class="math inline">\(\hat \eta_n(X) \in [0,1]\)</span>.</p>
</div>
<p><strong><em>Goal</em></strong>: To build <span class="math inline">\(\hat{\mathfrak f}_n\)</span> in such a way that <span class="math inline">\(R(\hat{\mathfrak f}_n) - R^*_B\)</span> goes to 0 as <span class="math inline">\(n \to \infty\)</span>.</p>
<div id="notions-of-consistency-for-families-of-binary-classifier" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Notions of Consistency for Families of Binary Classifier<a href="plug-in-classifiers-similarity-classifier.html#notions-of-consistency-for-families-of-binary-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>We say that <span class="math inline">\(\{\hat{\mathfrak f}_n\}_{n\in N}\)</span> is <strong><em>consistent</em></strong> for the distribution <span class="math inline">\(\rho\)</span>, if we have:<br />
<span class="math display">\[
\underset{n \to \infty}{\operatorname{lim}} (E_{(x_1, y_1), \cdots, (x_n, y_n)} [R(\hat{\mathfrak f}_n)] - R^*_B) = 0
\]</span></li>
</ol>
<div class="remark">
<p><span id="unlabeled-div-6" class="remark"><em>Remark</em>. </span></p>
<ul>
<li><p><span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n) \underset{i.i.d}{\sim} \rho\)</span>.</p></li>
<li><p><span class="math inline">\(R^*_B = \underset{\mathfrak f}{\operatorname{min}} P_{(X,Y) \sim \rho} [\mathfrak f(X) \neq Y]\)</span>.</p></li>
<li><p><span class="math inline">\(R(\hat{\mathfrak f}_n) = P_{(X,Y) \sim \rho} [\hat{\mathfrak f}_n(X) \neq Y]\)</span>.</p></li>
</ul>
</div>
<ol start="2" style="list-style-type: decimal">
<li>We say that <span class="math inline">\(\{\hat{\mathfrak f}_n\}_{n\in N}\)</span> is <strong><em>strongly consistent</em></strong> for the distribution <span class="math inline">\(\rho\)</span>, if we have:<br />
<span class="math display">\[
\underset{n \to \infty}{\operatorname{lim}} (R(\hat{\mathfrak f}_n) - R^*_B) = 0, ~\text{with probability 1 (almost sure convergence).}
\]</span></li>
</ol>
<hr />
<div class="example">
<p><span id="exm:student" class="example"><strong>Example 3.2  (Conceptual Explaination) </strong></span>Assume we have M students.Then for student m, we have:</p>
<p><span class="math display">\[
\begin{cases}
(x_1^m, y_1^m), \cdots, (x_n^m, y_n^m) \underset{i.i.d}{\sim} \rho\\
\text{The plug-in classifier}: ~\hat{\mathfrak f}_n^m (\cdot ~;(x_1^m, y_1^m), \cdots, (x_n^m, y_n^m))\\
\text{Risk: }R(\hat{\mathfrak f}_n^m) = P_{(X,Y) \sim \rho} [\hat{\mathfrak f}_n^m(X) \neq Y]
\end{cases}.
\]</span></p>
<p><em>Q1 (Consistency)</em>: Suppose I collect all risks that the class computed: <span class="math inline">\(R(\hat{\mathfrak f}_n^1), \cdots, R(\hat{\mathfrak f}_n^M)\)</span>, and suppose I average them: <span class="math inline">\(\frac{1}{M} \sum_{m=1}^M R(\hat{\mathfrak f}_n^m) \approx E_{(x_1, y_1), \cdots, (x_n, y_n)} [R(\hat{\mathfrak f}_n)]\)</span>. Is it true that as <span class="math inline">\(n \to \infty\)</span>, this <strong><em>average</em></strong> risk goes to <span class="math inline">\(R^*_B\)</span>.</p>
<p><em>Q2 (Strong Consistency)</em>: Is it true that for <strong><em>each</em></strong> <span class="math inline">\(m\)</span>, we have that <span class="math inline">\(R(\hat{\mathfrak f}_n)\)</span> converges to <span class="math inline">\(R^*_B\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
</div>
<hr />
<ol start="3" style="list-style-type: decimal">
<li>We say that a family <span class="math inline">\(\{\hat{\mathfrak f}_n\}_n\)</span> is <strong><em>universal (strong) consistent</em></strong> if <span class="math inline">\(\{\hat{\mathfrak f}_n\}_n\)</span> is (strong) consistent for all <span class="math inline">\(\rho\)</span>.</li>
</ol>
<hr />
<div class="example">
<p><span id="exm:uni" class="example"><strong>Example 3.3  (Universality in Statistical Inference) </strong></span>Let <span class="math inline">\(\mu\)</span> be a probability distribution over <span class="math inline">\(R\)</span> with well defined first moment <span class="math inline">\(\theta := E_{Z \sim \mu} (Z)\)</span>. The goal is to build an estimator <span class="math inline">\(\hat \theta_n(z_1, \cdots, z_n)\)</span> that is consistent for <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><p><span class="math inline">\(\hat \theta_n^1(z_1, \cdots, z_n) = \frac{1}{n} \sum z_i ~(\text{Sample Mean})\)</span>, is universal strong consistent.<br />
<span class="math inline">\(z_1, \cdots, z_n \underset{i.i.d}{\sim} \mu\)</span> for arbitrary <span class="math inline">\(\mu\)</span>, by Law of Large Number (Strong LLN), we know that with probability 1, <span class="math inline">\((\hat \theta_n^1(z_1, \cdots, z_n) - \theta) \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p></li>
<li><p><span class="math inline">\(\hat \theta_n^2(z_1, \cdots, z_n) = \text{Sample Median}\)</span>, is not universal consistent.<br />
If <span class="math inline">\(\mu\)</span> is a distribution such that its median is not equal to its mean, then it is not true that <span class="math inline">\((\hat \theta_n^2(z_1, \cdots, z_n) - \theta) \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>. However, <span class="math inline">\(\hat \theta_n^2(z_1, \cdots, z_n)\)</span> is consistent for <span class="math inline">\(\mu = N(1,1)\)</span> whose mean and median are the same.</p></li>
</ul>
</div>
<hr />
<p>For the plug-in classifiers, studying (strong) consistency reduce to understanding the following two questions:</p>
<ul>
<li><p>Does <span class="math inline">\(E_{(X,Y) \sim \rho} \Big[\mid \eta(X) - \hat \eta_n(X) \mid \Big]\)</span> converge to 0 as <span class="math inline">\(n \to \infty\)</span>, with probability 1?<br />
If yes, <span class="math inline">\(\{\hat{\mathfrak f}_n\}_n\)</span> is strong consistent for <span class="math inline">\(\rho\)</span>.</p></li>
<li><p>Does <span class="math inline">\(E_{(x_1,y_1), \cdots, (x_n,y_n), (X,Y) \sim \rho} \Big[\mid \eta(X) - \hat \eta_n(X) \mid \Big]\)</span> converge to 0 as <span class="math inline">\(n \to \infty\)</span>?<br />
If yes, <span class="math inline">\(\{\hat{\mathfrak f}_n\}_n\)</span> is consistent for <span class="math inline">\(\rho\)</span>.</p></li>
</ul>
</div>
<div id="functions-of-hat-eta_n" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Functions of <span class="math inline">\(\hat \eta_n\)</span><a href="plug-in-classifiers-similarity-classifier.html#functions-of-hat-eta_n" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math display">\[
\hat \eta_n(X; (x_1,y_1), \cdots, (x_n,y_n)) = \sum_{i=1}^n y_i \omega_{in} (X; x_1, \cdots, x_n)
\]</span></p>
<p>where <span class="math inline">\(\omega_{in} (X; x_1, \cdots, x_n)\)</span> is weight that we give to each point <span class="math inline">\((x_i, y_i)\)</span> and <span class="math inline">\(\sum_{i=1}^n \omega_{in} (X; x_1, \cdots, x_n) = 1\)</span>.</p>
<p>In other words, to determine <span class="math inline">\(\hat \eta_n(X)\)</span>, we consider weighted averages of the labels <span class="math inline">\(y_1, \cdots, y_n\)</span>.</p>
<p>Intuitively the <span class="math inline">\(i\)</span>s for which <span class="math inline">\(\omega_{in}\)</span> are larger should be ones for which <span class="math inline">\(x_i\)</span> is more similar to <span class="math inline">\(X\)</span>.</p>
<hr />
<div class="example">
<p><span id="exm:weight1" class="example"><strong>Example 3.4  (Kernel-Based Weights) </strong></span><span class="math inline">\(\mathcal X = R^d\)</span>, select a length-scale <span class="math inline">\(r &gt; 0\)</span>.</p>
<p><span class="math display">\[
\omega_{in}(X; x_1, \cdots, x_n) = \frac{exp(- \frac{|| X_i - X || ^2}{2r^2}) }{\sum_{i=1}^n exp(-\frac{|| X_i - X || ^2}{2r^2})}.
\]</span></p>
<p>Points <span class="math inline">\(x_i\)</span> for which <span class="math inline">\(|| X - x_i || \leq r\)</span> will be give higher weight than points that are such that <span class="math inline">\(|| X - x_i || \geq r\)</span>.</p>
</div>
<hr />
<div class="example">
<p><span id="exm:weight2" class="example"><strong>Example 3.5  (Histogram-Based Weights) </strong></span><span class="math inline">\(\mathcal X = R^d\)</span>, select a length-scale <span class="math inline">\(h &gt; 0\)</span>.</p>
<p>For <span class="math inline">\(d=2\)</span>, we have:</p>
<p>Introduce pre-weights:</p>
<p>Provided there is at least one <span class="math inline">\(x_i\)</span> in the same cell as <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\tilde \omega_{in}(X; x_1, \cdots, x_n) :=
\begin{cases}
1 &amp; \text{if } x_i \text{ and } X ~\text{belong to the same cell}\\
0 &amp; o.w.
\end{cases}
\]</span>
Otherwise we define:</p>
<p><span class="math display">\[
\tilde \omega_{in}(X; x_1, \cdots, x_n) := \frac{1}{n}
\]</span>
Then we have:</p>
<p><span class="math display">\[
\omega_{in}(X; x_1, \cdots, x_n) = \frac{\tilde \omega_{in}(X; x_1, \cdots, x_n)}{\sum_{j=1}^n \tilde \omega_{jn}(X; x_1, \cdots, x_n)}
\]</span></p>
</div>
<hr />
<div class="example">
<p><span id="exm:weight3" class="example"><strong>Example 3.6  (k-NN Based Weights) </strong></span><span class="math inline">\(k \in [1,n]\)</span></p>
<p><span class="math display">\[
\tilde \omega_{in}(X; x_1, \cdots, x_n) =
\begin{cases}
1 &amp; \text{if } x_i \text{ is among the k-closest points to } X\\
0 &amp; o.w.
\end{cases}
\]</span>
Then we have:</p>
<p><span class="math display">\[
\omega_{in}(X; x_1, \cdots, x_n) = \frac{\tilde \omega_{in}(X; x_1, \cdots, x_n)}{\sum_{j=1}^n \tilde \omega_{jn}(X; x_1, \cdots, x_n)}
\]</span></p>
</div>
<p>There may be ties, which will be discussed later.</p>
<hr />
</div>
<div id="consistency" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Consistency<a href="plug-in-classifiers-similarity-classifier.html#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:stone" class="theorem"><strong>Theorem 3.1  (Stone's Theorem) </strong></span>Let <span class="math inline">\(\omega_{in}(X; x_1, \cdots, x_n)\)</span> be such that following three conditions hold:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\exists c&gt;0\)</span> such that <span class="math inline">\(\forall \text{non-negative function } g: R^d \to R\)</span>, with <span class="math inline">\(E_{(X,y) \sim \rho} [g(X)] &lt; \infty\)</span>: <span class="math inline">\(E_{(X, x_1, \cdots, x_n)} [\sum_{i=1}^n \omega_{in} (X) g(x_i)] \leq c ~E_X [g(X)]\)</span>;</p></li>
<li><p><span class="math inline">\(\forall a &gt; 0\)</span>, <span class="math inline">\(\underset{n \to \infty}{\operatorname{lim}} E_{(X, x_1, \cdots, x_n)} [\sum_{i=1}^n \omega_{in}(X) ~1_{|x_i - X| &gt; a}] = 0\)</span> (weights are only relevant or large for <span class="math inline">\(x_i\)</span> close to <span class="math inline">\(X\)</span>);</p></li>
<li><p><span class="math inline">\(\underset{n \to \infty}{\operatorname{lim}} E_{(X, x_1, \cdots, x_n)} [\underset{i=1,\cdots, n}{\operatorname{max}} \omega_{in}(X)] = 0\)</span>.</p></li>
</ol>
<p>Then the family of similarity classifiers <span class="math inline">\(\{\hat{\mathfrak f}_n\}_n\)</span> induced by these weights is consistent for <span class="math inline">\(\rho\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span></p>
<p><strong><em>Goal</em></strong>: to show that <span class="math inline">\(E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\eta(X) - \hat \eta_n(X)|] \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<p>Introduce <span class="math inline">\(\bar \eta_n(X) := \sum_{i=1}^n \eta(x_i) \omega_{in}(X)\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\eta(X) - \hat \eta_n(X)|] &amp;\leq E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\eta(X) - \bar \eta_n(X)|] + \\
&amp;\quad E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\bar \eta_n(X) - \hat \eta_n(X)|]\\
&amp;\underset{C.S. Ineq}{\leq} E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\eta(X) - \bar \eta_n(X)|] \text{ (&quot;bias term&quot;)}+ \\
&amp;\quad \Big(E_{(X,Y), (x_1,y_1), \cdots, (x_n, y_n)} [|\bar \eta_n(X) - \hat \eta_n(X)|^2]\Big)^2 \text{ (&quot;variance term&quot;)}
\end{aligned}
\]</span></p>
<p>Then show both terms <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math display">\[
\begin{aligned}
E[\hat \eta_n(X) | X, x_1 \cdots, x_n] &amp;= \sum_{i=1}^n E[y_i | X, x_1, \cdots, x_n] ~\omega_{in}(X)\\
&amp;= \sum_{i=1}^n E[y_i | X, x_1, \cdots, x_n] ~\omega_{in}(X)\\
&amp;= \sum_{i=1}^n E[y_i | x_i] ~\omega_{in}(X)\\
&amp;= \bar \eta_n(X)
\end{aligned};
\]</span></p></li>
<li><p>To show that the variance term <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>, we only need to use condition 3 in the <strong>Theorem <a href="plug-in-classifiers-similarity-classifier.html#thm:stone">3.1</a></strong>;</p></li>
<li><p>To show that the bias term <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>, we only need to use conditions 1 and 2 in the <strong>Theorem <a href="plug-in-classifiers-similarity-classifier.html#thm:stone">3.1</a></strong>.</p></li>
</ol>
</div>
<div class="example">
<p><span id="exm:weight1-2" class="example"><strong>Example 3.7  (Continuation of Example <a href="plug-in-classifiers-similarity-classifier.html#exm:weight2">3.5</a>) </strong></span>Let <span class="math inline">\(h^d = h_n^d\)</span>. If <span class="math inline">\(n h_n \to \infty\)</span> as <span class="math inline">\(n \to \infty\)</span>, and if <span class="math inline">\(h_n^d \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>, then the family of histogram classifiers <span class="math inline">\(\{\hat {\mathfrak f}_{n, h_n}\}_{n \in N}\)</span> is universally consistent.</p>
<p>Conditions of the Stones Theorem:</p>
<ol style="list-style-type: decimal">
<li><p>Technical Assumption.</p></li>
<li><p>Locality (<span class="math inline">\(h = h_n \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>).</p></li>
<li><p>No weight dominates the others. (For every <span class="math inline">\(X\)</span>, we should use a growing number of training data points to make a prediction, i.e. the expected number of points in a cell <span class="math inline">\(nh_n^d \to \infty\)</span> as <span class="math inline">\(n \to \infty\)</span>)</p></li>
</ol>
<p>For example, let <span class="math inline">\(\rho = Uniform\)</span>, <span class="math inline">\(N_X = \#\{x_i ~s.t. ~x_i \in Cell(X)\}\)</span>, then <span class="math inline">\(N_X \sim Binomial(n, P_h)\)</span>, where <span class="math inline">\(P_h = P(x_i \in Cell(X)) \propto h^d\)</span>. As a result, <span class="math inline">\(E(N_X) \propto nh^d\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:weight1-3" class="example"><strong>Example 3.8  (Continuation of Example <a href="plug-in-classifiers-similarity-classifier.html#exm:weight3">3.6</a>) </strong></span></p>
<ul>
<li><p><span class="math inline">\(k \to \infty\)</span> (locality)</p></li>
<li><p><span class="math inline">\(\frac{k}{n} \to 0\)</span> (Related to condition 3 in Stone’s Theorem)</p></li>
</ul>
</div>
</div>
<div id="strong-consistency" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Strong Consistency<a href="plug-in-classifiers-similarity-classifier.html#strong-consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="theorems-from-probability-theory" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Theorems from probability theory:<a href="plug-in-classifiers-similarity-classifier.html#theorems-from-probability-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:slln" class="theorem"><strong>Theorem 3.2  (Strong Law of Large Numbers) </strong></span>Let <span class="math inline">\(Z_1, Z_2, \cdots\)</span> be <span class="math inline">\(i.i.d\)</span> real valued random variables, and <span class="math inline">\(E(Z_i) = m\)</span>. Then <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (Z_i - m) \to 0\)</span> almost surely as <span class="math inline">\(n \to \infty\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:clt" class="theorem"><strong>Theorem 3.3  (Central Limit Theorem) </strong></span>(It quantifies in a sense how fast is the convergence in the LLN)</p>
<p>Suppose <span class="math inline">\(Var(Z_i) = \sigma^2 &lt; \infty\)</span> for <span class="math inline">\(\{Z_i\}\)</span> in <strong><em>Theorem <a href="plug-in-classifiers-similarity-classifier.html#thm:slln">3.2</a></em></strong> , then <span class="math inline">\(\sqrt n (\frac{1}{n} \sum_{i=1}^n (Z_i - m)) \underset{d}{\to} N(0, \sigma^2)\)</span>.</p>
<p>Recall, what this means is that <span class="math inline">\(\forall t\in R\)</span>, we have:</p>
<p><span class="math display">\[
\underset{n\to \infty}{\operatorname{lim}} P(\frac{\sqrt n}{\sigma} [\frac{1}{n}\sum_{i=1}^n (Z_i - m)] &gt; t) = \int_t^\infty \frac{e^{-s^2/2}}{\sqrt {2\pi}} ds = 1-F(t)
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-9" class="remark"><em>Remark</em>. </span>CLT is still asymptotic (渐近的).</p>
</div>
<div class="theorem">
<p><span id="thm:bet" class="theorem"><strong>Theorem 3.4  (Berry-Essen Theorem) </strong></span>(It is a quantitative and non-asymptotic version of CLT)</p>
<p><span class="math inline">\(\forall t\)</span>, we have:
<span class="math display">\[
\Big| P(\frac{\sqrt n}{\sigma} [\frac{1}{n}\sum_{i=1}^n (Z_i - m)] \geq t) - \int_t^\infty \frac{e^{-s^2/2}}{\sqrt {2\pi}} ds \Big| \leq \frac{c \gamma}{\sigma^3 \sqrt{n}},
\]</span></p>
<p>where <span class="math inline">\(\gamma = E(|Z_1|^3) &lt; \infty\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-10" class="remark"><em>Remark</em>. </span>Berry-Essen Theorem doesn’t provide very precise information about <span class="math inline">\(P(\frac{\sqrt n}{\sigma} [\frac{1}{n}\sum_{i=1}^n (Z_i - m)] \geq t)\)</span> when <span class="math inline">\(t\)</span> is very large.</p>
</div>
</div>
<div id="concentration-inequalities" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Concentration Inequalities<a href="plug-in-classifiers-similarity-classifier.html#concentration-inequalities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong><em>Goal</em></strong>: To quantify precisely what is the behavior of <span class="math inline">\(P(\frac{\sqrt n}{\sigma} \sum_{i=1}^n (Z_i - m) \geq t)\)</span> (tail information).<br />
What do we want more precisely: If <span class="math inline">\(\frac{\sqrt n}{\sigma} [\frac{1}{n}\sum_{i=1}^n (Z_i - m)]\)</span> was truly a standard Gaussian random variable <span class="math inline">\(Z\)</span>, in that case what can we say about <span class="math inline">\(P(Z \geq t)\)</span>?<br />
What we could say is that <span class="math inline">\(P(Z \geq t) \leq e^{-\frac{t^2}{2}}\)</span>, <span class="math inline">\(\forall t &gt; 0\)</span>.<br />
The concentration inequalities we are after should give us some information similar to the one above.<br />
To get there, let us start with the basics: For a given real valued random variable <span class="math inline">\(W\)</span>, what can we say about <span class="math inline">\(P(W \geq t)\)</span> for <span class="math inline">\(t \geq 0\)</span>?</p>
<div class="theorem">
<p><span id="thm:mi" class="theorem"><strong>Theorem 3.5  (Markov Inequality) </strong></span></p>
<p>Let <span class="math inline">\(t &gt; 0\)</span>, then we have</p>
<p><span class="math display">\[
P(W \geq t) \leq E(\frac{W}{t} ~1_{W \geq t}) \leq \frac{E[|W|]}{t}
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
P(W \geq t) &amp;= E[1_{W \geq t}]\\
&amp;\leq E[\frac{W}{t} ~1_{W \geq t}]\\
&amp;\leq E[\frac{|W|}{t} ~1_{W \geq t}]\\
&amp;\leq E[\frac{|W|}{t}].
\end{aligned}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-12" class="remark"><em>Remark</em>. </span>Let <span class="math inline">\(\phi: R \to [0,\infty)\)</span> be non-decreasing. Then <span class="math inline">\(\forall t&gt;0\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
P(W \geq t) \leq P\Big(\phi(W) \geq \phi(t)\Big) \underset{M.I.}{\leq} \frac{E[\phi(W)]}{\phi(t)}
\end{aligned}
\]</span></p>
<p>How about we select <span class="math inline">\(\phi\)</span> to be <span class="math inline">\(\phi(t) := exp(t \cdot s)\)</span>, where <span class="math inline">\(s&gt;0\)</span>. Then <span class="math inline">\(\forall s&gt;0\)</span>:</p>
<p><span class="math display">\[
P(W \geq t) \leq \frac{E[exp(sW)]}{exp(st)} = exp(-st)E[exp(sW)].
\]</span></p>
<p>As a result, we have:</p>
<p><span class="math display">\[
P(W \geq t) \leq \underset{s&gt;0}{\operatorname{inf}}\{exp(-st) E[exp(sW)]\} \text{ (Chernoff&#39;s bound)}.
\]</span></p>
<p>It is clear that we need to understand the behavior of <span class="math inline">\(E[exp(sW)]\)</span>.</p>
<p>The <span class="math inline">\(W\)</span> we want to apply this to, for example, <span class="math inline">\(W:=\frac{1}{n} \sum_{i=1}^n (Z_i - m)\)</span>, where the <span class="math inline">\(Z_i\)</span>s are independent with mean <span class="math inline">\(m\)</span>. Thus, the <span class="math inline">\(W\)</span> for us should be of the form <span class="math inline">\(\sum_{i=1}^n Z_i\)</span>, where <span class="math inline">\(Z_i\)</span>s are independent and <span class="math inline">\(E(Z_i) = 0\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:hil" class="lemma"><strong>Lemma 3.2  </strong></span>Suppose <span class="math inline">\(Z\)</span> is a r.v. with <span class="math inline">\(E(Z) = 0\)</span> and <span class="math inline">\(\exists \text{ constant } a,b\)</span>, <span class="math inline">\(s.t. ~a\leq Z \leq b\)</span> (Consequently, <span class="math inline">\(a&lt;0\)</span> and <span class="math inline">\(b&gt;0\)</span>). Then <span class="math inline">\(\forall s&gt;0\)</span>, we have</p>
<p><span class="math display">\[
E[exp(sZ)] \leq exp(\frac{s^2(b-a)^2}{8})
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-13" class="remark"><em>Remark</em>. </span>This lemma actually is saying that a bounded random variable is a sub-Gaussian random variable.<br />
We say that a random variable <span class="math inline">\(Z\)</span> (with <span class="math inline">\(E(Z) = 0\)</span>) is <strong>sub-Gaussian</strong> with parameter <span class="math inline">\(\sigma^2\)</span> if</p>
<p><span class="math display">\[
E(e^{sZ}) \leq exp(\frac{\sigma^2 s^2}{2}), ~s &gt; 0
\]</span></p>
<p>The right hand side is exactly the moment generating function of a <span class="math inline">\(N(0, \sigma^2)\)</span>.</p>
<p>In particular, the lemma is saying that <span class="math inline">\(Z\)</span> satisfying <span class="math inline">\(a\leq b\)</span> and <span class="math inline">\(E(Z) = 0\)</span> is sub-Gaussian with parameter <span class="math inline">\(\frac{(b-a)^2}{4}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(Z = \frac{b-Z}{b-a} a + \frac{Z-a}{b-a} b\)</span>, then we have</p>
<p><span class="math display">\[
\begin{aligned}
E[exp(sZ)] &amp;= E \Big[ exp[s(\frac{b-Z}{b-a} a + \frac{Z-a}{b-a} b)] \Big] \\&amp;\underset{convexity}{\leq} E[\frac{b-Z}{b-a} exp(sa) + \frac{Z-a}{b-a} exp(sb)] \\&amp;\underset{E(Z) = 0}{=}  \frac{b}{b-a} exp(sa) - \frac{a}{b-a} exp(sb)
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(q = -\frac{a}{b-a}\)</span>, then <span class="math inline">\(\frac{b}{b-a} = 1-q\)</span>. Then,</p>
<p><span class="math display">\[
\begin{aligned}
E[exp(sZ)] &amp;\leq (1-q) ~exp(-s(b-a)q) + q ~exp(s(b-a)(1-q))\\
&amp;= exp[-s(b-a)q + log\Big((1-q) + q~exp(s(b-a))\Big)]
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\psi(h):= -qh + log\Big( (1-q) + q ~exp(h)\Big)\)</span>, then <span class="math inline">\(\psi(0)=0\)</span>, <span class="math inline">\(\psi&#39;(0) = 0\)</span>, <span class="math inline">\(\psi&#39;&#39;(h) \leq \frac{1}{4} ~\forall h &gt; 0\)</span>. By Taylor’s Theorem, <span class="math inline">\(\psi(h) \leq \frac{1}{2} (\frac{1}{4} h^2) = \frac{h^2}{8}\)</span>. As a result,</p>
<p><span class="math display">\[
E[exp(sZ)] \underset{h = s(b-a)}{\leq} exp(\frac{s^2(b-a)^2}{8})
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:hi" class="theorem"><strong>Theorem 3.6  (Hoeffding's Inequality) </strong></span>Let <span class="math inline">\(Z_1, \cdots, Z_n\)</span> be independent random variable (not necessarily identically distributed) such that:</p>
<ul>
<li><p><span class="math inline">\(E(Z_i) = 0\)</span>, <span class="math inline">\(\forall i=1,\cdots, n\)</span>;</p></li>
<li><p><span class="math inline">\(\forall i\)</span>, <span class="math inline">\(\exists \text{ constant } a_i, b_i\)</span>, <span class="math inline">\(s.t. ~a_i \leq Z_i \leq b_i\)</span>.</p></li>
</ul>
<p>Then <span class="math inline">\(\forall t\)</span>, we have:</p>
<p><span class="math display">\[
P(\frac{1}{n} \sum_{i=1}^n Z_i \geq t) \leq exp(\frac{-2t^2n^2}{\sum_{i=1}^n (b_i - a_i)^2})
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(W = \frac{1}{n} \sum_{i=1}^n Z_i\)</span>, then</p>
<p><span class="math display">\[
P(W \geq t) \underset{\text{Chernoff&#39;s Inequality}}{\leq} \underset{s&gt;0}{\operatorname{inf}} \{\frac{E[e^{\frac{s}{n} \sum_{i=1}^n Z_i}]}{e^{st}}\},
\]</span>
where</p>
<p><span class="math display">\[
E[e^{\frac{s}{n} \sum_{i=1}^n Z_i}] = E\Big[\prod_{i=1}^n e^{\frac{s}{n} Z_i}\Big] \underset{independence}{=} \prod_{i=1}^n E[e^{\frac{s}{n} Z_i}]
\]</span></p>
<p>Using <strong><em>Lemma <a href="plug-in-classifiers-similarity-classifier.html#lem:hil">3.2</a></em></strong>, we have:</p>
<p><span class="math display">\[
\prod_{i=1}^n E[e^{\frac{s}{n} Z_i}] \leq \prod_{i=1}^n exp(\frac{s^2(b_i-a_i)^2}{8n^2}) = exp(\frac{s^2}{8n^2} \sum_{i=1}^n (b_i-a_i)^2)
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
P(W \geq t) \leq \underset{s&gt;0}{\operatorname{inf}} \{\frac{exp(\frac{s^2}{8n^2} \sum_{i=1}^n (b_i-a_i)^2)}{exp(st)}\} = \underset{s&gt;0}{\operatorname{inf}} \{exp(\frac{s^2}{8n^2} \sum_{i=1}^n (b_i-a_i)^2 - st)\}
\]</span></p>
<p>For the quadratic, <span class="math inline">\(s^* = \frac{4tn^2}{\sum_{i=1}^n (b_i-a_i)^2}\)</span> minimizes it. As a result,</p>
<p><span class="math display">\[
P(W \geq t) \leq exp(\frac{-2t^2n^2}{\sum_{i=1}^n (b_i - a_i)^2})
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-16" class="remark"><em>Remark</em>. </span></p>
<ol style="list-style-type: decimal">
<li><p>The <span class="math inline">\(Z_i\)</span>s can have different distributions with <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(a_i \leq Z_i \leq b_i\)</span>. Independence is essential though.</p></li>
<li><p>What if <span class="math inline">\(E(Z_i) \neq 0\)</span>? Let <span class="math inline">\(\tilde Z_i := Z_i - E(Z_i)\)</span> and <span class="math inline">\(a_i \leq \tilde Z_i \leq b_i\)</span>. Then <span class="math inline">\(\forall t&gt;0\)</span>, we have:</p></li>
</ol>
<p><span class="math display">\[
P(\frac{1}{n} \sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) = P(\frac{1}{n} \sum_{i=1}^n \tilde Z_i &gt; t) \leq exp(\frac{2n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2})
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p><span class="math inline">\(\forall t &gt; 0\)</span>, we have:<br />
<span class="math display">\[
\begin{aligned}
P(\frac{1}{n} \sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) &amp;\leq exp(\frac{2n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2})
\end{aligned}
\]</span><br />
and<br />
<span class="math display">\[
\begin{aligned}
P(\frac{1}{n} \sum_{i=1}^n (E(Z_i) - Z_i) \geq t) &amp;\leq exp(\frac{2n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2})
\end{aligned}
\]</span><br />
As a result,<br />
<span class="math display">\[
\begin{aligned}
P\Big(|\frac{1}{n} \sum_{i=1}^n (Z_i - E(Z_i))| \geq t \Big) &amp;\leq P(\frac{1}{n} \sum_{i=1}^n (Z_i - E(Z_i)) &gt; t) + P(\frac{1}{n} \sum_{i=1}^n (E(Z_i) - Z_i) \geq t) \\
&amp;\leq 2~exp(\frac{2n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2})
\end{aligned}
\]</span></p></li>
<li><p>Suppose all <span class="math inline">\(Z_i\)</span>s come from the same distribution, say Uniform or Gaussian. According to <strong><em>Theorem <a href="plug-in-classifiers-similarity-classifier.html#thm:hi">3.6</a></em></strong>, the right hand side (<span class="math inline">\(exp(\frac{2n^2 t^2}{\sum_{i=1}^n (b_i - a_i)^2})\)</span>) would be the same in both cases, which means that Hoeffding’s does not use variance information.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:bi" class="theorem"><strong>Theorem 3.7  (Bernstein's Inequality) </strong></span>Let <span class="math inline">\(Z_1, \cdots, Z_n\)</span> be independent random variables with <span class="math inline">\(E(Z_i) = 0\)</span> and such that <span class="math inline">\(-M \leq Z_i \leq M\)</span>, and let <span class="math inline">\(\sigma_i^2 = Var(Z_i)\)</span>, <span class="math inline">\(\forall i = 1,\cdots,n\)</span>. Then <span class="math inline">\(\forall t&gt;0\)</span>, we have:</p>
<p><span class="math display">\[
P(\frac{1}{n} \sum_{i=1}^n Z_i \geq t) \leq exp(\frac{-\frac{1}{2} t^2n^2}{\frac{1}{3} Mnt + \sum_{i=1}^n \sigma_i^2})
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P\Big(| \frac{1}{n} \sum_{i=1}^n Z_i | \geq t\Big) \leq 2~exp(\frac{-\frac{1}{2} t^2n^2}{\frac{1}{3} Mnt + \sum_{i=1}^n \sigma_i^2})
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-17" class="remark"><em>Remark</em>. </span>Suppose <span class="math inline">\(\forall i\)</span>, <span class="math inline">\(\sigma_i^2 &lt; \sigma^2\)</span>, then we have</p>
<p><span class="math display">\[
P(\frac{1}{n} \sum_{i=1}^n Z_i \geq t) \leq exp(\frac{-\frac{1}{2} t^2n}{\frac{1}{3} Mt + \sigma^2})
\]</span></p>
</div>
<ul>
<li><p><strong>Hoeffding’s v.s. Bernstein’s:</strong><br />
Let <span class="math inline">\(a_i = -M\)</span>, <span class="math inline">\(b_i = M\)</span>, <span class="math inline">\(\sigma^2 = Var(Z_i)\)</span>. Then for Hoeffding’s, we have:<br />
<span class="math display">\[
\begin{aligned}
P(\frac{1}{n} \sum_{i=1}^n Z_i \geq t) &amp;\leq exp(\frac{-2t^2n^2}{\sum_{i=1}^n (b_i - a_i)^2})\\
&amp;= exp(\frac{-2t^2n^2}{4n M^2})\\
&amp;= exp(\frac{-t^2n}{2M^2})
\end{aligned}
\]</span><br />
For Bernstein’s, we have:<br />
<span class="math display">\[
\begin{aligned}
P(\frac{1}{n} \sum_{i=1}^n Z_i \geq t) &amp;\leq exp(\frac{-\frac{1}{2} t^2n^2}{\frac{1}{3} Mnt + \sum_{i=1}^n \sigma^2})\\
&amp;= exp(\frac{-\frac{1}{2} t^2n}{\frac{1}{3} Mt + \sigma^2})
\end{aligned}
\]</span><br />
As<br />
<span class="math display">\[
\frac{-\frac{1}{2} t^2n}{\frac{1}{3} Mt + \sigma^2} \leq \frac{-\frac{1}{2}t^2n}{M^2} \Leftrightarrow M^2 \geq \sigma^2 + \frac{1}{3}Mt
\]</span><br />
which means that <span class="math inline">\(M^2 \geq \sigma^2 + \frac{1}{3} Mt, ~\forall t \in [0,M]\)</span>, or <span class="math inline">\(\frac{2}{3}M^2 \geq \sigma^2\)</span>.<br />
As a result, when <span class="math inline">\(\sigma^2\)</span> is considerably smaller than <span class="math inline">\(M^2\)</span>, we should use Bernstein’s Inequality rather than Hoeffding’s Inequality.<br />
In general, both inequalities have the form of<br />
<span class="math display">\[
P(| W_n - E(W_n) | \geq t) \leq 2 ~exp(\cdots)
\]</span><br />
i.e. what is the likelihood that some variable is away from its mean more than distance <span class="math inline">\(t\)</span>.</p></li>
<li><p><strong><em>Summary</em></strong>: all of these are theorems about sums of independent random variable.</p></li>
</ul>
</div>
<div id="more-about-con-ineq" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> More about Con-Ineq<a href="plug-in-classifiers-similarity-classifier.html#more-about-con-ineq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(Z_1, \cdots, Z_n\)</span> be real-valued random variables. Also that <span class="math inline">\(U_1, \cdots, U_n\)</span> be another sequence of random variables (not necessarily real-valued). We say <span class="math inline">\(\{Z_i\}_{i=1,\cdots,n}\)</span> is a <strong><em>martingale difference sequence</em></strong> w.r.t. <span class="math inline">\(\{U_i\}_{i=1,\cdots,n}\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(Z_k = h_k(U_1, \cdots, U_k)\)</span>, <span class="math inline">\(\forall k=1,\cdots,n\)</span></p></li>
<li><p><span class="math inline">\(E(Z_{k+1} | U_1, \cdots, U_k) = 0\)</span>, <span class="math inline">\(\forall k = 1,\cdots, n-1\)</span>.</p></li>
</ol>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Suppose that <span class="math inline">\(Z_1, \cdots, Z_n\)</span> are independent random variables with mean 0 and <span class="math inline">\(U_i = Z_i\)</span>, <span class="math inline">\(\forall i=1,\cdots,n\)</span>. Then <span class="math inline">\(\{Z_i\}_{i=1,\cdots,n}\)</span> is a MDS w.r.t. (relative to) <span class="math inline">\(\{U_i\}_{i=1,\cdots,n}\)</span></p>
</div>
<div class="theorem">
<p><span id="thm:hiMDS" class="theorem"><strong>Theorem 3.8  (Hoeffding's Inequality for MDS) </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(\{Z_i\}_{i=1,\cdots,n}\)</span> be a MDS relative to <span class="math inline">\(\{X_i\}_{i=1,\cdots,n}\)</span> (which generalizes “Let <span class="math inline">\(\{Z_i\}_{i=1,\cdots,n}\)</span> be independent r.v.s with mean 0”).</p></li>
<li><p>Assume that <span class="math inline">\(\forall i=1,\cdots, n\)</span>, we have:<br />
<span class="math display">\[
V_i \leq Z_i \leq V_i + C_i,
\]</span><br />
where <span class="math inline">\(C_i &gt;0\)</span> is a fixed number, and <span class="math inline">\(V_i\)</span> can be a r.v. and <span class="math inline">\(V_i = \psi_i(X_1, \cdots, X_{i-1})\)</span>, <span class="math inline">\(\forall i\)</span> (which generalizes the assumption <span class="math inline">\(a_i \leq Z_i \leq b_i\)</span>).</p></li>
</ol>
<p>Then, <span class="math inline">\(\forall t&gt;0\)</span>, we have:</p>
<p><span class="math display">\[
P(\frac{1}{n}\sum_{i=1}^n Z_i \geq t) \leq exp(\frac{-2t^2n^2}{\sum_{i=1}^n C_i^2})
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:McI" class="theorem"><strong>Theorem 3.9  (McDiarmid's Inequality) </strong></span>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be independent r.v.s in <span class="math inline">\(R^d\)</span>. Suppose we have a function <span class="math inline">\(g: (R^d)^n \to R\)</span>, i.e. <span class="math inline">\(g(x_1, \cdots, x_n) \in R, ~x_1, \cdots, x_n \in R^d\)</span>, satisfying: <span class="math inline">\(\forall i=1,\cdots, n\)</span>,</p>
<p><span class="math display">\[
\underset{x_1, \cdots, x_n, x_i&#39;}{\operatorname{sup}} \Big| g(x_1, \cdots, x_n) - g(x_1, \cdots, x_{i-1}, x_i&#39;, x_{i+1}, \cdots, x_n) \Big| \leq C_i.
\]</span></p>
<p>Then we have:</p>
<p><span class="math display">\[
P(W_n - E(W_n) \geq t) \leq exp(\frac{-2t^2}{\sum_{i=1}^n C_i^2}),
\]</span></p>
<p>where <span class="math inline">\(W_n := g(X_1, \cdots, X_n)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span></p>
<p><strong>Idea</strong>: To show that <span class="math inline">\(W_n - E(W_n) = \sum_{i=1}^n Z_i\)</span> for a suitable MDS satisfying the assumptions in <strong><em>Theorem <a href="plug-in-classifiers-similarity-classifier.html#thm:hiMDS">3.8</a></em></strong>.</p>
<p>Let <span class="math inline">\(Z_k:= E(W_n | X_1, \cdots, X_k) - E(W_n | X_1, \cdots, X_{k-1})\)</span>, <span class="math inline">\(k \geq 2\)</span>; <span class="math inline">\(Z_1 := E(W_n | X_1) - E(W_n)\)</span>. Then we have:</p>
<ol style="list-style-type: decimal">
<li><p>we can write <span class="math inline">\(W_n - E(W_n) = \sum_{i=1}^n Z_i\)</span>:<br />
<span class="math display">\[
\begin{aligned}
\sum_{i=1}^n Z_i &amp;= E(W_n | X_1, \cdots, X_n) - E(W_n)\\
&amp;= E(g(X_1, \cdots, X_n) | X_1, \cdots, X_n) - E(W_n)\\
&amp;= g(X_1, \cdots, X_n)- E(W_n)\\
&amp;= W_n- E(W_n)
\end{aligned}
\]</span></p></li>
<li><p><span class="math inline">\(\{Z_i\}\)</span> is a MDS relative to <span class="math inline">\(\{X_i\}\)</span>:<br />
<span class="math display">\[
\begin{aligned}
Z_k &amp;= E(W_n | X_1, \cdots, X_k) - E(W_n | X_1, \cdots, X_{k-1})\\
&amp;= \tilde h_k(X_1, \cdots, X_k) - \tilde h_{k-1}(X_1, \cdots, X_{k-1})\\
&amp;= h_k(X_1, \cdots, X_k)
\end{aligned}
\]</span><br />
and<br />
<span class="math display">\[
\begin{aligned}
E(Z_{k+1} | X_1, \cdots, X_k) &amp;= E\Big(E(W_n |X_1, \cdots, X_{k+1}) - E(W_n | X_1, \cdots, X_k) \Big| X_1, \cdots, X_k \Big)\\
&amp;= E\Big( E(W_n |X_1, \cdots, X_{k+1}) \Big | X_1, \cdots, X_k\Big) - E\Big( E(W_n |X_1, \cdots, X_k) \Big | X_1, \cdots, X_k\Big)\\
&amp;\underset{\text{Tower Property}}{=} E(W_n |X_1, \cdots, X_k) - E(W_n |X_1, \cdots, X_k)\\
&amp;= 0
\end{aligned}
\]</span></p></li>
<li><p>we can bound <span class="math inline">\(V_k \leq Z_k \leq V_k + C_k\)</span> for some <span class="math inline">\(V_k, C_k\)</span>:<br />
(Show that in HW2)</p></li>
</ol>
</div>
</div>
<div id="strong-consistency-1" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Strong Consistency<a href="plug-in-classifiers-similarity-classifier.html#strong-consistency-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statements that may encounter (<span class="math inline">\(P(|W_n - E(W_n)| \geq t) \leq 2 exp(-cnt^2)\)</span>):</p>
<ol style="list-style-type: decimal">
<li><p>With probability greater than <span class="math inline">\(1-\delta\)</span>, we have: <span class="math inline">\(|W_n - E(W_n)| \leq f(\delta, n)\)</span> (fixing threshold)</p></li>
<li><p>With probability greater than <span class="math inline">\(1-\delta\)</span>, we have: <span class="math inline">\(|W_n - E(W_n)| \leq \sqrt{\frac{1}{cn} log(\frac{2}{\delta})}\)</span> (let <span class="math inline">\(\delta = 2 exp(-cnt^2)\)</span>, i.e. fixing probability)</p></li>
</ol>
<div class="theorem">
<p><span id="thm:bc" class="theorem"><strong>Theorem 3.10  (Borel-Cantelli Theorem) </strong></span>Let <span class="math inline">\(E_1, E_2, \cdots\)</span> be events in a probability space <span class="math inline">\((\Omega, \mathcal F, P)\)</span>, <span class="math inline">\(\sum_{i=1}^\infty P(E_i) \leq \infty\)</span> (the events <span class="math inline">\(E_i\)</span> become less and less likely), then for <span class="math inline">\(E=\{\omega \in \Omega: ~\exists N(\omega), s.t. ~ \forall n \geq N(\omega), \omega \in E_n^c\}\)</span>, we have <span class="math inline">\(P(E) = 1\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:bc1" class="example"><strong>Example 3.9  </strong></span><span class="math inline">\(Z_1, Z_2, \cdots\)</span> independent and <span class="math inline">\(Z_i \sim Bernoulli(p_i)\)</span>, <span class="math inline">\(\sum_{i=1}^\infty p_i &lt; \infty\)</span>. Notice <span class="math inline">\(p_i = P(Z_i=1) \to 0\)</span> as <span class="math inline">\(i \to \infty\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:bc2" class="example"><strong>Example 3.10  </strong></span><span class="math inline">\(Z_1, Z_2, \cdots\)</span> are i.i.d. r.v.s with mean <span class="math inline">\(m\)</span> and <span class="math inline">\(|Z_i| \leq M\)</span>, <span class="math inline">\(\forall i=1,2,\cdots\)</span></p>
<p>By Hoeffding’s Inequality, <span class="math inline">\(\forall t&gt;0\)</span>, we have:</p>
<p><span class="math display">\[
P(|\frac{1}{n}\sum_{i=1}^n Z_i - m| \geq t) \leq 2 ~exp(\frac{-nt^2}{2M^2})
\]</span></p>
<p>The idea is to define a sequence <span class="math inline">\(\{t_n\}_{n\in N}\)</span> such that <span class="math inline">\(t_n \to 0\)</span> and <span class="math inline">\(\sum_{n=1}^\infty 2 ~exp(\frac{-nt_n^2}{2M^2}) &lt; \infty\)</span>.</p>
<p>Then let <span class="math inline">\(E_n := \{|\frac{1}{n}\sum_{i=1}^n Z_i - m| \geq t_n \}\)</span>. As a result, <span class="math inline">\(\sum_{n=1}^\infty P(E_n) \leq 2~exp(\frac{-nt^2}{2M^2}) &lt; \infty\)</span>.</p>
<p>By Borel-Cantelli: <span class="math inline">\(P(E) = 1\)</span>, where <span class="math inline">\(E = \{\omega \in \Omega: \forall n \text{ large enough, } \omega \in E_n^c \}\)</span>.</p>
<p>Therefore, what we are saying is that for almost every <span class="math inline">\(\omega \in \Omega\)</span>, <span class="math inline">\(\forall\)</span> large enough <span class="math inline">\(n\)</span>, we have <span class="math inline">\(|\frac{1}{n}\sum_{i=1}^n Z_i - m| \leq t_n\)</span>.</p>
<p>As <span class="math inline">\(t_n \to 0\)</span>, we then have for almost every <span class="math inline">\(\omega \in \Omega\)</span>, <span class="math inline">\(\underset{n \to \infty}{\operatorname{lim}} |\frac{1}{n}\sum_{i=1}^n Z_i - m| = 0\)</span>.</p>
<p>For example, <span class="math inline">\(t_n := \sqrt{4M^2 \frac{log(n)}{n}}\)</span> satisfies the above assumptions.</p>
</div>
<div class="theorem">
<p><span id="thm:hist" class="theorem"><strong>Theorem 3.11  </strong></span>Let <span class="math inline">\(\hat{\mathfrak f}_n\)</span> be a histogram classifier built from data <span class="math inline">\((x_1, y_1), \cdots, (x_n, y_n)\)</span>. Let <span class="math inline">\(h_n\)</span> be the length-scale associated to <span class="math inline">\(\hat{\mathfrak f}_n\)</span>. If <span class="math inline">\(\begin{cases} h_n \to 0 ~as~ n\to \infty\\ nh_n^d \to \infty ~as~ n \to \infty\end{cases}\)</span>, then <span class="math inline">\(\{\hat{\mathfrak f}_n\}\)</span> is universally strong consistent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>Recall <strong><em>histogram classifiers <a href="plug-in-classifiers-similarity-classifier.html#exm:weight2">3.5</a></em></strong></p>
<p><span class="math inline">\(\forall z\in R^d\)</span>, denote <span class="math inline">\(A_n(z)\)</span> by the cell in the grid that contains <span class="math inline">\(z\)</span>. Then, <span class="math inline">\(\omega_{in}(X) = \frac{1_{A_n(X)}(x_i)}{N_n(X)}\)</span>, where <span class="math inline">\(1_{A_n(X)}(x_i) := \begin{cases}1&amp;x_i \in A_n(X)\\0&amp;o.w. \end{cases}\)</span>, <span class="math inline">\(N_n(X):= \#\{j ~s.t. 1_{A_n(X)}(x_j)=1 \}\)</span>.</p>
<p>As <span class="math inline">\(\hat \eta_n(X) = \sum_{i=1}^n Y_i \omega_{in}(X) = \sum_{i=1}^n Y_i \frac{1_{A_n(X)}(x_i)}{N_n(X)}\)</span>, we have</p>
<p><span class="math display">\[
\hat \eta_n(X) \geq \frac{1}{2} \Leftrightarrow \sum_{i=1}^n Y_i1_{A_n(X)}(x_i) \geq \sum_{i=1}^n (1-Y_i)1_{A_n(X)}(x_i)
\]</span></p>
<p>Let <span class="math inline">\(\rho_X(A_n(x)):= P_{(X,Y) \sim \rho} (X\in A_n(x))\)</span>, <span class="math inline">\(\hat \alpha_n^0(x):= \frac{\sum_{i=1}^n (1-Y_i) 1_{A_n(X)}(x_i)}{n\rho_X(A_n(x))}\)</span>, <span class="math inline">\(\hat \alpha_n^1(x):=\frac{\sum_{i=1}^n Y_i 1_{A_n(X)}(x_i)}{n\rho_X(A_n(x))}\)</span>, then</p>
<p><span class="math display">\[
\hat \eta_n(x) \geq \frac{1}{2} \Leftrightarrow  \hat \alpha_n^0(x) \geq \hat \alpha_n^1(x)
\]</span></p>
<p>It can be shown that <span class="math inline">\(R(\hat{\mathfrak f}_n) - R^* \leq E_{(X,Y)\sim \rho} \Big[|1-\eta(X) - \hat \alpha_n^0(X)|\Big] + E_{(X,Y)\sim \rho} \Big[|\eta(X) - \hat \alpha_n^1(X)|\Big]\)</span> (use <span class="math inline">\(\hat \alpha_n^0(X)\)</span> to estimate <span class="math inline">\(1-\eta(X)\)</span> and use <span class="math inline">\(\hat \alpha_n^1(X)\)</span> to estimate <span class="math inline">\(\eta(X)\)</span>).</p>
<p>Let</p>
<p><span class="math display">\[
\begin{aligned}
g_n((x_1, y_1), \cdots, (x_n, y_n)) &amp;=  E_{(X,Y)\sim \rho} \Big[|\eta(X) - \hat \alpha_n^1(X)|\Big]\\
&amp;= g_n((x_1, y_1), \cdots, (x_n, y_n)) - E_{(x_1, y_1), \cdots, (x_n, y_n)[g_n((x_1, y_1), \cdots, (x_n, y_n))]} \text{ (variance)}\\
&amp;+E_{(x_1, y_1), \cdots, (x_n, y_n)[g_n((x_1, y_1), \cdots, (x_n, y_n))]} \text{ (bias)}
\end{aligned}
\]</span></p>
<p>then for the bias term, it goes to 0 as <span class="math inline">\(n \to \infty\)</span> following a slight generalization of Stone’s Theorem since here weights don’t exactly add to 1.</p>
<p>For the variance term, we have:</p>
<p><span class="math display">\[
\begin{aligned}
g_n((x_1, y_1), \cdots, (x_i,y_i),\cdots, (x_n, y_n)) &amp;= E_{(X,Y) \sim \rho}\Big[ |\eta(X) - \frac{\sum_{k=1}^n y_k 1_{A_n(X)}(x_k)}{n\rho_X(A_n(x))}| \Big]\\
&amp;= E_{(X,Y) \sim \rho}\Big[ |\eta(X) - \frac{\sum_{k\neq i} y_k 1_{A_n(X)}(x_k)}{n\rho_X(A_n(x))} + \frac{y_i 1_{A_n(X)}(x_i)}{n\rho_X(A_n(x))}| \Big]
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
g_n((x_1, y_1), \cdots, (x_i&#39;,y_i&#39;),\cdots, (x_n, y_n)) &amp;= E_{(X,Y) \sim \rho}\Big[ |\eta(X) - \frac{\sum_{k\neq i} y_k 1_{A_n(X)}(x_k)}{n\rho_X(A_n(x))} + \frac{y_i&#39; 1_{A_n(X)}(x_i&#39;)}{n\rho_X(A_n(x))}| \Big]
\end{aligned}
\]</span></p>
<p>As a reuslt,</p>
<p><span class="math display">\[
\begin{aligned}
| g_n((x_1, y_1), \cdots, (x_i,y_i),\cdots, (x_n, y_n)) -\\
g_n((x_1, y_1), \cdots, (x_i&#39;,y_i&#39;),\cdots, (x_n, y_n))| &amp;\leq E_{(X,Y)\sim \rho}[\frac{y_i 1_{A_n(X)}(x_i)}{n\rho_X(A_n(x))} + \frac{y_i&#39; 1_{A_n(X)}(x_i&#39;)}{n\rho_X(A_n(x))}]\\
&amp;\underset{x_i \in A_n(X) \Leftrightarrow X \in A_n(x_i)}{\leq} E[\frac{1_{A_n(x_i)}(X)}{n\rho_X(A_n(x_i))}] + E[\frac{1_{A_n(x_i&#39;)}(X)}{n\rho_X(A_n(x_i&#39;))}]\\
&amp;= \frac{1}{n \rho_X(A_n(x_i))}P(X\in A_n(x_i)) + \frac{1}{n \rho_X(A_n(x_i&#39;))}P(X\in A_n(x_i&#39;))\\
&amp;= \frac{2}{n}
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(C_i = \frac{2}{n}\)</span>, by McDiarmid’s Inequality, we have:</p>
<p><span class="math display">\[
\begin{aligned}
P(|g_n - E[g_n]| \geq t) &amp;\leq 2~exp(\frac{-2t^2}{n\sum C_i^2})\\
&amp;= 2~exp(-\frac{1}{2}nt^2)
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(t_n:= \sqrt{c \frac{log(n)}{n}}\)</span>, where we can choose <span class="math inline">\(C\)</span> large enough so that by Borel-Cantelli Theorem, we have: with probability 1, <span class="math inline">\(\forall\)</span> large enough <span class="math inline">\(n\)</span>, we have <span class="math inline">\(|g_n - E[g_n]| \leq t_n\)</span>. As a result, we have <span class="math inline">\(|g_n - E[g_n]| \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span> a.s.</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-problem-bayes-classifier.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="empirical-risk-minimization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-Plug-in.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
